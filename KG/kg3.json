{
  "nodes": [
    {
      "id": "I Supervised learning",
      "type": "major",
      "parent": null,
      "description": "Introduction to supervised learning techniques in machine learning."
    },
    {
      "id": "1 Linear regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Linear regression models and their applications."
    },
    {
      "id": "1.1 LMS algorithm",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Least Mean Squares (LMS) algorithm for linear regression."
    },
    {
      "id": "1.2 The normal equations",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Normal equations method in linear regression."
    },
    {
      "id": "1.2.1 Matrix derivatives",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Derivation of matrix derivatives used in normal equations."
    },
    {
      "id": "1.2.2 Least squares revisited",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Revisiting least squares method with matrix notation."
    },
    {
      "id": "1.3 Probabilistic interpretation",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Explains why least-squares regression is a reasonable choice for regression problems based on probabilistic assumptions."
    },
    {
      "id": "2 Classification and logistic regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Classification problems and logistic regression model."
    },
    {
      "id": "2.1 Logistic regression",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Logistic regression for binary classification tasks."
    },
    {
      "id": "2.2 Digression: the perceptron learning algorithm",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Introduction to Perceptron Learning Algorithm."
    },
    {
      "id": "2.3 Multi-class classification",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Classification problem with response variable taking k values"
    },
    {
      "id": "2.4 Another algorithm for maximizing \\(\\ell(\\theta)\\)",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Alternative algorithms to maximize the likelihood function in classification tasks."
    },
    {
      "id": "3 Generalized linear models",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Generalized Linear Models (GLMs) for various types of data."
    },
    {
      "id": "3.1 The exponential family",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Introduction to the Exponential Family in GLM theory."
    },
    {
      "id": "3.2 Constructing GLMs",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Constructing Generalized Linear Models using different distributions and link functions."
    },
    {
      "id": "3.2.1 Ordinary least squares",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Ordinary Least Squares method in the context of GLMs."
    },
    {
      "id": "3.2.2 Logistic regression",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Logistic Regression as a specific case of GLM for binary classification."
    },
    {
      "id": "4 Generative learning algorithms",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Generative models and their applications in machine learning."
    },
    {
      "id": "4.1 Gaussian discriminant analysis",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Gaussian Discriminant Analysis (GDA) for classification tasks."
    },
    {
      "id": "4.1.1 The multivariate normal distribution",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Properties of the Multivariate Normal Distribution in GDA."
    },
    {
      "id": "4.1.2 The Gaussian discriminant analysis model",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Model formulation and properties of GDA."
    },
    {
      "id": "4.1.3 Discussion: GDA and logistic regression",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Comparison between GDA and Logistic Regression models."
    },
    {
      "id": "4.2 Naive bayes (Option Reading)",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Naive Bayes Classifier for text classification tasks."
    },
    {
      "id": "4.2.1 Laplace smoothing",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Laplace Smoothing technique in Naive Bayes Classifier."
    },
    {
      "id": "4.2.2 Event models for text classification",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Event Models used in Text Classification with Naive Bayes."
    },
    {
      "id": "5 Kernel methods",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Kernel Methods for non-linear classification and regression tasks."
    },
    {
      "id": "5.1 Feature maps",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Feature mapping techniques in kernel methods."
    },
    {
      "id": "5.2 LMS (least mean squares) with features",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Least Mean Squares algorithm applied to feature mapped data."
    },
    {
      "id": "5.3 LMS with the kernel trick",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Application of the kernel trick in Least Mean Squares algorithm."
    },
    {
      "id": "5.4 Properties of kernels",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Properties and characteristics of different kernels used in machine learning."
    },
    {
      "id": "6 Support vector machines",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Support Vector Machines for classification tasks."
    },
    {
      "id": "II Deep learning",
      "type": "major",
      "parent": null,
      "description": "Introduction to deep learning techniques and neural networks."
    },
    {
      "id": "7 Deep learning",
      "type": "subnode",
      "parent": "II Deep learning",
      "description": "Overview of deep learning concepts and applications."
    },
    {
      "id": "7.1 Supervised learning with non-linear models",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Supervised learning using non-linear models in deep learning."
    },
    {
      "id": "7.2 Neural networks",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Introduction to neural networks and their architecture."
    },
    {
      "id": "7.3 Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Modules used in modern neural network architectures."
    },
    {
      "id": "7.4 Backpropagation",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Backpropagation algorithm for training deep neural networks."
    },
    {
      "id": "7.4.1 Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Basics of partial derivatives used in backpropagation."
    },
    {
      "id": "7.4.2 General strategy of backpropagation",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "General approach to the backpropagation algorithm."
    },
    {
      "id": "7.4.3 Backward functions for basic modules",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Backward propagation functions for simple neural network components."
    },
    {
      "id": "7.4.4 Back-propagation for MLPs",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Backpropagation algorithm applied to Multi-Layer Perceptrons (MLPs)."
    },
    {
      "id": "Modern Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Overview of modern neural network architectures and concepts."
    },
    {
      "id": "Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Discussion on the building blocks of current neural networks."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Algorithm for efficiently computing gradients in neural networks by propagating errors backwards through layers."
    },
    {
      "id": "Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to the mathematical concept of partial derivatives used in backpropagation."
    },
    {
      "id": "General strategy of backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the general approach and steps involved in backpropagation."
    },
    {
      "id": "Backward functions for basic modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Details on backward propagation functions for fundamental network components."
    },
    {
      "id": "Back-propagation for MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Specific application of backpropagation to multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Vectorization over training examples",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Techniques for efficient computation using vectorized operations."
    },
    {
      "id": "Generalization and regularization",
      "type": "major",
      "parent": null,
      "description": "Topics related to improving model performance on unseen data through generalization techniques and regularization."
    },
    {
      "id": "Bias-variance tradeoff",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Explanation of the balance between bias and variance in machine learning models."
    },
    {
      "id": "A mathematical decomposition (for regression)",
      "type": "subnode",
      "parent": "Bias-variance tradeoff",
      "description": "Mathematical breakdown for understanding bias-variance in regression problems."
    },
    {
      "id": "The double descent phenomenon",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Discussion on the unexpected performance improvement with increased model complexity."
    },
    {
      "id": "Sample complexity bounds (optional readings)",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Bounds related to sample size requirements for learning algorithms."
    },
    {
      "id": "Preliminaries",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Introduction to necessary concepts for understanding sample complexity bounds."
    },
    {
      "id": "The case of finite H",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Analysis under the assumption that hypothesis space is finite."
    },
    {
      "id": "The case of infinite H",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Analysis when the hypothesis space is infinite."
    },
    {
      "id": "Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Techniques for controlling overfitting through regularization and selecting optimal models."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Methods to prevent overfitting by penalizing overly complex models."
    },
    {
      "id": "Implicit regularization effect (optional reading)",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Discussion on the implicit regularization effects in certain optimization algorithms."
    },
    {
      "id": "Model selection via cross validation",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Procedure for choosing the best model using cross-validation techniques."
    },
    {
      "id": "Bayesian statistics and regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Application of Bayesian methods in regularization to improve model generalization."
    },
    {
      "id": "Unsupervised learning",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning from unlabeled data to discover hidden structures."
    },
    {
      "id": "Clustering and the k-means algorithm",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Introduction to clustering methods with a focus on the k-means algorithm."
    },
    {
      "id": "EM algorithms",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Exploration of expectation-maximization algorithms for parameter estimation in probabilistic models."
    },
    {
      "id": "EM for mixture of Gaussians",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Application of EM to the problem of estimating parameters in a Gaussian mixture model."
    },
    {
      "id": "Jensen's inequality",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Mathematical principle used in deriving properties of EM algorithm."
    },
    {
      "id": "General EM algorithms",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Discussion on the general formulation and application of expectation-maximization techniques."
    },
    {
      "id": "Other interpretation of ELBO",
      "type": "subnode",
      "parent": "General EM algorithms",
      "description": "Alternative perspective on evidence lower bound (ELBO) in variational inference."
    },
    {
      "id": "Mixture of Gaussians revisited",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Revisit and deepen understanding of mixture models with Gaussian distributions."
    },
    {
      "id": "Variational inference and variational auto-encoder (optional reading)",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Advanced topics on variational methods for approximate Bayesian inference."
    },
    {
      "id": "Principal components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Dimensionality reduction technique that projects data onto principal component axes."
    },
    {
      "id": "Independent components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Technique for separating a multivariate signal into independent, non-Gaussian components."
    },
    {
      "id": "ICA ambiguities",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Discussion on the inherent limitations and indeterminacies of ICA solutions."
    },
    {
      "id": "Densities and linear transformations",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Analysis of how densities change under linear transformations in ICA context."
    },
    {
      "id": "ICA algorithm",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Detailed explanation of the Independent Components Analysis algorithm."
    },
    {
      "id": "Self-supervised learning and foundation models",
      "type": "major",
      "parent": null,
      "description": "Approaches to training models using self-generated supervision signals and foundational model architectures."
    },
    {
      "id": "Pretraining and adaptation",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Overview of pre-training techniques followed by fine-tuning for specific tasks."
    },
    {
      "id": "Pretraining methods in computer vision",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Review of various self-supervised pre-training strategies used in visual recognition tasks."
    },
    {
      "id": "Pretrained large language models",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Discussion on the development and applications of large-scale pretrained language models."
    },
    {
      "id": "Open up the blackbox of Transformers",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Insight into the architecture and workings of Transformer-based models."
    },
    {
      "id": "Zero-shot learning and in-context learning",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Exploration of capabilities for zero-shot task adaptation with pretrained models."
    },
    {
      "id": "Reinforcement Learning and Control",
      "type": "major",
      "parent": null,
      "description": "Topics related to reinforcement learning algorithms and control theory applications."
    },
    {
      "id": "Reinforcement learning",
      "type": "subnode",
      "parent": "Reinforcement Learning and Control",
      "description": "Introduction to the principles and techniques of reinforcement learning."
    },
    {
      "id": "Markov decision processes",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Foundation for understanding sequential decision-making problems in RL."
    },
    {
      "id": "Value iteration and policy iteration",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Detailed explanation of iterative methods to find optimal policies in MDPs."
    },
    {
      "id": "Learning a model for an MDP",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Techniques for estimating the transition and reward functions of an environment."
    },
    {
      "id": "Continuous state MDPs",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Discussion on handling environments with continuous state spaces in RL."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Method to convert a continuous state space into discrete for computational feasibility."
    },
    {
      "id": "Value function approximation",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Approaches to approximate value functions in large or continuous state spaces."
    },
    {
      "id": "Connections between Policy and Value Iteration (Optional)",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Analysis of the relationship and equivalences between policy iteration and value iteration methods."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": null,
      "description": "Topics related to linear quadratic regulation (LQR), differential dynamic programming (DDP) and linear-quadratic-Gaussian control."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Discussion on Markov decision processes with a finite time horizon."
    },
    {
      "id": "Linear Q",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Introduction to linear quadratic control problems in the context of reinforcement learning."
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Optimal control for linear systems with quadratic cost functions"
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Approaches to convert nonlinear dynamics into an LQR problem"
    },
    {
      "id": "Linearization of dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Approximating nonlinear systems with linear models"
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Optimization technique for trajectory optimization in robotics and control"
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Control strategy combining LQR with stochastic dynamics"
    },
    {
      "id": "Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Method for optimizing policies in reinforcement learning"
    },
    {
      "id": "Supervised Learning Examples",
      "type": "major",
      "parent": null,
      "description": "Introduction to supervised learning through examples"
    },
    {
      "id": "Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning process where a model is trained on labeled data to predict outcomes."
    },
    {
      "id": "Hypothesis",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Function learned by the algorithm that maps inputs to outputs."
    },
    {
      "id": "Regression Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Type of supervised learning where target variable is continuous."
    },
    {
      "id": "Classification Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Type of supervised learning where target variable takes discrete values."
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": "Supervised Learning Algorithm",
      "description": "Technique for modeling the relationship between a scalar response and one or more explanatory variables using a linear predictor function."
    },
    {
      "id": "MachineLearningBasics",
      "type": "major",
      "parent": null,
      "description": "Fundamental concepts in machine learning including probability distributions and random variables."
    },
    {
      "id": "FunctionRepresentation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "How functions/hypotheses are represented in machine learning models."
    },
    {
      "id": "LinearHypothesis",
      "type": "subnode",
      "parent": "FunctionRepresentation",
      "description": "Using linear functions to approximate the relationship between input and output variables."
    },
    {
      "id": "ParametersWeights",
      "type": "subnode",
      "parent": "LinearHypothesis",
      "description": "Explanation of parameters (weights) in a linear model."
    },
    {
      "id": "CostFunction",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "A function that measures the error between predicted and actual outputs."
    },
    {
      "id": "OrdinaryLeastSquares",
      "type": "subnode",
      "parent": "CostFunction",
      "description": "A method for minimizing the cost function using least squares regression."
    },
    {
      "id": "LMSAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "An iterative algorithm to minimize the cost function by adjusting parameters."
    },
    {
      "id": "GradientDescent",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Optimization algorithm used to minimize the cost function in machine learning models."
    },
    {
      "id": "LearningRate",
      "type": "subnode",
      "parent": "GradientDescent",
      "description": "Hyperparameter controlling the step size in gradient descent."
    },
    {
      "id": "CostFunctionJ",
      "type": "subnode",
      "parent": "GradientDescent",
      "description": "Function to be minimized, often representing error or loss."
    },
    {
      "id": "UpdateRule",
      "type": "subnode",
      "parent": "GradientDescent",
      "description": "Specific formula for updating parameters in gradient descent."
    },
    {
      "id": "LMSUpdateRule",
      "type": "subnode",
      "parent": "UpdateRule",
      "description": "Least mean squares update rule, also known as Widrow-Hoff learning rule."
    },
    {
      "id": "PartialDerivative",
      "type": "subnode",
      "parent": "CostFunctionJ",
      "description": "Derivative of the cost function with respect to parameters."
    },
    {
      "id": "SingleTrainingExample",
      "type": "subnode",
      "parent": "UpdateRule",
      "description": "Special case of update rule for a single training example."
    },
    {
      "id": "Machine_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "General concepts in machine learning including models and optimization techniques."
    },
    {
      "id": "Gradient_Descent_Methods",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for minimizing cost functions in ML models."
    },
    {
      "id": "LMS_Update_Rule",
      "type": "subnode",
      "parent": "Gradient_Descent_Methods",
      "description": "Least Mean Squares update rule used to adjust model parameters based on error."
    },
    {
      "id": "Widrow-Hoff_Learning_Rule",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Alternative name for the LMS update rule in neural networks."
    },
    {
      "id": "Error_Term",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Difference between predicted and actual values used to adjust parameters."
    },
    {
      "id": "Batch_Gradient_Descent",
      "type": "subnode",
      "parent": "Gradient_Descent_Methods",
      "description": "Algorithm that computes the gradient using all available training data."
    },
    {
      "id": "Convergence_Criteria",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Conditions under which the EM algorithm is considered to have converged"
    },
    {
      "id": "LinearRegressionOptimization",
      "type": "major",
      "parent": null,
      "description": "Discusses the optimization problem in linear regression and its solution."
    },
    {
      "id": "GradientDescentConvergence",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Explains how gradient descent converges to a global minimum for convex quadratic functions."
    },
    {
      "id": "BatchGradientDescentExample",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Provides an example of batch gradient descent in predicting housing prices based on living area and number of bedrooms."
    },
    {
      "id": "StochasticGradientDescentAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Introduces the concept and algorithm for stochastic gradient descent as an alternative to batch gradient descent."
    },
    {
      "id": "StochasticGradientDescent",
      "type": "major",
      "parent": "GradientDescent",
      "description": "Variant of gradient descent that uses a single data point for each iteration."
    },
    {
      "id": "BatchGradientDescent",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Requires scanning entire dataset before updating parameters."
    },
    {
      "id": "IncrementalGradientDescent",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Synonym for stochastic gradient descent, updates parameters incrementally."
    },
    {
      "id": "TrainingSetSizeImpact",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Performance advantage of SGD over batch when training set size is large."
    },
    {
      "id": "ConvergenceBehavior",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Parameters oscillate around minimum but still provide good approximations."
    },
    {
      "id": "LearningRateAdjustment",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Slowly decreasing learning rate can ensure convergence to global minimum."
    },
    {
      "id": "NormalEquations",
      "type": "major",
      "parent": "LinearRegression",
      "description": "Explains normal equations used in linear regression for finding optimal parameters."
    },
    {
      "id": "MatrixDerivatives",
      "type": "subnode",
      "parent": "NormalEquations",
      "description": "Notation and rules for taking derivatives of functions with matrix inputs."
    },
    {
      "id": "Matrix Derivatives",
      "type": "major",
      "parent": null,
      "description": "Derivation of matrix derivatives for functions mapping from matrices to real numbers."
    },
    {
      "id": "Gradient Calculation Example",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Example calculation of the gradient for a given function and 2x2 matrix."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares using matrix derivatives to find optimal theta."
    },
    {
      "id": "Design Matrix Definition",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Definition and explanation of the design matrix used in linear regression problems."
    },
    {
      "id": "Target Vector y",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Explanation of the target vector containing all training set target values."
    },
    {
      "id": "MachineLearningOverview",
      "type": "major",
      "parent": null,
      "description": "Introduction to machine learning concepts and neural networks."
    },
    {
      "id": "LinearRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Basic linear regression model for predicting outcomes."
    },
    {
      "id": "HypothesisFunction",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Function hθ(x) derived from the conditional distribution p(y|x;θ)."
    },
    {
      "id": "GradientCalculation",
      "type": "subnode",
      "parent": "CostFunction",
      "description": "Derivation and calculation of gradients for minimizing the loss function."
    },
    {
      "id": "Regression problem",
      "type": "subnode",
      "parent": "1.3 Probabilistic interpretation",
      "description": "A statistical approach to predict continuous outcomes based on input variables."
    },
    {
      "id": "Least-squares cost function J",
      "type": "subnode",
      "parent": "1.3 Probabilistic interpretation",
      "description": "Cost function used in linear regression to minimize the sum of squared errors between predicted and actual values."
    },
    {
      "id": "Target variable y^(i)",
      "type": "subnode",
      "parent": "1.3 Probabilistic interpretation",
      "description": "Dependent variable in a regression model, represented as a linear combination of input features plus an error term."
    },
    {
      "id": "Input variables x^(i)",
      "type": "subnode",
      "parent": "1.3 Probabilistic interpretation",
      "description": "Independent variables used to predict the target variable in a regression model."
    },
    {
      "id": "Error term ε^(i)",
      "type": "subnode",
      "parent": "1.3 Probabilistic interpretation",
      "description": "Represents unmodeled effects or random noise in the prediction of y^(i)."
    },
    {
      "id": "Gaussian distribution",
      "type": "subnode",
      "parent": "1.3 Probabilistic interpretation",
      "description": "Normal distribution used to model the error term ε^(i) with mean zero and variance σ^2."
    },
    {
      "id": "θ (parameters)",
      "type": "subnode",
      "parent": "1.3 Probabilistic interpretation",
      "description": "Parameters of the linear regression model that determine the relationship between input variables and target variable."
    },
    {
      "id": "ConditionalProbabilityDistribution",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explains the conditional probability distribution of y given x with parameter θ."
    },
    {
      "id": "DesignMatrixX",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Describes X as a design matrix containing all xi's."
    },
    {
      "id": "ProbabilityOfData",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explains the probability of data given X and θ."
    },
    {
      "id": "LikelihoodFunction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "The likelihood of the training data given model parameters in terms of product probabilities over all emails."
    },
    {
      "id": "IndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Assumption that training examples are independent, affecting likelihood computation and performance."
    },
    {
      "id": "LikelihoodExpression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Expresses likelihood as a product of individual probabilities for each data point."
    },
    {
      "id": "MaximumLikelihoodEstimation",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process to estimate model parameters by maximizing joint likelihood over training data."
    },
    {
      "id": "LogLikelihoodFunction",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Introduces log likelihood as a simpler function for maximization."
    },
    {
      "id": "Likelihood_Maximization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Process of maximizing likelihood to estimate parameters."
    },
    {
      "id": "Least_Squares_Regression",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "A model for predicting continuous outcomes based on minimizing the sum of squared errors."
    },
    {
      "id": "Maximum_Likelihood_Estimation",
      "type": "subnode",
      "parent": "Likelihood_Maximization",
      "description": "A method for estimating model parameters by maximizing the likelihood of observing the data given those parameters."
    },
    {
      "id": "Probabilistic_Assumptions",
      "type": "subnode",
      "parent": "Least_Squares_Regression",
      "description": "Assumptions about the distribution of errors in regression models."
    },
    {
      "id": "Locally_weighted_linear_regression",
      "type": "major",
      "parent": null,
      "description": "Technique for fitting a linear model to data with local weighting."
    },
    {
      "id": "FeatureSelection",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Choosing relevant features for model accuracy."
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Situation where a model is too simple to capture the underlying pattern of the data."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Condition where a model performs well on training data but poorly on unseen data."
    },
    {
      "id": "LocallyWeightedLinearRegression",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Variant of linear regression that weights training examples based on proximity to the query point."
    },
    {
      "id": "FittingTheta",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Process of fitting parameters \\(\\theta\\) in a linear model."
    },
    {
      "id": "WeightedFitting",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Adjusting the fitting process to include weights for different training examples."
    },
    {
      "id": "WeightsCalculation",
      "type": "subnode",
      "parent": "WeightedFitting",
      "description": "Method of calculating weights based on distance from query point."
    },
    {
      "id": "BandwidthParameter",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Parameter \\(\\tau\\) that controls the influence radius of training examples."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "General overview of machine learning concepts and applications."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "A non-parametric algorithm that requires the entire training set for predictions."
    },
    {
      "id": "Non-Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms where the amount of data needed to represent the hypothesis grows with the size of the training set."
    },
    {
      "id": "Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms that have a fixed, finite number of parameters independent of the dataset size."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "A classification problem where the output variable can take only two possible outcomes."
    },
    {
      "id": "Logistic Regression",
      "type": "major",
      "parent": "Newton's Method",
      "description": "Application of Newton's method in logistic regression for parameter estimation."
    },
    {
      "id": "LogisticRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of logistic regression for binary classification using Bernoulli distribution."
    },
    {
      "id": "LinearRegressionLimitations",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Discussion on why linear regression is unsuitable for classification tasks."
    },
    {
      "id": "SigmoidFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Definition and properties of the logistic function used in logistic regression."
    },
    {
      "id": "HypothesisFormulation",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Formulating hypotheses using the sigmoid function for classification."
    },
    {
      "id": "DerivativeSigmoid",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Calculation and properties of the derivative of the logistic function."
    },
    {
      "id": "Machine_Learning_Models",
      "type": "major",
      "parent": null,
      "description": "Overview of models used in machine learning including regression and classification."
    },
    {
      "id": "Classification_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Models designed to predict categorical outcomes, often using probabilistic approaches."
    },
    {
      "id": "Probabilistic_Assumptions_Classification",
      "type": "subnode",
      "parent": "Classification_Models",
      "description": "Assumptions made about the probability distribution of classification outcomes based on input features."
    },
    {
      "id": "Likelihood_Function",
      "type": "subnode",
      "parent": "Maximum_Likelihood_Estimation",
      "description": "A function that represents the likelihood of observing a dataset given model parameters, used in maximum likelihood estimation."
    },
    {
      "id": "Log_Likelihood",
      "type": "subnode",
      "parent": "Likelihood_Function",
      "description": "The logarithm of the likelihood function, often easier to work with due to its properties."
    },
    {
      "id": "Gradient_Ascend_Method",
      "type": "subnode",
      "parent": "Maximum_Likelihood_Estimation",
      "description": "An optimization technique used to maximize a function by iteratively moving in the direction of steepest ascent, as opposed to gradient descent which minimizes functions."
    },
    {
      "id": "Stochastic_Gradient_Ascend_Rule",
      "type": "subnode",
      "parent": "Gradient_Ascend_Method",
      "description": "A rule for updating model parameters based on a single data point in the context of maximizing likelihood."
    },
    {
      "id": "GradientAscentRule",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Update rule for parameters in logistic regression."
    },
    {
      "id": "NonLinearModels",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Models that use non-linear functions for predictions."
    },
    {
      "id": "LogisticLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Definition and properties of the logistic loss function."
    },
    {
      "id": "NegativeLogLikelihood",
      "type": "subnode",
      "parent": "LogisticLossFunction",
      "description": "Measure of how well a probabilistic model fits the data, used in loss functions."
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "General concepts in machine learning including classification and decision boundaries."
    },
    {
      "id": "Logistic Regression Derivation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Derivation of logistic regression formulae and their relationship to probabilistic interpretations."
    },
    {
      "id": "Perceptron Learning Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Historical algorithm for binary classification with a threshold function."
    },
    {
      "id": "Multi-class Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Classification problems where the response variable can take on multiple values."
    },
    {
      "id": "Response Variable",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Discrete variable that can take on more than two values"
    },
    {
      "id": "Multinomial Distribution",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Distribution over k possible discrete outcomes"
    },
    {
      "id": "Softmax Function",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Function to convert scores into probabilities summing up to 1"
    },
    {
      "id": "SoftmaxFunction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A function that converts a vector of real numbers into a probability distribution."
    },
    {
      "id": "Logits",
      "type": "subnode",
      "parent": "SoftmaxFunction",
      "description": "Outputs of the model before applying softmax function, representing predictions for each class."
    },
    {
      "id": "ProbabilityVector",
      "type": "subnode",
      "parent": "SoftmaxFunction",
      "description": "Output of the softmax function representing probabilities that sum up to 1."
    },
    {
      "id": "ProbabilisticModel",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A model using softmax outputs as conditional probabilities for classification tasks."
    },
    {
      "id": "CrossEntropyLoss",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A loss function that quantifies the difference between two probability distributions."
    },
    {
      "id": "Cross_Entropy_Loss",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and usage of cross-entropy loss in classification tasks."
    },
    {
      "id": "Softmax_Function",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Explanation of the softmax function used within cross-entropy loss."
    },
    {
      "id": "Gradient_Calculation",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Derivation and formula for calculating gradients in cross-entropy loss."
    },
    {
      "id": "NewtonMethod",
      "type": "major",
      "parent": null,
      "description": "Explanation of Newton's method as an algorithm for finding zeros of a function."
    },
    {
      "id": "Newton's Method",
      "type": "major",
      "parent": null,
      "description": "Optimization technique that uses second-order derivatives for faster convergence."
    },
    {
      "id": "Finding Roots",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Process of determining values of theta where f(theta) = 0."
    },
    {
      "id": "Maximizing Functions",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Using Newton's method to find the maximum of a function by setting its derivative to zero."
    },
    {
      "id": "Gradient Descent",
      "type": "major",
      "parent": null,
      "description": "Optimization algorithm that uses iterative steps to find the minimum or maximum of a function."
    },
    {
      "id": "Hessian Matrix",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Matrix of second-order partial derivatives used in Newton-Raphson method for multidimensional optimization."
    },
    {
      "id": "Fisher Scoring",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Application of Newton's method to maximize logistic regression likelihood function."
    },
    {
      "id": "Generalized Linear Models (GLMs)",
      "type": "major",
      "parent": null,
      "description": "Broad family of models that includes both regression and classification methods."
    },
    {
      "id": "Exponential Family Distributions",
      "type": "subnode",
      "parent": "Generalized Linear Models (GLMs)",
      "description": "Class of distributions written in exponential form for GLM derivation."
    },
    {
      "id": "ExponentialFamilyDistributions",
      "type": "major",
      "parent": "GeneralizedLinearModelsGLM",
      "description": "A class of probability distributions that includes many common distributions like Poisson and Gaussian."
    },
    {
      "id": "NaturalParameter",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The parameter η in the exponential family distribution formula."
    },
    {
      "id": "SufficientStatistic",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "A function T(y) that summarizes all necessary information from data y for inference about a parameter."
    },
    {
      "id": "LogPartitionFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The function a(η) that ensures the distribution sums/integrates to 1."
    },
    {
      "id": "BernoulliDistribution",
      "type": "major",
      "parent": "GeneralizedLinearModelsGLM",
      "description": "A discrete probability distribution over binary random variables with parameter φ."
    },
    {
      "id": "NaturalParameterForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "η = log(φ/(1-φ)), related to the sigmoid function."
    },
    {
      "id": "SufficientStatisticForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "T(y) = y, indicating that the sufficient statistic is simply the outcome itself."
    },
    {
      "id": "LogPartitionFunctionForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "a(η) = log(1 + e^η), ensuring normalization of probabilities."
    },
    {
      "id": "GeneralizedLinearModelsGLM",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Models that extend linear regression and logistic regression for various distributions."
    },
    {
      "id": "GaussianDistribution",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Continuous distribution used for regression problems with real-valued outputs."
    },
    {
      "id": "LogisticRegressionAsGLM",
      "type": "subnode",
      "parent": "GeneralizedLinearModelsGLM",
      "description": "Binary classification model formulated as a GLM using Bernoulli distribution."
    },
    {
      "id": "ConstructingGLMs",
      "type": "major",
      "parent": null,
      "description": "Process of building models for different types of data and distributions."
    },
    {
      "id": "MachineLearningModels",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning models including logistic regression and generative algorithms."
    },
    {
      "id": "PoissonDistribution",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A statistical distribution often used to model count data such as website visitors."
    },
    {
      "id": "GeneralizedLinearModel(GLM)",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Statistical models that extend linear regression to accommodate non-normal distributions."
    },
    {
      "id": "AssumptionsForGLMs",
      "type": "subnode",
      "parent": "GeneralizedLinearModel(GLM)",
      "description": "Three key assumptions used to derive GLMs: conditional distribution, prediction goal, and linear relationship between parameters."
    },
    {
      "id": "GLMsDesignChoices",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Three design choices in Generalized Linear Models (GLMs)."
    },
    {
      "id": "ConditionalDistributionModeling",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Techniques for modeling the conditional distribution p(y|x;θ)."
    },
    {
      "id": "LogisticFunction",
      "type": "subnode",
      "parent": "HypothesisFunction",
      "description": "Sigmoid function used for binary classification and probability estimation."
    },
    {
      "id": "CanonicalResponseFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Function giving the mean of a distribution as a function of its natural parameter."
    },
    {
      "id": "CanonicalLinkFunction",
      "type": "subnode",
      "parent": "CanonicalResponseFunction",
      "description": "Inverse of the canonical response function, mapping from the natural parameter to the expected value."
    },
    {
      "id": "MachineLearningAlgorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of different types of machine learning algorithms."
    },
    {
      "id": "DiscriminativeAlgorithms",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Algorithms that learn p(y|x) directly or map inputs to labels."
    },
    {
      "id": "PerceptronAlgorithm",
      "type": "subnode",
      "parent": "DiscriminativeAlgorithms",
      "description": "Attempts to find a decision boundary for classification."
    },
    {
      "id": "GenerativeAlgorithms",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Learn p(x|y) and p(y) to model data distributions."
    },
    {
      "id": "ClassPriors",
      "type": "subnode",
      "parent": "GenerativeAlgorithms",
      "description": "Model the prior probability of each class."
    },
    {
      "id": "BayesRule",
      "type": "subnode",
      "parent": "GenerativeAlgorithms",
      "description": "Use Bayes theorem to derive posterior distribution p(y|x)."
    },
    {
      "id": "Bayes Rule Application",
      "type": "major",
      "parent": null,
      "description": "Using Bayes rule to derive posterior distribution on y given x."
    },
    {
      "id": "Class Priors",
      "type": "subnode",
      "parent": "Bayes Rule Application",
      "description": "Probability of each class before observing data."
    },
    {
      "id": "Conditional Probability p(x|y)",
      "type": "subnode",
      "parent": "Bayes Rule Application",
      "description": "Distribution of features given the class label."
    },
    {
      "id": "Gaussian Discriminant Analysis (GDA)",
      "type": "major",
      "parent": null,
      "description": "Generative learning algorithm assuming multivariate normal distribution for p(x|y)."
    },
    {
      "id": "Multivariate Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Distribution parameterized by mean vector and covariance matrix."
    },
    {
      "id": "Mean Vector",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Vector representing the expected value of a multivariate normal distribution."
    },
    {
      "id": "Covariance Matrix",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Matrix describing the variance and covariance between variables in a multivariate normal distribution."
    },
    {
      "id": "MeanVector",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "The mean vector μ in a multivariate Gaussian distribution."
    },
    {
      "id": "CovarianceMatrix",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "A matrix that generalizes the notion of variance to multiple dimensions."
    },
    {
      "id": "StandardNormalDistribution",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "Special case of Gaussian distribution with zero mean and identity covariance matrix."
    },
    {
      "id": "DensityPlots",
      "type": "subnode",
      "parent": "GaussianDistribution",
      "description": "Visual representations showing how the density changes with different parameters."
    },
    {
      "id": "CovarianceDefinition",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Definition of covariance for vector-valued random variables and its relation to variance."
    },
    {
      "id": "MachineLearningConcepts",
      "type": "major",
      "parent": null,
      "description": "Overview of concepts in machine learning including Gaussian distributions and classification models."
    },
    {
      "id": "GaussianDistributions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Exploration of multivariate normal distribution properties through covariance matrix manipulation."
    },
    {
      "id": "CovarianceMatrixManipulation",
      "type": "subnode",
      "parent": "GaussianDistributions",
      "description": "Demonstration of how varying the off-diagonal elements affects density contours."
    },
    {
      "id": "MeanVectorVariation",
      "type": "subnode",
      "parent": "GaussianDistributions",
      "description": "Illustration of moving mean vectors while keeping covariance matrix constant."
    },
    {
      "id": "GaussianDiscriminantAnalysis",
      "type": "major",
      "parent": "MachineLearningModels",
      "description": "Model for classification problems using multivariate normal distributions for different classes."
    },
    {
      "id": "Log-Likelihood Function",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Function used to estimate model parameters by maximizing likelihood of data given parameters."
    },
    {
      "id": "Parameters Estimation",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Process of finding the maximum likelihood estimates for φ, μ₀, μ₁, and Σ."
    },
    {
      "id": "Decision Boundary",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Line where probability of y=1 given x equals 0.5, separating predictions for y=0 and y=1."
    },
    {
      "id": "Relationship to Logistic Regression",
      "type": "major",
      "parent": null,
      "description": "Exploration of similarities between GDA model output and logistic regression function form."
    },
    {
      "id": "DecisionBoundariesComparison",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Discussion on when GDA and logistic regression provide different decision boundaries."
    },
    {
      "id": "ModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Analysis of assumptions made by GDA versus logistic regression."
    },
    {
      "id": "AsymptoticEfficiency",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "Property indicating optimal performance with large datasets when assumptions are correct."
    },
    {
      "id": "GDA",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Generative Discriminative Algorithm that makes strong modeling assumptions."
    },
    {
      "id": "PoissonDataExample",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Illustration of how logistic regression performs better on Poisson data compared to GDA."
    },
    {
      "id": "NaiveBayesAlgorithm",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "Machine_Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "Text_Classification",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques for classifying text into categories such as spam or non-spam."
    },
    {
      "id": "Spam_Filtering",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Application of machine learning to identify spam emails."
    },
    {
      "id": "Training_Set",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "A dataset used for training a model with examples and labels."
    },
    {
      "id": "Feature_Vector",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Vector representation of an email based on its content."
    },
    {
      "id": "Vocabulary",
      "type": "subnode",
      "parent": "Feature_Vector",
      "description": "Set of words used to represent emails in a feature vector."
    },
    {
      "id": "Stop_Words",
      "type": "subnode",
      "parent": "Vocabulary",
      "description": "Commonly excluded high-frequency words like 'the', 'of', and 'and'."
    },
    {
      "id": "Machine_Learning_Topic",
      "type": "major",
      "parent": null,
      "description": "General topic encompassing machine learning concepts and techniques."
    },
    {
      "id": "Feature_Vector_Selection",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Choosing relevant features (words) to represent documents in a vector form."
    },
    {
      "id": "Stop_Words_Exclusion",
      "type": "subnode",
      "parent": "Feature_Vector_Selection",
      "description": "Excluding common words that do not contribute much to the classification task."
    },
    {
      "id": "Generative_Modeling",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Building models that generate probability distributions over features given a class label."
    },
    {
      "id": "Naive_Bayes_Assumption",
      "type": "subnode",
      "parent": "Generative_Modeling",
      "description": "Assumption of conditional independence between features given the class label."
    },
    {
      "id": "Conditional_Independence",
      "type": "subnode",
      "parent": "Naive_Bayes_Assumption",
      "description": "Features are independent given the class, but not necessarily independent overall."
    },
    {
      "id": "ConditionalProbability",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "The probability of a feature given the class label, key to Naive Bayes classification."
    },
    {
      "id": "JointLikelihood",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Product of individual likelihoods for each training example under the model parameters."
    },
    {
      "id": "PredictionCalculation",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Method for predicting class labels using Bayes' theorem and estimated probabilities."
    },
    {
      "id": "BinaryFeatures",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Assumption that features take binary values, though Naive Bayes can be extended to other types."
    },
    {
      "id": "MultinomialFeatures",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Generalization to features taking values in {1,2,...,k_j} modeled as multinomial distributions."
    },
    {
      "id": "LaplaceSmoothing",
      "type": "major",
      "parent": "ParameterEstimation",
      "description": "Technique to improve Naive Bayes by addressing zero probability problem in text classification."
    },
    {
      "id": "MachineLearningConferences",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning conferences and their significance."
    },
    {
      "id": "NeurIPSConference",
      "type": "subnode",
      "parent": "MachineLearningConferences",
      "description": "Details about the NeurIPS conference including submission deadlines."
    },
    {
      "id": "NaiveBayesFilter",
      "type": "major",
      "parent": null,
      "description": "Introduction to Naive Bayes filter and its application in email classification."
    },
    {
      "id": "SpamDetection",
      "type": "subnode",
      "parent": "NaiveBayesFilter",
      "description": "Use of Naive Bayes for detecting spam emails based on word frequency."
    },
    {
      "id": "NewWordChallenge",
      "type": "subnode",
      "parent": "SpamDetection",
      "description": "Issues faced by the filter when encountering new words not present in training data."
    },
    {
      "id": "ProbabilityEstimation",
      "type": "major",
      "parent": null,
      "description": "Discussion on estimating probabilities for events unseen during training."
    },
    {
      "id": "Probability Estimation",
      "type": "major",
      "parent": null,
      "description": "Avoiding zero probability estimates in finite training sets."
    },
    {
      "id": "Multinomial Random Variable",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "Random variable taking values from 1 to k."
    },
    {
      "id": "Maximum Likelihood Estimates",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "Estimates based on observed frequencies in training set."
    },
    {
      "id": "Laplace Smoothing",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "Technique to avoid zero probability estimates by adding 1 to numerator and k to denominator."
    },
    {
      "id": "Naive Bayes Classifier",
      "type": "major",
      "parent": null,
      "description": "Classification algorithm using Laplace smoothing for parameter estimation."
    },
    {
      "id": "Event Models for Text Classification",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "Models used in text classification tasks."
    },
    {
      "id": "EventModelsForTextClassification",
      "type": "major",
      "parent": null,
      "description": "Discussion of models specifically for text classification in machine learning."
    },
    {
      "id": "BernoulliEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "Generative model assuming binary presence or absence of words based on class priors."
    },
    {
      "id": "MultinomialEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "A model where each word in an email is generated independently from a multinomial distribution based on the class label."
    },
    {
      "id": "NaiveBayes",
      "type": "subnode",
      "parent": "BernoulliEventModel",
      "description": "Basic Naive Bayes algorithm used as a foundation for more specialized models."
    },
    {
      "id": "SpamNonSpamClassification",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "Determining whether an email is spam or not-spam as the first step in generating an email according to the model."
    },
    {
      "id": "WordGenerationProcess",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "The process of independently selecting each word from a multinomial distribution conditioned on the class label."
    },
    {
      "id": "ProbabilityFormula",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "Overall probability formula for an email, given by p(y) multiplied by the product of probabilities for each word based on its position in the email."
    },
    {
      "id": "ParametersDefinition",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "Definitions of model parameters including phi_y, phi_k|y=1, and phi_k|y=0 for spam and non-spam classes."
    },
    {
      "id": "ParameterEstimation",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "Method for estimating model parameters by maximizing the likelihood function over the training set."
    },
    {
      "id": "NaiveBayesClassifier",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "KernelMethods",
      "type": "major",
      "parent": null,
      "description": "Techniques that extend linear algorithms to handle non-linear data through feature space transformations."
    },
    {
      "id": "FeatureMaps",
      "type": "subnode",
      "parent": "KernelMethods",
      "description": "Transformation of input features into a higher-dimensional space to enable learning of more complex functions."
    },
    {
      "id": "Linear_Functions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Representation of linear functions over transformed feature variables."
    },
    {
      "id": "Cubic_Function",
      "type": "subnode",
      "parent": "Linear_Functions",
      "description": "Example of a cubic function represented as a linear combination of features."
    },
    {
      "id": "Feature_Map",
      "type": "subnode",
      "parent": "Linear_Functions",
      "description": "Transformation from input attributes to feature variables using a feature map."
    },
    {
      "id": "LMS_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Least Mean Squares algorithm for fitting linear models with features."
    },
    {
      "id": "Gradient_Descent_Update",
      "type": "subnode",
      "parent": "LMS_Algorithm",
      "description": "Update rule for gradient descent in the context of feature variables."
    },
    {
      "id": "FeatureMapping",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Transformation of input data into a higher-dimensional space for better separability."
    },
    {
      "id": "HighDimensionalFeatures",
      "type": "subnode",
      "parent": "FeatureMapping",
      "description": "Discussion on the computational challenges posed by high-dimensional feature spaces."
    },
    {
      "id": "KernelTrick",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique to compute inner products of vectors in a high-dimensional space without explicitly mapping them."
    },
    {
      "id": "KernelTrickIntroduction",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Introduces the kernel trick to optimize runtime and memory usage in ML models."
    },
    {
      "id": "PhiFunctionExplanation",
      "type": "subnode",
      "parent": "KernelTrickIntroduction",
      "description": "Explains the function φ(x) used in the context of the kernel trick."
    },
    {
      "id": "ThetaVectorRepresentation",
      "type": "subnode",
      "parent": "KernelTrickIntroduction",
      "description": "Describes how the vector θ can be represented as a linear combination of vectors φ(x^(i))."
    },
    {
      "id": "UpdateRuleDerivation",
      "type": "subnode",
      "parent": "ThetaVectorRepresentation",
      "description": "Details the derivation of update rules for coefficients β_i in relation to vector θ."
    },
    {
      "id": "RuntimeOptimization",
      "type": "subnode",
      "parent": "KernelTrickIntroduction",
      "description": "Explains how runtime can be improved using the kernel trick without explicitly storing θ."
    },
    {
      "id": "Machine_Learning_Algorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms including optimization techniques."
    },
    {
      "id": "Gradient_Descent",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Optimization technique used to minimize loss functions in machine learning models."
    },
    {
      "id": "Beta_Update_Equation",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Equation describing how \beta_i is updated in each iteration of batch gradient descent."
    },
    {
      "id": "Inner_Products",
      "type": "subnode",
      "parent": "Beta_Update_Equation",
      "description": "Computation and pre-computation of inner products between feature vectors."
    },
    {
      "id": "Efficient_Computation",
      "type": "subnode",
      "parent": "Inner_Products",
      "description": "Methods to efficiently compute φ(x),φ(z) without explicitly calculating the feature map."
    },
    {
      "id": "Feature_Maps_and_Kernels",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Exploration of feature maps and their corresponding kernel functions."
    },
    {
      "id": "Kernel_Function_Defined",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Definition and properties of the kernel function."
    },
    {
      "id": "Efficient_Computation_Algorithm",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Algorithm for efficient computation using kernels."
    },
    {
      "id": "Properties_of_Kernels",
      "type": "major",
      "parent": null,
      "description": "Further exploration of properties and implications of kernel functions in machine learning."
    },
    {
      "id": "Kernels_in_Machine_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction to kernel functions in the context of machine learning algorithms."
    },
    {
      "id": "Feature_Map_Phi",
      "type": "subnode",
      "parent": "Kernels_in_Machine Learning",
      "description": "Description and role of feature map phi in defining kernels."
    },
    {
      "id": "Kernel_Function_K",
      "type": "subnode",
      "parent": "Kernels_in_Machine_Learning",
      "description": "Definition and properties of the kernel function K(x,z)."
    },
    {
      "id": "Kernel_Characterization",
      "type": "subnode",
      "parent": "Properties_of_Kernels",
      "description": "Conditions under which a function can be considered a valid kernel function."
    },
    {
      "id": "KernelFunctions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and properties of kernel functions in machine learning."
    },
    {
      "id": "PolynomialKernels",
      "type": "subnode",
      "parent": "KernelFunctions",
      "description": "Description of polynomial kernels, including the squared dot product example."
    },
    {
      "id": "ComputationalEfficiency",
      "type": "subnode",
      "parent": "KernelFunctions",
      "description": "Discussion on the computational efficiency benefits of using kernels over direct computation in high-dimensional spaces."
    },
    {
      "id": "KernelsAsSimilarityMetrics",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Interpretation of kernels as measures of similarity between data points."
    },
    {
      "id": "GaussianKernel",
      "type": "subnode",
      "parent": "KernelsAsSimilarityMetrics",
      "description": "Specific kernel function that corresponds to an infinite-dimensional feature space."
    },
    {
      "id": "ValidKernelConditions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Criteria a function must meet to be considered a valid kernel in machine learning."
    },
    {
      "id": "Kernel Function Properties",
      "type": "major",
      "parent": null,
      "description": "Properties a function K must satisfy to be a valid kernel."
    },
    {
      "id": "Necessary Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Kernel Function Properties",
      "description": "Conditions that must hold if K is a valid kernel."
    },
    {
      "id": "Symmetry of Kernel Matrix",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "K must be symmetric to be a valid kernel."
    },
    {
      "id": "Positive Semi-Definiteness",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "Kernel matrix must be positive semi-definite."
    },
    {
      "id": "Sufficient Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Kernel Function Properties",
      "description": "Conditions that are both necessary and sufficient for a function to be a valid kernel."
    },
    {
      "id": "Kernel Matrix Properties",
      "type": "major",
      "parent": null,
      "description": "Properties and conditions for a kernel matrix to be valid."
    },
    {
      "id": "Mercer's Theorem",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Theorem stating necessary and sufficient condition for a function K to be a Mercer kernel."
    },
    {
      "id": "Feature Mapping",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Method of testing if a given function is a valid kernel through feature mapping."
    },
    {
      "id": "Digit Recognition Example",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Example using polynomial and Gaussian kernels for digit recognition problem."
    },
    {
      "id": "String Classification Example",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Brief discussion on classifying strings as objects with complex structures."
    },
    {
      "id": "FeatureRepresentation",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Techniques for representing data in a form suitable for machine learning models."
    },
    {
      "id": "StringFeatureVectors",
      "type": "subnode",
      "parent": "FeatureRepresentation",
      "description": "Using substring counts as features for string classification problems."
    },
    {
      "id": "SupportVectorMachines",
      "type": "major",
      "parent": null,
      "description": "Algorithm for classification and regression analysis that uses the kernel trick."
    },
    {
      "id": "Kernel_Trick",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Technique for extending linear classifiers to non-linear problems using inner products."
    },
    {
      "id": "Support_Vector_Machines",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithm that uses inner products for efficient learning in high dimensions."
    },
    {
      "id": "Margins_Intuition",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Introduction to margins in SVMs, focusing on confidence of predictions."
    },
    {
      "id": "Logistic_Regression_Analogy",
      "type": "subnode",
      "parent": "Margins_Intuition",
      "description": "Comparison with logistic regression for understanding prediction confidence."
    },
    {
      "id": "Optimal_Margin_Classifier",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Result of solving the optimization problem with convex quadratic objective and linear constraints."
    },
    {
      "id": "Kernels_in_SVMs",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Technique for applying SVMs in high-dimensional spaces efficiently."
    },
    {
      "id": "SMO_Algorithm",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Sequential Minimal Optimization algorithm for solving SVM problems efficiently."
    },
    {
      "id": "Functional Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Concept used to formalize the idea of confident classifications in linear classifiers."
    },
    {
      "id": "Geometric Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Measure of confidence in predictions based on distance from decision boundary."
    },
    {
      "id": "Training Data Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of classifying training data points using a linear classifier."
    },
    {
      "id": "Support Vector Machines (SVMs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Binary classification algorithm focusing on maximizing the margin between classes."
    },
    {
      "id": "Notation for SVMs",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Introduction of notation used in discussing SVMs, including parameters and classifier function."
    },
    {
      "id": "Functional Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Definition of functional margin for a training example with respect to the classifier parameters."
    },
    {
      "id": "Geometric Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Conceptual understanding of geometric margins in SVM context, not fully detailed here."
    },
    {
      "id": "Functional_Margin",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Measure indicating confidence and correctness of predictions for linear classifiers."
    },
    {
      "id": "Confidence_Issue",
      "type": "subnode",
      "parent": "Functional_Margin",
      "description": "Scaling issues with functional margin due to arbitrary scaling of w and b."
    },
    {
      "id": "Normalization_Condition",
      "type": "subnode",
      "parent": "Confidence_Issue",
      "description": "Proposed normalization condition to address confidence issue in functional margin."
    },
    {
      "id": "Function_Margin",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Smallest functional margin across all training examples, indicating overall model performance."
    },
    {
      "id": "Geometric_Margins",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Concept of geometric margins in relation to decision boundaries and hyperplanes."
    },
    {
      "id": "DecisionBoundary",
      "type": "major",
      "parent": null,
      "description": "The boundary that separates different classes in a classification problem."
    },
    {
      "id": "VectorW",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "A vector orthogonal to the decision boundary and pointing towards positive class."
    },
    {
      "id": "DistanceToBoundary",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "The perpendicular distance from a point to the decision boundary."
    },
    {
      "id": "GammaCalculation",
      "type": "subnode",
      "parent": "DistanceToBoundary",
      "description": "Formula for calculating γ using w, b, and x^{(i)}. (w^T(x^{(i)}-γ⋅(w/||w||))+b=0)"
    },
    {
      "id": "GeometricMargin",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "The signed distance from a point to the decision boundary, scaled by the label y^{(i)}. (y^{(i)}((w/||w||)^Tx^{(i)}+b/||w||))"
    },
    {
      "id": "FunctionalMargin",
      "type": "subnode",
      "parent": "GeometricMargin",
      "description": "The margin without normalization, equal to geometric margin if ||w||=1."
    },
    {
      "id": "ParameterScalingInvariance",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "Property that the geometric margin remains unchanged under scaling of w and b."
    },
    {
      "id": "Scaling Invariance",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Invariance of model parameters to scaling constraints."
    },
    {
      "id": "Optimal Margin Classifier",
      "type": "major",
      "parent": null,
      "description": "Classifier that maximizes the geometric margin for linearly separable data sets."
    },
    {
      "id": "Linear Separability",
      "type": "subnode",
      "parent": "Optimal Margin Classifier",
      "description": "Condition where positive and negative examples can be separated by a hyperplane."
    },
    {
      "id": "Maximizing Geometric Margin",
      "type": "subnode",
      "parent": "Optimal Margin Classifier",
      "description": "Objective to find the decision boundary with maximum geometric margin."
    },
    {
      "id": "Support Vector Machines (SVM)",
      "type": "major",
      "parent": "Machine Learning Overview",
      "description": "Binary classification model maximizing margin between classes"
    },
    {
      "id": "Optimization Problem",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Formulation including penalty term for non-linearly separable data."
    },
    {
      "id": "Non-Convex Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint that complicates optimization due to non-convexity"
    },
    {
      "id": "Scaling Constraint",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraint used to simplify the optimization problem by setting functional margin to 1"
    },
    {
      "id": "Machine_Learning_Optimization_Problems",
      "type": "major",
      "parent": null,
      "description": "Optimization problems in machine learning with quadratic objectives and linear constraints."
    },
    {
      "id": "Support_Vector_Machines_SVMs",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization_Problems",
      "description": "Classification algorithm for machine learning that maximizes the margin between classes."
    },
    {
      "id": "Quadratic_Programming_QP",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization_Problems",
      "description": "Software used to solve optimization problems with quadratic objectives and linear constraints."
    },
    {
      "id": "Lagrange_Duality",
      "type": "major",
      "parent": null,
      "description": "Theory explaining the dual form of constrained optimization problems."
    },
    {
      "id": "Constrained_Optimization_Problems",
      "type": "subnode",
      "parent": "Lagrange_Duality",
      "description": "Problems with a function to minimize and constraints in the form of equalities."
    },
    {
      "id": "Lagrangian_Function",
      "type": "subnode",
      "parent": "Constrained_Optimization_Problems",
      "description": "Combination of objective function and constraint functions using Lagrange multipliers."
    },
    {
      "id": "Lagrange_Multipliers",
      "type": "subnode",
      "parent": "Constrained_Optimization_Problems",
      "description": "Multipliers used to handle constraints in the optimization problem."
    },
    {
      "id": "Dual_Form",
      "type": "subnode",
      "parent": "Lagrange_Duality",
      "description": "Alternative form of optimization problem allowing efficient solutions and kernel usage."
    },
    {
      "id": "ConstrainedOptimization",
      "type": "major",
      "parent": null,
      "description": "Generalization of optimization problems with equality and inequality constraints."
    },
    {
      "id": "LagrangeMultipliers",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Scalars used to incorporate constraints into the Lagrangian function."
    },
    {
      "id": "PrimalProblem",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Minimizing a function subject to inequality and equality constraints."
    },
    {
      "id": "GeneralizedLagrangian",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Combination of the objective function with Lagrange multipliers for constraints."
    },
    {
      "id": "ThetaP",
      "type": "subnode",
      "parent": "GeneralizedLagrangian",
      "description": "Function that evaluates to f(w) if w satisfies primal constraints, else infinity."
    },
    {
      "id": "Primal Problem",
      "type": "major",
      "parent": null,
      "description": "Optimization problem defined by maximizing Lagrangian with respect to alpha and beta under primal constraints."
    },
    {
      "id": "Dual Problem",
      "type": "major",
      "parent": null,
      "description": "Optimization problem derived from the primal problem where minimization over w is performed first before maximization over alpha and beta."
    },
    {
      "id": "Objective Value Primal",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Defined as p* which is the minimum value of theta_P(w) under primal constraints."
    },
    {
      "id": "Objective Value Dual",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Defined as d* which is the maximum value of theta_D(alpha, beta) over alpha and beta satisfying dual constraints."
    },
    {
      "id": "Lagrangian Function",
      "type": "subnode",
      "parent": null,
      "description": "Function L(w, alpha, beta) central to both primal and dual formulations."
    },
    {
      "id": "Primal Constraints",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Constraints that must be satisfied for the primal problem's solution."
    },
    {
      "id": "Dual Constraints",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Constraints on alpha and beta in the dual formulation, specifically alpha_i >= 0."
    },
    {
      "id": "Machine_Learning_Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimization techniques in machine learning."
    },
    {
      "id": "Primal_Dual_Problems",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Relationship between primal and dual optimization problems."
    },
    {
      "id": "d_star_p_star_Equality",
      "type": "subnode",
      "parent": "Primal_Dual_Problems",
      "description": "Conditions under which the optimal values of primal and dual problems are equal."
    },
    {
      "id": "Convexity_Assumptions",
      "type": "subnode",
      "parent": "d_star_p_star_Equality",
      "description": "Assumptions about convex functions for equality to hold."
    },
    {
      "id": "Feasibility_Conditions",
      "type": "subnode",
      "parent": "d_star_p_star_Equality",
      "description": "Conditions ensuring strict feasibility of constraints."
    },
    {
      "id": "KKT_Conditions",
      "type": "subnode",
      "parent": "Primal_Dual_Problems",
      "description": "Karush-Kuhn-Tucker conditions for optimality in constrained optimization problems."
    },
    {
      "id": "Optimization_Problems",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Challenges in solving optimization problems for model parameters."
    },
    {
      "id": "Dual_Complementarity",
      "type": "subnode",
      "parent": "KKT_Conditions",
      "description": "Condition indicating active constraints in optimization."
    },
    {
      "id": "Support_Vectors_SVM",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Key concept in Support Vector Machines (SVM)."
    },
    {
      "id": "Primal_Dual_Formulation",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Equivalence between primal and dual optimization formulations in ML."
    },
    {
      "id": "Support_Vectors",
      "type": "major",
      "parent": "Machine_Learning_Concepts",
      "description": "Training examples that define the decision boundary in SVMs."
    },
    {
      "id": "Decision_Boundary",
      "type": "subnode",
      "parent": "Support_Vectors",
      "description": "Hyperplane separating positive and negative classes in SVM problems."
    },
    {
      "id": "Alpha_i",
      "type": "subnode",
      "parent": "Support_Vectors",
      "description": "Lagrange multipliers corresponding to support vectors."
    },
    {
      "id": "Dual_Formulation",
      "type": "subnode",
      "parent": "Kernel_Trick",
      "description": "Form of optimization problem expressed in terms of Lagrange multipliers and support vectors."
    },
    {
      "id": "Lagrangian",
      "type": "subnode",
      "parent": "Dual_Formulation",
      "description": "Function used to find the optimal solution by combining the objective function with constraints using Lagrange multipliers."
    },
    {
      "id": "Lagrangian_Formulation",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Formulating optimization problems using Lagrangians"
    },
    {
      "id": "Dual_Problem",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Deriving and solving the dual problem for optimization"
    },
    {
      "id": "Primal_Dual_Relationship",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Relationship between primal and dual optimization problems"
    },
    {
      "id": "Optimal_W_Value",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Finding the optimal value for w in a model."
    },
    {
      "id": "Intercept_Term_B",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Calculating the intercept term b based on the optimal w value."
    },
    {
      "id": "Prediction_Equation",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Equation for making predictions using optimized parameters."
    },
    {
      "id": "Dual_Form_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Insight gained from the dual form of optimization problem."
    },
    {
      "id": "Non-separable Case",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Handling datasets that cannot be perfectly separated."
    },
    {
      "id": "\\(\\ell_{1}\\) Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Penalizes the absolute value of coefficients to reduce model complexity."
    },
    {
      "id": "\\(\\mathcal{L}(w,b,\\xi,\\alpha,r)\\)",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Lagrangian function incorporating constraints and multipliers."
    },
    {
      "id": "Support_Vector_Machines_SVM",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Algorithm for classification and regression analysis."
    },
    {
      "id": "Dual_Problem_Formulation",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVM",
      "description": "Formulation of SVM optimization problem in dual form."
    },
    {
      "id": "Sequential_Minimal_Optimization_SMO",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "Efficient method for solving the dual problem in SVM optimization."
    },
    {
      "id": "Coordinate_Ascend_Algorithm",
      "type": "subnode",
      "parent": "Sequential_Minimal_Optimization_SMO",
      "description": "Optimization technique used to solve unconstrained problems iteratively."
    },
    {
      "id": "Gradient_Ascent_Newtons_Method",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Other optimization methods for solving machine learning problems."
    },
    {
      "id": "Coordinate_Ascend_Method",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "A method for optimizing functions by moving along one coordinate at a time."
    },
    {
      "id": "Quadratic_Function_Optimization",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Method",
      "description": "Optimizing quadratic functions using coordinate ascent."
    },
    {
      "id": "SVM_Optimization_Problem",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "Dual optimization problem to find optimal hyperplane in SVM."
    },
    {
      "id": "Alpha_I_Update",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Process of updating alpha_i values to satisfy constraints in SMO algorithm."
    },
    {
      "id": "Heuristic_Selection",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Process of selecting pairs α_i and α_j to update in the SMO algorithm."
    },
    {
      "id": "Efficient_Update",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Key ideas behind deriving efficient updates in the SMO algorithm."
    },
    {
      "id": "Convergence_Tolerance",
      "type": "subnode",
      "parent": "KKT_Conditions",
      "description": "Parameter _tol used to determine if KKT conditions are satisfied within a certain tolerance level."
    },
    {
      "id": "OptimizationInML",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Discussion on optimization methods used in machine learning problems."
    },
    {
      "id": "AlphaConstraints",
      "type": "subnode",
      "parent": "OptimizationInML",
      "description": "Explains constraints on alpha values for ensuring they lie within a specified range."
    },
    {
      "id": "Equation622",
      "type": "subnode",
      "parent": "OptimizationInML",
      "description": "Describes the equation that expresses \\(\\alpha_1\\) as a function of \\(\\alpha_2\\)."
    },
    {
      "id": "ObjectiveFunctionW",
      "type": "subnode",
      "parent": "OptimizationInML",
      "description": "Details on how the objective function W can be expressed in terms of quadratic functions."
    },
    {
      "id": "MaximizingQuadraticFunctions",
      "type": "subnode",
      "parent": "OptimizationInML",
      "description": "Explains the process to maximize a quadratic function without constraints and with box constraints."
    },
    {
      "id": "Sequential Minimal Optimization (SMO) Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Efficient algorithm to solve the optimization problem in SVM."
    },
    {
      "id": "Alpha Update Process",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO) Algorithm",
      "description": "Process of updating alpha values during SMO."
    },
    {
      "id": "Deep Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to deep learning concepts and neural networks."
    },
    {
      "id": "Supervised Learning with Non-Linear Models",
      "type": "subnode",
      "parent": "Deep Learning Introduction",
      "description": "Exploration of non-linear models in supervised learning context."
    },
    {
      "id": "NonLinearModel",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Abstract non-linear model representation and example."
    },
    {
      "id": "TrainingExamples",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Definition of training examples in machine learning context."
    },
    {
      "id": "RegressionProblems",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Explanation and cost function for regression problems."
    },
    {
      "id": "LeastSquareCostFunction",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Definition of least square cost function for individual examples."
    },
    {
      "id": "MeanSquaredCostFunction",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Definition and explanation of mean squared cost function for dataset."
    },
    {
      "id": "BinaryClassification",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Introduction to binary classification problems in machine learning."
    },
    {
      "id": "Logit",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Output of the linear model before applying the logistic function."
    },
    {
      "id": "ProbabilityFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Converts logit to probability using the sigmoid function."
    },
    {
      "id": "ConditionalDistribution",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Models the conditional distribution of y given x and theta."
    },
    {
      "id": "NegativeLikelihoodLoss",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Calculates negative log-likelihood as loss function for binary classification."
    },
    {
      "id": "TotalLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Averages individual losses over all training examples to get total loss."
    },
    {
      "id": "MultiClassClassification",
      "type": "major",
      "parent": null,
      "description": "Extension of binary classification for multiple classes using softmax function."
    },
    {
      "id": "Loss Function",
      "type": "major",
      "parent": "Backpropagation",
      "description": "Function designed to minimize distance between positive pairs and maximize it for negative pairs."
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Specific form of loss function for probabilistic models."
    },
    {
      "id": "Cross-Entropy Loss",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Loss function used to train the model by minimizing negative log-likelihood."
    },
    {
      "id": "Average Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Total loss averaged over all training examples."
    },
    {
      "id": "Conditional Probabilistic Models",
      "type": "major",
      "parent": null,
      "description": "Models predicting probabilities of outcomes given input data."
    },
    {
      "id": "Optimizers",
      "type": "major",
      "parent": "Pretraining_Phase",
      "description": "Optimization algorithms like SGD or ADAM used for minimizing the pretraining loss."
    },
    {
      "id": "Gradient Descent (GD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Algorithm updating parameters to minimize loss function iteratively."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Optimization algorithm that updates parameters using a single data point at each iteration."
    },
    {
      "id": "Machine Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Collection of algorithms used in machine learning for model training and optimization."
    },
    {
      "id": "Mini-batch Stochastic Gradient Descent",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Variant of SGD where parameter updates are based on small batches of samples for better performance and efficiency."
    },
    {
      "id": "Hyperparameters",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent (SGD)",
      "description": "Parameters like learning rate, number of iterations that control the optimization process."
    },
    {
      "id": "Initialization",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Process of setting initial values for parameters before training starts."
    },
    {
      "id": "Gradient Calculation",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent (SGD)",
      "description": "Exploration of gradient computation efficiency relative to loss function calculation."
    },
    {
      "id": "Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Non-linear models using matrix multiplications and non-linear operations to solve complex problems."
    },
    {
      "id": "Parametrization",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Definition of the model's architecture including weights and biases that define how inputs are transformed into outputs."
    },
    {
      "id": "UnifiedTreatment",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Unified approach for regression and classification problems using neural network output."
    },
    {
      "id": "RegressionProblem",
      "type": "subnode",
      "parent": "UnifiedTreatment",
      "description": "Handling regression problems with direct output from the model."
    },
    {
      "id": "ClassificationProblem",
      "type": "subnode",
      "parent": "UnifiedTreatment",
      "description": "Handling classification problems using logits and softmax functions."
    },
    {
      "id": "NeuralNetworkBasics",
      "type": "major",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to basic components and operations in neural networks."
    },
    {
      "id": "SingleNeuronModel",
      "type": "subnode",
      "parent": "NeuralNetworkBasics",
      "description": "Introduction to a neural network with a single neuron for simple predictions."
    },
    {
      "id": "HousingPricePrediction",
      "type": "subnode",
      "parent": "SingleNeuronModel",
      "description": "Example of predicting housing prices using a single neuron model."
    },
    {
      "id": "ReLUActivationFunction",
      "type": "subnode",
      "parent": "SingleNeuronModel",
      "description": "Explanation and use of the ReLU activation function in neural networks."
    },
    {
      "id": "Activation Function",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Non-linear function applied to neuron output."
    },
    {
      "id": "ReLU",
      "type": "subnode",
      "parent": "Activation Function",
      "description": "Rectified Linear Unit, max(t, 0) function."
    },
    {
      "id": "Bias",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Adjustment term in neuron output calculation."
    },
    {
      "id": "Weight Vector",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Vector of weights for input features."
    },
    {
      "id": "Single Neuron Model",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Model with one neuron and activation function."
    },
    {
      "id": "Stacking Neurons",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Combining multiple neurons to form complex networks."
    },
    {
      "id": "Machine_Learning_Features",
      "type": "major",
      "parent": null,
      "description": "Features used in machine learning models for predicting housing prices."
    },
    {
      "id": "Family_Size",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Derived feature based on house size and number of bedrooms."
    },
    {
      "id": "Walkability",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Derived feature indicating ease of walking to amenities in the neighborhood."
    },
    {
      "id": "School_Quality",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Predicted quality of local elementary school based on zip code and neighborhood wealth."
    },
    {
      "id": "Neural_Network_Input",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Set of input features used in a neural network model for housing prices prediction."
    },
    {
      "id": "Hidden_Units",
      "type": "subnode",
      "parent": "Neural_Network_Input",
      "description": "Intermediate variables (a1, a2, a3) representing derived features in the neural network."
    },
    {
      "id": "ReLU_Activation",
      "type": "subnode",
      "parent": "Hidden_Units",
      "description": "Activation function used for hidden units to introduce non-linearity."
    },
    {
      "id": "Output_Function",
      "type": "subnode",
      "parent": "Neural_Network_Input",
      "description": "Final output of the neural network model, a linear combination of intermediate variables."
    },
    {
      "id": "NeuralNetworkParameters",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of parameters in a neural network model."
    },
    {
      "id": "BiologicalInspiration",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Connection between artificial and biological neural networks."
    },
    {
      "id": "TwoLayerNetworks",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of two-layer fully-connected neural networks."
    },
    {
      "id": "NeuralNetworks",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Overview of neural networks and their components."
    },
    {
      "id": "FullyConnectedNN",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Description of fully-connected neural network architecture."
    },
    {
      "id": "IntermediateVariables",
      "type": "subnode",
      "parent": "FullyConnectedNN",
      "description": "Computation of intermediate variables u[0], u[1],..., u[k] used in forward pass calculations."
    },
    {
      "id": "Vectorization",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Combining operations into a single unified formulation using matrix notation."
    },
    {
      "id": "VectorizationInMachineLearning",
      "type": "major",
      "parent": null,
      "description": "The process of converting algorithms into matrix operations for efficiency."
    },
    {
      "id": "EfficiencyConcerns",
      "type": "subnode",
      "parent": "VectorizationInMachineLearning",
      "description": "Discussion on the inefficiencies of using loops in neural network implementations."
    },
    {
      "id": "ParallelismAndGPUs",
      "type": "subnode",
      "parent": "VectorizationInMachineLearning",
      "description": "The importance of leveraging GPU parallelism for deep learning efficiency."
    },
    {
      "id": "MatrixAlgebra",
      "type": "subnode",
      "parent": "VectorizationInMachineLearning",
      "description": "Use of matrix algebra to optimize neural network computations."
    },
    {
      "id": "BLASOptimization",
      "type": "subnode",
      "parent": "VectorizationInMachineLearning",
      "description": "Utilizing highly optimized numerical linear algebra packages for speed."
    },
    {
      "id": "TwoLayerNetworkExample",
      "type": "subnode",
      "parent": "VectorizationInMachineLearning",
      "description": "An example of vectorizing a two-layer fully-connected neural network."
    },
    {
      "id": "WeightMatrices",
      "type": "subnode",
      "parent": "NeuralNetworkBasics",
      "description": "Description of weight matrices used in neural network layers."
    },
    {
      "id": "BiasVectors",
      "type": "subnode",
      "parent": "NeuralNetworkBasics",
      "description": "Explanation of bias vectors and their role in neural networks."
    },
    {
      "id": "ActivationFunctions",
      "type": "subnode",
      "parent": "NeuralNetworkBasics",
      "description": "Various functions used in neural networks to introduce non-linearity."
    },
    {
      "id": "LayerOperations",
      "type": "subnode",
      "parent": "NeuralNetworkBasics",
      "description": "Description of operations performed in each layer of a neural network."
    },
    {
      "id": "TwoLayerNN",
      "type": "subnode",
      "parent": "NeuralNetworkBasics",
      "description": "Definition and explanation of a two-layer (one hidden layer) neural network."
    },
    {
      "id": "MultiLayerNN",
      "type": "subnode",
      "parent": "NeuralNetworkBasics",
      "description": "Explanation of multi-layer fully-connected neural networks."
    },
    {
      "id": "Multi-layer Fully-Connected Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Networks with multiple layers of neurons connected fully to each other."
    },
    {
      "id": "Weight Matrices and Biases",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Matrices W[k] and biases b[k] for layer k."
    },
    {
      "id": "ReLU Activation Function",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Rectified Linear Unit function used in hidden layers."
    },
    {
      "id": "Total Neurons and Parameters",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Sum of neurons and parameters across all layers."
    },
    {
      "id": "Notational Consistency",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Consistent notation for input and output layers."
    },
    {
      "id": "Other Activation Functions",
      "type": "major",
      "parent": null,
      "description": "Alternative non-linear functions to ReLU in neural networks."
    },
    {
      "id": "TanhFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Similar to sigmoid but outputs values between -1 and 1."
    },
    {
      "id": "ReLUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Outputs the input directly if positive, otherwise outputs zero."
    },
    {
      "id": "LeakyReLU",
      "type": "subnode",
      "parent": "ReLUFunction",
      "description": "A variant of ReLU that allows a small gradient when the unit is not active."
    },
    {
      "id": "GELUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Smoothly approximates the ReLU function with a Gaussian distribution."
    },
    {
      "id": "SoftplusFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "A smooth approximation of the rectifier function, ensuring differentiability."
    },
    {
      "id": "IdentityFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Outputs the input directly without any transformation."
    },
    {
      "id": "GradientVanishing",
      "type": "subnode",
      "parent": "SigmoidFunction",
      "description": "Problem where gradients become very small, hindering learning in deep networks."
    },
    {
      "id": "Feature_Engineering",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of selecting and transforming raw data into features for use in machine learning models."
    },
    {
      "id": "Deep_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Subfield focusing on neural networks with many layers to learn complex representations from data."
    },
    {
      "id": "Feature_Maps",
      "type": "subnode",
      "parent": "Feature_Engineering",
      "description": "Functions that map raw input data into a new feature space for better model performance."
    },
    {
      "id": "Neural_Networks",
      "type": "subnode",
      "parent": "Deep_Learning",
      "description": "Models composed of layers of interconnected nodes (neurons) to simulate the human brain's learning process."
    },
    {
      "id": "Parameters_Beta",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Collection of parameters excluding those in the last layer of a neural network."
    },
    {
      "id": "Feature_Space",
      "type": "subnode",
      "parent": "Feature_Maps",
      "description": "Abstract space where features are represented, often used for linear models."
    },
    {
      "id": "Learned_Features",
      "type": "subnode",
      "parent": "Deep_Learning",
      "description": "Features automatically discovered by deep learning models in the penultimate layer of neural networks."
    },
    {
      "id": "Deep_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in deep learning including neural network representations."
    },
    {
      "id": "House_Price_Prediction",
      "type": "subnode",
      "parent": "Deep_Learning_Concepts",
      "description": "Example illustrating the use of fully-connected neural networks for predicting house prices."
    },
    {
      "id": "Feature_Discovery",
      "type": "subnode",
      "parent": "House_Price_Prediction",
      "description": "Automatic discovery of useful features by neural networks in prediction tasks."
    },
    {
      "id": "Black_Box_Models",
      "type": "subnode",
      "parent": "Deep_Learning_Concepts",
      "description": "Neural networks are often referred to as black boxes due to difficulty in interpreting discovered features."
    },
    {
      "id": "Modern_Neural_Network_Modules",
      "type": "major",
      "parent": null,
      "description": "Introduction to various building blocks and layers used in modern neural network architectures."
    },
    {
      "id": "Matrix_Multiplication_Module",
      "type": "subnode",
      "parent": "Modern_Neural_Network_Modules",
      "description": "Building block representing a matrix multiplication operation with parameters W and b."
    },
    {
      "id": "MLP_Architecture",
      "type": "subnode",
      "parent": "Modern_Neural_Network_Modules",
      "description": "Multi-layer perceptron architecture composed of multiple matrix multiplication and nonlinear activation modules."
    },
    {
      "id": "MLPArchitecture",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Describes the architecture of a Multi-Layer Perceptron (MLP)."
    },
    {
      "id": "MatrixMultiplicationModule",
      "type": "subnode",
      "parent": "MLPArchitecture",
      "description": "Component of MLP that performs matrix multiplication."
    },
    {
      "id": "NonlinearActivationModule",
      "type": "subnode",
      "parent": "MLPArchitecture",
      "description": "Component of MLP that applies nonlinear activation functions."
    },
    {
      "id": "ResNetArchitecture",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Deep residual network architecture using convolution layers and batch normalization."
    },
    {
      "id": "ResidualBlock",
      "type": "subnode",
      "parent": "ResNetArchitecture",
      "description": "Building block in ResNet that includes residual connections."
    },
    {
      "id": "SimplifiedResNet",
      "type": "subnode",
      "parent": "ResNetArchitecture",
      "description": "A simplified version of ResNet using multiple residual blocks."
    },
    {
      "id": "MachineLearningArchitectures",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning architectures including ResNet and Transformer."
    },
    {
      "id": "TransformerArchitecture",
      "type": "subnode",
      "parent": "MachineLearningArchitectures",
      "description": "Modern architecture for large language models, including ResNet-S and layer normalization."
    },
    {
      "id": "LayerNormalization",
      "type": "subnode",
      "parent": "ResNetArchitecture",
      "description": "Module that normalizes a vector to have mean zero and standard deviation one."
    },
    {
      "id": "LN_S_Module",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Sub-module of layer normalization, normalizing each element by empirical mean and standard deviation."
    },
    {
      "id": "LearnableParameters",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Scalars \\(\\beta\\) and \\(\\gamma\\) used for shifting and scaling the normalized input vector."
    },
    {
      "id": "LN-S",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Standardized version of the input vector before scaling and shifting."
    },
    {
      "id": "AffineTransformation",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Transforms standardized inputs using learnable parameters to achieve desired mean and standard deviation."
    },
    {
      "id": "ScalingInvariantProperty",
      "type": "major",
      "parent": null,
      "description": "Property ensuring model invariance to parameter scaling within subsequent modules."
    },
    {
      "id": "Normalization Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for adjusting data to a common scale and mean."
    },
    {
      "id": "Layer Normalization (LN)",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Normalizes the input of each layer for stable training."
    },
    {
      "id": "Scale-Invariant Property",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Property where scaling weights doesn't affect network output except in last layer."
    },
    {
      "id": "Batch Normalization (BN)",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Normalizes the input of each mini-batch for stable training, common in computer vision."
    },
    {
      "id": "Group Normalization",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Similar to batch normalization but normalizes groups of channels."
    },
    {
      "id": "Convolutional Layers",
      "type": "major",
      "parent": null,
      "description": "Layers in neural networks that perform convolution operations for feature extraction."
    },
    {
      "id": "1-D Convolution",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Type of convolution used primarily in natural language processing."
    },
    {
      "id": "2-D Convolution",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Type of convolution suitable for image data with two dimensions."
    },
    {
      "id": "Convolutional_Neural_Networks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Neural networks that use convolution operations for feature extraction."
    },
    {
      "id": "1D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "One-dimensional convolution used in sequence data processing."
    },
    {
      "id": "2D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "Two-dimensional convolution commonly used for image processing."
    },
    {
      "id": "Natural_Language_Processing",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Application of machine learning techniques to text and speech data."
    },
    {
      "id": "Conv1D_Simplified",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Simplified version of 1-D convolution layer for matrix multiplication with shared parameters."
    },
    {
      "id": "Filter_Vector",
      "type": "subnode",
      "parent": "Conv1D_Simplified",
      "description": "Vector used in Conv1D-S that defines the filter size and coefficients."
    },
    {
      "id": "Bias_Scalar",
      "type": "subnode",
      "parent": "Conv1D_Simplified",
      "description": "Scalar value added to each output dimension of the convolution layer."
    },
    {
      "id": "Matrix_Multiplication_Convolution",
      "type": "subnode",
      "parent": "Conv1D_Simplified",
      "description": "Representation of 1-D convolution as a matrix multiplication with shared parameters."
    },
    {
      "id": "Convolutional_Layers",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on convolutional layers used in neural networks."
    },
    {
      "id": "Parameter_Sharing",
      "type": "subnode",
      "parent": "Convolutional_Layers",
      "description": "Explanation of parameter sharing in convolutional operations."
    },
    {
      "id": "Efficiency_of_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Layers",
      "description": "Comparison between the efficiency of convolution and generic matrix multiplication."
    },
    {
      "id": "Channel_Support_in_Conv1D",
      "type": "subnode",
      "parent": "Convolutional_Layers",
      "description": "Description of how Conv1D supports multiple channels in input/output."
    },
    {
      "id": "Conv1D",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "One-dimensional convolution operation in neural networks with multiple channels."
    },
    {
      "id": "Conv2D",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Two-dimensional convolution operation in neural networks, extending Conv1D to 2D inputs."
    },
    {
      "id": "Differentiable Circuit",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Composition of arithmetic operations and elementary functions."
    },
    {
      "id": "Gradient Computation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Computation of gradients with respect to hidden activations and parameters."
    },
    {
      "id": "Machine Learning Fundamentals",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and techniques."
    },
    {
      "id": "Loss Function Computation",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Discussion on computing loss functions in neural networks."
    },
    {
      "id": "Backpropagation Overview",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Introduction to backpropagation and its implementation in deep learning packages."
    },
    {
      "id": "Chain Rule Perspective",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "New perspective on the chain rule for understanding backpropagation."
    },
    {
      "id": "General Backprop Strategy",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "Introduction to general strategies used in backpropagation algorithms."
    },
    {
      "id": "Basic Modules Backward Function",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "Discussion on computing backward functions for basic neural network modules."
    },
    {
      "id": "Concrete Backprop Algorithm",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "Detailed backpropagation algorithm for MLPs (Multi-Layer Perceptrons)."
    },
    {
      "id": "Partial Derivatives Basics",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Introduction to partial derivatives and their dimensions in the context of machine learning."
    },
    {
      "id": "Partial_Derivatives",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Explanation and notation for partial derivatives in multi-variate functions."
    },
    {
      "id": "Chain_Rule",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Review of the chain rule with focus on auto-differentiation."
    },
    {
      "id": "Scalar_Functions",
      "type": "subnode",
      "parent": "Partial_Derivatives",
      "description": "Discussion on derivatives involving scalar functions and vector/matrix/tensor variables."
    },
    {
      "id": "Mathematical_Notations",
      "type": "subnode",
      "parent": "Partial_Derivatives",
      "description": "Challenges in notation for partial derivatives of multi-variate functions."
    },
    {
      "id": "Computational_Efficiency",
      "type": "subnode",
      "parent": "Partial_Derivatives",
      "description": "Issues related to computational cost and storage efficiency when dealing with high-dimensional derivatives."
    },
    {
      "id": "Machine_Learning_Backward_Propagation",
      "type": "major",
      "parent": null,
      "description": "Overview of backward propagation in machine learning"
    },
    {
      "id": "Chain_Rule_Application",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Propagation",
      "description": "Application of the chain rule to compute gradients efficiently."
    },
    {
      "id": "Jacobian_Matrix_Implications",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Propagation",
      "description": "Implications and simplifications related to Jacobian matrices"
    },
    {
      "id": "Gradient_Computation_Simplified",
      "type": "subnode",
      "parent": "Machine_Learning_Backward_Propagation",
      "description": "Simpler methods for gradient computation in complex scenarios"
    },
    {
      "id": "Chain Rule",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Rule for computing derivatives in compositions of functions."
    },
    {
      "id": "Loss Function Composition",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Abstract representation of loss functions as compositions of modules."
    },
    {
      "id": "BinaryClassificationProblem",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A specific problem involving binary classification using a neural network model."
    },
    {
      "id": "MLPModel",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "Multilayer Perceptron (MLP) used in the context of solving a binary classification problem."
    },
    {
      "id": "LossFunction",
      "type": "subnode",
      "parent": "BinaryClassificationProblem",
      "description": "The loss function formulated for the MLP model to quantify prediction error."
    },
    {
      "id": "ModulesInNetwork",
      "type": "subnode",
      "parent": "MLPModel",
      "description": "Various modules (M1, M2,...) involved in the network architecture with parameters and operations."
    },
    {
      "id": "ForwardPass",
      "type": "subnode",
      "parent": "ModulesInNetwork",
      "description": "Sequential computation process during the forward propagation step."
    },
    {
      "id": "BackwardPass",
      "type": "subnode",
      "parent": "ModulesInNetwork",
      "description": "Computation of gradients for parameters and intermediate variables in reverse order during backpropagation."
    },
    {
      "id": "DerivativesCalculation",
      "type": "subnode",
      "parent": "BackwardPass",
      "description": "Calculation of derivatives with respect to the loss function and model parameters."
    },
    {
      "id": "Machine_Learning_Backpropagation",
      "type": "major",
      "parent": null,
      "description": "Overview of backpropagation in machine learning."
    },
    {
      "id": "Gradient_Computation",
      "type": "subnode",
      "parent": "Chain_Rule_Application",
      "description": "Computation of gradients with respect to intermediate variables and parameters."
    },
    {
      "id": "Efficient_Backward_Propagation",
      "type": "subnode",
      "parent": "Machine_Learning_Backpropagation",
      "description": "Discussion on efficient computation of backward functions for small modules."
    },
    {
      "id": "Backward_Functions_Basic_Modules",
      "type": "subnode",
      "parent": "Machine_Learning_Backpropagation",
      "description": "Discussion on computing backward functions for basic modules such as matrix multiplication and activations."
    },
    {
      "id": "Matrix_Multiplication_Module_MM",
      "type": "subnode",
      "parent": "Backward_Functions_Basic_Modules",
      "description": "Details of the backward function computation for a matrix multiplication module."
    },
    {
      "id": "Activation_Backward_Function",
      "type": "subnode",
      "parent": "Backward_Functions_Basic_Modules",
      "description": "Overview of computing backward functions for activation modules."
    },
    {
      "id": "Loss_Function_Backward",
      "type": "subnode",
      "parent": "Backward_Functions_Basic_Modules",
      "description": "Details on the computation of backward functions for loss functions."
    },
    {
      "id": "Backward Function Overview",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Explanation of backward functions in the context of machine learning modules."
    },
    {
      "id": "W Variable Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Detailed explanation and equation for the backward function concerning variable W."
    },
    {
      "id": "b Variable Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Explanation of the backward function related to bias b."
    },
    {
      "id": "Activation Functions Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Description and equation for the backward function concerning activation functions."
    },
    {
      "id": "Computational Efficiency",
      "type": "subnode",
      "parent": "W Variable Backward Function",
      "description": "Discussion on computational efficiency of computing the backward function."
    },
    {
      "id": "Efficiency Considerations",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on computational efficiency and simplifications for backward functions."
    },
    {
      "id": "Activation Functions",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Derivation of the derivative of activation functions in vectorized form."
    },
    {
      "id": "Loss Functions Backward Pass",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Explanation of backward pass for different loss functions such as MSE, logistic, and cross-entropy."
    },
    {
      "id": "Squared Loss (MSE)",
      "type": "subnode",
      "parent": "Loss Functions Backward Pass",
      "description": "Derivation of the backward function for squared loss or mean squared error loss."
    },
    {
      "id": "Logistic Loss",
      "type": "subnode",
      "parent": "Loss Functions Backward Pass",
      "description": "Explanation and derivation of the backward pass for logistic loss."
    },
    {
      "id": "Machine Learning Loss Functions",
      "type": "major",
      "parent": null,
      "description": "Overview of loss functions used in machine learning."
    },
    {
      "id": "Logistic Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Loss Functions",
      "description": "Binary classification loss function."
    },
    {
      "id": "Cross-Entropy Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Loss Functions",
      "description": "Commonly used in multi-class classification problems."
    },
    {
      "id": "Forward Pass Operations",
      "type": "subnode",
      "parent": "Back-propagation for MLPs",
      "description": "Sequence of operations to compute the loss function."
    },
    {
      "id": "BackPropagationAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Details on the backpropagation algorithm including gradient computation."
    },
    {
      "id": "GradientComputation",
      "type": "subnode",
      "parent": "BackPropagationAlgorithm",
      "description": "Challenges and methods for computing gradients in variational inference problems."
    },
    {
      "id": "IntermediateValuesStorage",
      "type": "subnode",
      "parent": "BackPropagationAlgorithm",
      "description": "Storing intermediate values like a^[i] and z^[i] after forward pass."
    },
    {
      "id": "VectorizationTrainingExamples",
      "type": "major",
      "parent": null,
      "description": "Discussion on vectorizing over training examples in neural networks."
    },
    {
      "id": "ParallelismAcrossExamples",
      "type": "subnode",
      "parent": "VectorizationTrainingExamples",
      "description": "Leveraging parallelism across multiple training examples for forward and backward passes."
    },
    {
      "id": "TrainingExamplesMatrixNotation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Representation of training examples using matrix notation."
    },
    {
      "id": "LayerActivations",
      "type": "subnode",
      "parent": "TrainingExamplesMatrixNotation",
      "description": "First-layer activations for each example in a neural network."
    },
    {
      "id": "Broadcasting",
      "type": "subnode",
      "parent": "Vectorization",
      "description": "Technique used to add a vector to each column of a matrix in practice."
    },
    {
      "id": "Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Study of algorithms and statistical models for computer systems to perform tasks without explicit instructions."
    },
    {
      "id": "Deep Learning Implementation",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Details on the practical implementation of deep learning models."
    },
    {
      "id": "Matricization Approach",
      "type": "subnode",
      "parent": "Deep Learning Implementation",
      "description": "Method to represent data and operations in matrix form for multi-layer neural networks."
    },
    {
      "id": "Data Representation",
      "type": "subnode",
      "parent": "Deep Learning Implementation",
      "description": "Discussion on how data points are represented as rows or columns in matrices."
    },
    {
      "id": "Generalization",
      "type": "major",
      "parent": "Optimizers",
      "description": "How different optimizer settings affect model generalization performance."
    },
    {
      "id": "Training Loss Function",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Function used to measure and minimize error during the training phase."
    },
    {
      "id": "Machine_Learning_Fundamentals",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning."
    },
    {
      "id": "Loss_Functions",
      "type": "subnode",
      "parent": "Machine_Learning_Fundamentals",
      "description": "Functions used to measure the performance of a model during training and testing."
    },
    {
      "id": "Training_Loss",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Measure of how well a model fits the training data."
    },
    {
      "id": "Test_Error",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Evaluation metric for assessing model performance on unseen test examples."
    },
    {
      "id": "Empirical_Distribution",
      "type": "subnode",
      "parent": "Training_Loss",
      "description": "Distribution of training data used to approximate the population distribution."
    },
    {
      "id": "Population_Distribution",
      "type": "subnode",
      "parent": "Test_Error",
      "description": "True underlying distribution from which test examples are drawn."
    },
    {
      "id": "Mean_Squared_Error",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Common loss function used in regression tasks, calculated as the average squared difference between predicted and actual values."
    },
    {
      "id": "Learning Settings",
      "type": "major",
      "parent": null,
      "description": "Settings in which training and test examples are drawn from the same distribution."
    },
    {
      "id": "Domain Shift",
      "type": "subnode",
      "parent": "Learning Settings",
      "description": "Scenario where training and test distributions differ."
    },
    {
      "id": "Test Error vs Training Error",
      "type": "major",
      "parent": null,
      "description": "Difference between errors on unseen data versus seen data."
    },
    {
      "id": "Generalization Gap",
      "type": "subnode",
      "parent": "Test Error vs Training Error",
      "description": "Difference between test error and training error, indicating model's ability to generalize."
    },
    {
      "id": "Bias-Variance Tradeoff",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Decomposition of test error into bias and variance terms for understanding overfitting and underfitting."
    },
    {
      "id": "Double Descent Phenomenon",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Phenomenon where test error decreases after initial decrease then increase with model complexity or sample size."
    },
    {
      "id": "Classical Theoretical Results",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Theoretical findings and proofs related to machine learning models."
    },
    {
      "id": "Training Dataset",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Dataset used for training a model with randomly chosen inputs and noisy outputs."
    },
    {
      "id": "Test Dataset",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Dataset used to evaluate the performance of trained models on unseen data."
    },
    {
      "id": "Linear Regression Models",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Models that predict a linear relationship between input and output variables."
    },
    {
      "id": "Polynomial Models",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Models with higher degree polynomials to capture more complex relationships in data."
    },
    {
      "id": "MachineLearningIssues",
      "type": "major",
      "parent": null,
      "description": "Discusses issues in machine learning models such as underfitting and overfitting."
    },
    {
      "id": "LinearModelLimitations",
      "type": "subnode",
      "parent": "MachineLearningIssues",
      "description": "Explains the limitations of linear models in capturing data structure."
    },
    {
      "id": "BiasDefinition",
      "type": "subnode",
      "parent": "LinearModelLimitations",
      "description": "Defines bias as test error with infinite training dataset."
    },
    {
      "id": "UnderfittingExample",
      "type": "subnode",
      "parent": "LinearModelLimitations",
      "description": "Illustrates underfitting through linear models' inability to capture data structure."
    },
    {
      "id": "PolynomialModelFitting",
      "type": "subnode",
      "parent": "MachineLearningIssues",
      "description": "Discusses fitting a 5th-degree polynomial model to the data."
    },
    {
      "id": "OverfittingExample",
      "type": "subnode",
      "parent": "PolynomialModelFitting",
      "description": "Illustrates overfitting through high-degree polynomials' poor generalization on test data."
    },
    {
      "id": "GeneralizationFailure",
      "type": "subnode",
      "parent": "PolynomialModelFitting",
      "description": "Describes the failure of 5th-degree polynomial models to generalize well despite fitting training data perfectly."
    },
    {
      "id": "5th Degree Polynomial",
      "type": "subnode",
      "parent": "Overfitting",
      "description": "Example of a complex model that can overfit."
    },
    {
      "id": "Variance",
      "type": "subnode",
      "parent": "Overfitting",
      "description": "Measure of how much the model changes with different training datasets."
    },
    {
      "id": "Model Complexity",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Degree to which a model can capture patterns in data, impacting bias and variance."
    },
    {
      "id": "Test Error Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Formal breakdown of test error into bias and variance components."
    },
    {
      "id": "BiasVarianceTradeoff",
      "type": "subnode",
      "parent": "RegressionProblems",
      "description": "Discussion on balancing model complexity to minimize error."
    },
    {
      "id": "TrainingDataset",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Description of a dataset used for training models."
    },
    {
      "id": "TestExample",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "An example used to test the trained model's performance."
    },
    {
      "id": "MSECalculation",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Explanation of how MSE is calculated for a given input x."
    },
    {
      "id": "Claim8.1.1",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Mathematical claim used to decompose the MSE into bias and variance terms."
    },
    {
      "id": "DecompositionOfMSE",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Process of breaking down MSE into components for analysis."
    },
    {
      "id": "Model Error Analysis",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Analysis of errors made by a model during prediction."
    },
    {
      "id": "Mean Squared Error (MSE)",
      "type": "subnode",
      "parent": "Model Error Analysis",
      "description": "Measure of the average squared difference between predicted and actual values."
    },
    {
      "id": "Average Model (h_avg)",
      "type": "subnode",
      "parent": "Model Error Analysis",
      "description": "Theoretical model representing the average prediction over an infinite number of datasets."
    },
    {
      "id": "Bias-Variance Decomposition",
      "type": "subnode",
      "parent": "Mean Squared Error (MSE)",
      "description": "Decomposing MSE into bias and variance components for error analysis."
    },
    {
      "id": "Bias Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Error due to overly simplistic models."
    },
    {
      "id": "Variance Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Model sensitivity to dataset randomness."
    },
    {
      "id": "Noise (σ^2)",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Unpredictable error component in data."
    },
    {
      "id": "Model-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Peak in model complexity when number of parameters is similar to data size."
    },
    {
      "id": "Sample-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Test error pattern observed as sample size increases relative to model complexity."
    },
    {
      "id": "Overparameterized Models",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Models with more parameters than necessary, showing a decrease in test error after initial increase."
    },
    {
      "id": "Historical Context",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Discovery of the phenomenon by Opper and recent popularization by Belkin et al., Hastie et al."
    },
    {
      "id": "Explanation and Mitigation Strategy",
      "type": "major",
      "parent": null,
      "description": "Explains the peak and suggests strategies for improvement."
    },
    {
      "id": "Optimal Regularization",
      "type": "subnode",
      "parent": "Explanation and Mitigation Strategy",
      "description": "Regularization tuning can mitigate double descent peaks."
    },
    {
      "id": "Implicit Regularization",
      "type": "major",
      "parent": "Optimizers",
      "description": "Explains how optimizers like gradient descent regularize overparameterized models implicitly."
    },
    {
      "id": "Gradient Descent Optimizer",
      "type": "subnode",
      "parent": "Implicit Regularization",
      "description": "Provides implicit regularization effect in overparameterized regimes."
    },
    {
      "id": "GradientDescentOptimizer",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Optimization technique used to minimize loss functions."
    },
    {
      "id": "MinimumNormSolution",
      "type": "subnode",
      "parent": "GradientDescentOptimizer",
      "description": "The solution with the smallest norm that fits the data."
    },
    {
      "id": "OverparameterizedRegime",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Scenario where model complexity exceeds training data dimensionality."
    },
    {
      "id": "DoubleDescentPhenomenon",
      "type": "major",
      "parent": null,
      "description": "Observation of increasing test error before decreasing again with increased model complexity."
    },
    {
      "id": "ModelComplexityMeasures",
      "type": "subnode",
      "parent": "DoubleDescentPhenomenon",
      "description": "Different ways to measure the complexity of a machine learning model."
    },
    {
      "id": "NumberofParameters",
      "type": "subnode",
      "parent": "ModelComplexityMeasures",
      "description": "Common metric for measuring model complexity."
    },
    {
      "id": "NormOfModels",
      "type": "subnode",
      "parent": "ModelComplexityMeasures",
      "description": "Alternative measure of model complexity based on the norm of learned models."
    },
    {
      "id": "Sample Complexity Bounds",
      "type": "major",
      "parent": null,
      "description": "Theoretical bounds on the number of samples needed for learning tasks."
    },
    {
      "id": "Model Selection Methods",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Techniques for choosing the best model based on training data performance."
    },
    {
      "id": "Generalization Error",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Error of a model when applied to unseen data, crucial in machine learning evaluation."
    },
    {
      "id": "Learning Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical framework for understanding the performance of learning algorithms."
    },
    {
      "id": "Union Bound Lemma",
      "type": "subnode",
      "parent": "Learning Theory",
      "description": "Lemma providing an upper bound on the probability of multiple events occurring."
    },
    {
      "id": "Learning Theory Proofs",
      "type": "major",
      "parent": null,
      "description": "Conditions and lemmas for proving learning algorithm effectiveness."
    },
    {
      "id": "Hoeffding Inequality",
      "type": "subnode",
      "parent": "Learning Theory Proofs",
      "description": "Bound for deviation between sample mean and true probability in Bernoulli distribution."
    },
    {
      "id": "Machine_Learning_Basics",
      "type": "major",
      "parent": null,
      "description": "Introduction to fundamental concepts in machine learning."
    },
    {
      "id": "Hypothesis_Function",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Function representing the model's predictions based on input data."
    },
    {
      "id": "Training_Error",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Measure of how well a hypothesis performs on the training set."
    },
    {
      "id": "Generalization_Error",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Measure of how well a hypothesis generalizes to new, unseen data."
    },
    {
      "id": "PAC_Framework",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Probably Approximately Correct framework for learning theory assumptions."
    },
    {
      "id": "Empirical_Risk_Minimization",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Process of minimizing training error to fit model parameters."
    },
    {
      "id": "Hypothesis_Class",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Set of all classifiers considered by a learning algorithm."
    },
    {
      "id": "Finite_Hypothesis_Class",
      "type": "subnode",
      "parent": "Empirical_Risk_Minimization",
      "description": "Case where the hypothesis class is finite and consists of k hypotheses."
    },
    {
      "id": "EmpiricalRiskMinimization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of selecting a hypothesis with the smallest training error from a set of hypotheses."
    },
    {
      "id": "GeneralizationErrorGuarantees",
      "type": "subnode",
      "parent": "EmpiricalRiskMinimization",
      "description": "Strategies to ensure that empirical risk minimization leads to low generalization error."
    },
    {
      "id": "ReliableEstimateOfEpsilon",
      "type": "subnode",
      "parent": "GeneralizationErrorGuarantees",
      "description": "Proving that training error is a reliable estimate of true error for all hypotheses."
    },
    {
      "id": "UpperBoundOnGeneralizationError",
      "type": "subnode",
      "parent": "GeneralizationErrorGuarantees",
      "description": "Establishing an upper bound on the generalization error based on reliability of training error."
    },
    {
      "id": "BernoulliRandomVariableZ",
      "type": "subnode",
      "parent": "ReliableEstimateOfEpsilon",
      "description": "Definition and properties of a Bernoulli random variable indicating misclassification by hypothesis h_i."
    },
    {
      "id": "TrainingErrorMean",
      "type": "subnode",
      "parent": "UpperBoundOnGeneralizationError",
      "description": "Expression for training error as the mean of n iid Bernoulli variables with parameter epsilon(h)."
    },
    {
      "id": "HoeffdingInequalityApplication",
      "type": "subnode",
      "parent": "TrainingErrorMean",
      "description": "Use of Hoeffding inequality to bound the probability that true error differs from empirical error by more than gamma."
    },
    {
      "id": "SimultaneousGuaranteesForAllHypotheses",
      "type": "subnode",
      "parent": "UpperBoundOnGeneralizationError",
      "description": "Ensuring that training error is close to generalization error for all hypotheses in the hypothesis set."
    },
    {
      "id": "Uniform_Convergence",
      "type": "major",
      "parent": null,
      "description": "Result that bounds the probability of error for all hypotheses in a class simultaneously."
    },
    {
      "id": "Training_Error_Generalization_Error",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Relationship between training and generalization errors under uniform convergence conditions."
    },
    {
      "id": "Union_Bound_Application",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Use of union bound to extend individual hypothesis error bounds to all hypotheses in a class."
    },
    {
      "id": "Probability_Error_Bounds",
      "type": "subnode",
      "parent": "Training_Error_Generalization_Error",
      "description": "Bounding the probability that training error deviates from generalization error by more than gamma."
    },
    {
      "id": "Quantities_of_Interest",
      "type": "major",
      "parent": null,
      "description": "n (sample size), γ (error margin), and δ (probability of error bound)."
    },
    {
      "id": "Sample_Size_Calculation",
      "type": "subnode",
      "parent": "Quantities_of_Interest",
      "description": "Determining the minimum sample size required to ensure a given probability of error bound with respect to training and generalization errors."
    },
    {
      "id": "MachineLearningTheoreticalBounds",
      "type": "major",
      "parent": null,
      "description": "Overview of theoretical bounds in machine learning."
    },
    {
      "id": "TrainingErrorGeneralizationGap",
      "type": "subnode",
      "parent": "MachineLearningTheoreticalBounds",
      "description": "Relationship between training error and generalization gap."
    },
    {
      "id": "SampleComplexityDefinition",
      "type": "subnode",
      "parent": "MachineLearningTheoreticalBounds",
      "description": "Definition of sample complexity in machine learning."
    },
    {
      "id": "UniformConvergence",
      "type": "subnode",
      "parent": "TrainingErrorGeneralizationGap",
      "description": "Concept of uniform convergence and its implications."
    },
    {
      "id": "HypothesesSpaceK",
      "type": "subnode",
      "parent": "TrainingErrorGeneralizationGap",
      "description": "Role of the number of hypotheses in Ω (k) in determining sample complexity."
    },
    {
      "id": "BestPossibleHypothesis",
      "type": "subnode",
      "parent": "UniformConvergence",
      "description": "Definition and importance of the best possible hypothesis h*."
    },
    {
      "id": "Machine_Learning_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including generalization bounds and hypothesis classes."
    },
    {
      "id": "Generalization_Error_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Bound on the difference between training error and true error for a given hypothesis."
    },
    {
      "id": "Uniform_Convergence_Assumption",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Assumption that the empirical risk over all hypotheses is close to their actual risks with high probability."
    },
    {
      "id": "Hypothesis_Class_Size",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "The size of a hypothesis class impacts generalization error bounds."
    },
    {
      "id": "Bias_Variance_Tradeoff",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Paper discussing reconciliation between modern practice and classical theory on bias-variance trade-off."
    },
    {
      "id": "Machine_Learning_Bias_Variance_Tradeoff",
      "type": "major",
      "parent": null,
      "description": "Exploration of bias and variance in hypothesis class expansion"
    },
    {
      "id": "Hypothesis_Class_Expansion",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Switching to a larger hypothesis class Δ'"
    },
    {
      "id": "Bias_Decrease",
      "type": "subnode",
      "parent": "Hypothesis_Class_Expansion",
      "description": "Reduction in bias by increasing the size of the hypothesis set"
    },
    {
      "id": "Variance_Increase",
      "type": "subnode",
      "parent": "Hypothesis_Class_Expansion",
      "description": "Increase in variance when using a larger hypothesis class"
    },
    {
      "id": "Sample_Complexity_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Bound on sample size required for learning accuracy guarantee"
    },
    {
      "id": "Infinite_Hypothesis_Classes",
      "type": "subnode",
      "parent": "Machine_Learning_Bias_Variance_Tradeoff",
      "description": "Discussion of hypothesis classes parameterized by real numbers"
    },
    {
      "id": "Finite_Approximation",
      "type": "subnode",
      "parent": "Infinite_Hypothesis_Classes",
      "description": "Approximating infinite hypothesis classes using finite precision floating point representation"
    },
    {
      "id": "Sample_Complexity",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Number of training examples required for a model to learn effectively."
    },
    {
      "id": "Empirical_Risk_Minimization_(ERM)",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Learning algorithm that minimizes the empirical risk over training data."
    },
    {
      "id": "Floating_Point_Precision",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Precision used in representing floating point numbers affects sample complexity."
    },
    {
      "id": "Linear_Classifiers",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Class of classifiers that predict based on linear combinations of input features."
    },
    {
      "id": "Hypothesis_Class_Parameterization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Different ways to parameterize a hypothesis class."
    },
    {
      "id": "VC_Dimension_Theory",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Theoretical framework for measuring the capacity of a hypothesis class."
    },
    {
      "id": "Shattering",
      "type": "subnode",
      "parent": "VC_Dimension_Theory",
      "description": "Concept where a hypothesis class can perfectly classify any subset of a given size"
    },
    {
      "id": "VC_Dimension_Definition",
      "type": "subnode",
      "parent": "VC_Dimension_Theory",
      "description": "Definition and calculation method for the Vapnik-Chervonenkis dimension."
    },
    {
      "id": "VC Dimension",
      "type": "major",
      "parent": null,
      "description": "Measure of the capacity of a statistical classification algorithm"
    },
    {
      "id": "Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "Theorem establishing the relationship between VC dimension and generalization error"
    },
    {
      "id": "Generalization Error Bound",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "Bound on the difference between empirical and true errors for a hypothesis class"
    },
    {
      "id": "Uniform Convergence",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "Concept where empirical error converges uniformly to true error as sample size increases"
    },
    {
      "id": "Corollary on Training Examples",
      "type": "major",
      "parent": null,
      "description": "Result stating the number of training examples needed is linear in VC dimension"
    },
    {
      "id": "Chapter9",
      "type": "major",
      "parent": null,
      "description": "Regularization and model selection in machine learning"
    },
    {
      "id": "9.1Regularization",
      "type": "subnode",
      "parent": "Chapter9",
      "description": "Introduction to regularization technique for controlling model complexity"
    },
    {
      "id": "OverfittingComplexity",
      "type": "subnode",
      "parent": "9.1Regularization",
      "description": "Discussion on overfitting and choosing proper model complexity"
    },
    {
      "id": "ModelComplexityMeasure",
      "type": "subnode",
      "parent": "9.1Regularization",
      "description": "Measuring model complexity using parameters or functions of parameters"
    },
    {
      "id": "RegularizerFunction",
      "type": "subnode",
      "parent": "9.1Regularization",
      "description": "Definition and role of the regularizer function in training loss"
    },
    {
      "id": "RegularizedLoss",
      "type": "subnode",
      "parent": "RegularizerFunction",
      "description": "Expression for regularized loss including regularization parameter lambda"
    },
    {
      "id": "Regularized Loss",
      "type": "major",
      "parent": null,
      "description": "Combination of original loss and regularizer to balance model fit and complexity."
    },
    {
      "id": "Original Loss J(θ)",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Loss function that measures how well the model fits the training data."
    },
    {
      "id": "Regularizer R(θ)",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Penalizes model complexity to prevent overfitting, often measured by norm or sparsity."
    },
    {
      "id": "Regularization Parameter λ",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Controls the trade-off between fitting data and reducing model complexity."
    },
    {
      "id": "L2 Regularization",
      "type": "subnode",
      "parent": "Regularizer R(θ)",
      "description": "Penalizes the square of the magnitude of coefficients to reduce model complexity."
    },
    {
      "id": "Weight Decay",
      "type": "subnode",
      "parent": "L2 Regularization",
      "description": "Gradient descent with L2 regularization shrinks weights by a factor of (1-λη)."
    },
    {
      "id": "Sparsity Inducing Regularization",
      "type": "subnode",
      "parent": "Regularizer R(θ)",
      "description": "Encourages model parameters to have few non-zero values, leveraging prior beliefs on parameter sparsity."
    },
    {
      "id": "Regularization in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by adding a penalty for complexity."
    },
    {
      "id": "Sparsity of Model Parameters",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Reduces model complexity by limiting the number of non-zero parameters."
    },
    {
      "id": "L1 Regularization (LASSO)",
      "type": "subnode",
      "parent": "Sparsity of Model Parameters",
      "description": "Promotes sparsity through sum of absolute values of coefficients."
    },
    {
      "id": "Gradient Descent Compatibility",
      "type": "subnode",
      "parent": "Sparsity of Model Parameters",
      "description": "L1 norm is not differentiable and thus incompatible with gradient descent, L2 used as a surrogate."
    },
    {
      "id": "Kernel Methods",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Uses inner products to transform data into higher dimensions for better separation."
    },
    {
      "id": "Deep Learning Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Includes L2 regularization, dropout, and other methods to prevent overfitting in deep networks."
    },
    {
      "id": "Regularization in Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of regularization techniques and their importance in deep learning."
    },
    {
      "id": "Explicit Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitzness regularization."
    },
    {
      "id": "Implicit Regularization Effect",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "The impact of optimizers on model parameters beyond explicit regularization."
    },
    {
      "id": "Classical Settings vs. Deep Learning",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "Comparison between classical settings and deep learning regarding global minima and optimizer convergence."
    },
    {
      "id": "Optimizer Impact on Generalization",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "How optimizers influence the generalization performance of models by preferring certain types of global minima."
    },
    {
      "id": "Learning Rate Schedules",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Impact of learning rate schedules on training and generalization."
    },
    {
      "id": "Batch Size",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Influence of batch size on generalization and training."
    },
    {
      "id": "Momentum",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Role of momentum in biasing towards more generalizable solutions."
    },
    {
      "id": "Cross Validation",
      "type": "major",
      "parent": "Model Selection",
      "description": "Technique to evaluate and select models by splitting data into subsets."
    },
    {
      "id": "Model Selection",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Process of choosing the best model based on cross-validation results."
    },
    {
      "id": "Polynomial Regression Models",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Models with polynomial terms for feature transformation."
    },
    {
      "id": "Regularization Parameters",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Parameters like C in SVM that control model complexity."
    },
    {
      "id": "Model Set M",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Finite set of models to choose from for a given problem."
    },
    {
      "id": "Empirical Risk Minimization",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Method for model selection based on minimizing training error."
    },
    {
      "id": "Hold-out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Method involving splitting data into training and validation sets."
    },
    {
      "id": "Training Set (S)",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Dataset used for training models."
    },
    {
      "id": "Hypothesis Selection",
      "type": "subnode",
      "parent": "Empirical Risk Minimization",
      "description": "Process of selecting hypotheses based on error minimization."
    },
    {
      "id": "Validation Set (S_cv)",
      "type": "subnode",
      "parent": "Hold-out Cross Validation",
      "description": "Subset of data used for validating models after training."
    },
    {
      "id": "Model_Selection",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of choosing the best model among a set of candidate models."
    },
    {
      "id": "Validation_Set",
      "type": "subnode",
      "parent": "Model_Selection",
      "description": "Subset of data used to evaluate and tune model parameters."
    },
    {
      "id": "Hold_Out_Cross_Validation",
      "type": "subnode",
      "parent": "Validation_Set",
      "description": "Method where a portion of the dataset is held out for validation purposes."
    },
    {
      "id": "k_Fold_Cross_Validation",
      "type": "subnode",
      "parent": "Validation_Set",
      "description": "Technique to reduce data wastage by splitting into k subsets and validating iteratively."
    },
    {
      "id": "Machine_Learning_Challenges",
      "type": "major",
      "parent": null,
      "description": "Challenges in machine learning when data is scarce."
    },
    {
      "id": "K_Fold_Cross_Validation",
      "type": "subnode",
      "parent": "Machine_Learning_Challenges",
      "description": "A method to validate models by splitting the dataset into k folds."
    },
    {
      "id": "Leave_One_Out_Cross_Validation",
      "type": "subnode",
      "parent": "K_Fold_Cross_Validation",
      "description": "Special case of K-Fold where each fold size is one data point."
    },
    {
      "id": "Data_Scarcity_Impact",
      "type": "subnode",
      "parent": "Machine_Learning_Challenges",
      "description": "Impact of scarce data on model selection and validation methods."
    },
    {
      "id": "Leave-One-Out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Method where one training example is held out at a time."
    },
    {
      "id": "Bayesian Statistics",
      "type": "major",
      "parent": null,
      "description": "Approach to parameter estimation considering parameters as random variables."
    },
    {
      "id": "Frequentist Approach",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "View of parameters as constant but unknown values."
    },
    {
      "id": "Maximum Likelihood Estimation (MLE)",
      "type": "subnode",
      "parent": "Frequentist Approach",
      "description": "Method for estimating parameter values by maximizing likelihood function."
    },
    {
      "id": "Prior Distribution",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Distribution expressing beliefs about parameters before seeing data."
    },
    {
      "id": "Bayesian Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of Bayesian approaches in machine learning."
    },
    {
      "id": "Training Set",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Set of training examples used to learn the model parameters."
    },
    {
      "id": "Posterior Distribution on Parameters",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Distribution of model parameters given the training set."
    },
    {
      "id": "Bayes' Theorem Application",
      "type": "subnode",
      "parent": "Posterior Distribution on Parameters",
      "description": "Application of Bayes' theorem to compute posterior distribution."
    },
    {
      "id": "Model Specification",
      "type": "subnode",
      "parent": "Posterior Distribution on Parameters",
      "description": "Specification of the model used for prediction, e.g., logistic regression."
    },
    {
      "id": "Predictive Posterior Distribution",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Distribution of class labels given a new test example and training set."
    },
    {
      "id": "Fully Bayesian Prediction",
      "type": "subnode",
      "parent": "Predictive Posterior Distribution",
      "description": "Prediction method that averages over the posterior distribution on parameters."
    },
    {
      "id": "Computational Challenges",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Challenges in computing high-dimensional integrals for posterior distributions."
    },
    {
      "id": "Bayesian Inference",
      "type": "major",
      "parent": null,
      "description": "Involves approximating posterior distributions for parameters."
    },
    {
      "id": "Posterior Distribution Approximation",
      "type": "subnode",
      "parent": "Bayesian Inference",
      "description": "Methods to approximate the posterior distribution of θ."
    },
    {
      "id": "MAP Estimate",
      "type": "subnode",
      "parent": "Posterior Distribution Approximation",
      "description": "Point estimate that maximizes the posterior probability."
    },
    {
      "id": "MLE Estimate",
      "type": "subnode",
      "parent": "Posterior Distribution Approximation",
      "description": "Estimate that maximizes likelihood without considering prior."
    },
    {
      "id": "Prior Choice",
      "type": "subnode",
      "parent": "MAP Estimate",
      "description": "Selection of a Gaussian prior to regularize the model parameters."
    },
    {
      "id": "Unsupervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning from data without labeled responses; includes clustering."
    },
    {
      "id": "Clustering",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Grouping of unlabeled data into clusters based on similarity."
    },
    {
      "id": "K-Means Algorithm",
      "type": "subnode",
      "parent": "Clustering",
      "description": "Iterative algorithm to partition data into k clusters."
    },
    {
      "id": "k-means_algorithm",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "initialization_method",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Randomly selects initial cluster centroids from training examples."
    },
    {
      "id": "inner_loop",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Repeats assigning and moving steps until convergence."
    },
    {
      "id": "assigning_step",
      "type": "subnode",
      "parent": "inner_loop",
      "description": "Assigns each training example to the closest cluster centroid."
    },
    {
      "id": "moving_step",
      "type": "subnode",
      "parent": "inner_loop",
      "description": "Moves centroids to mean of assigned points."
    },
    {
      "id": "distortion_function",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Measures sum of squared distances between examples and their cluster centroid."
    },
    {
      "id": "distortion_function_J",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Function measuring the quality of clustering; should decrease monotonically and converge."
    },
    {
      "id": "convergence_of_k_means",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Process where cluster centroids move to mean positions after each iteration."
    },
    {
      "id": "local_optima_issue",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Risk of getting stuck in local minima rather than global minimum."
    },
    {
      "id": "em_algorithm",
      "type": "major",
      "parent": null,
      "description": "Algorithm for density estimation and parameter learning in probabilistic models."
    },
    {
      "id": "gaussian_mixture_models",
      "type": "subnode",
      "parent": "em_algorithm",
      "description": "Models where data is generated from a mixture of multiple Gaussian distributions."
    },
    {
      "id": "unsupervised_learning",
      "type": "major",
      "parent": null,
      "description": "Learning without labeled data"
    },
    {
      "id": "joint_distribution_modeling",
      "type": "subnode",
      "parent": "unsupervised_learning",
      "description": "Modeling the joint distribution of x and z"
    },
    {
      "id": "multinomial_distribution",
      "type": "subnode",
      "parent": "joint_distribution_modeling",
      "description": "Distribution for latent variable z"
    },
    {
      "id": "gaussian_distributions",
      "type": "subnode",
      "parent": "joint_distribution_modeling",
      "description": "Conditional distribution of x given z"
    },
    {
      "id": "mixture_of_gaussians",
      "type": "major",
      "parent": null,
      "description": "Model using multiple Gaussian distributions with latent variables"
    },
    {
      "id": "latent_variables",
      "type": "subnode",
      "parent": "mixture_of_gaussians",
      "description": "Hidden random variables z that influence x"
    },
    {
      "id": "model_parameters",
      "type": "subnode",
      "parent": "mixture_of_gaussians",
      "description": "Parameters phi, mu, and Sigma to be estimated"
    },
    {
      "id": "likelihood_function",
      "type": "subnode",
      "parent": "model_parameters",
      "description": "Function used for estimating model parameters"
    },
    {
      "id": "maximum_likelihood_estimation",
      "type": "major",
      "parent": null,
      "description": "Estimating parameters to maximize likelihood of observed data"
    },
    {
      "id": "GaussianMixtureModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters."
    },
    {
      "id": "EMAlgorithm",
      "type": "subnode",
      "parent": "GaussianMixtureModel",
      "description": "Iterative method to find maximum likelihood or maximum a posteriori estimates of parameters in probabilistic models, where the model depends on unobserved latent variables."
    },
    {
      "id": "EM_Algorithm",
      "type": "major",
      "parent": "Machine_Learning_Concepts",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates in probabilistic models."
    },
    {
      "id": "E_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Estimation step where posterior distribution of hidden variables is calculated given observed data and current parameters"
    },
    {
      "id": "M_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Maximization step where model parameters are updated to maximize the expected log-likelihood found in E-step"
    },
    {
      "id": "Gaussian_Mixture_Models",
      "type": "major",
      "parent": null,
      "description": "Models data as a mixture of multiple Gaussian distributions."
    },
    {
      "id": "Soft_Guesses",
      "type": "subnode",
      "parent": "E_Step",
      "description": "Probabilistic assignments for latent variables in contrast to hard guesses."
    },
    {
      "id": "K_Means_Clustering",
      "type": "major",
      "parent": null,
      "description": "Partitioning method that assigns data points to clusters based on distance from cluster centers."
    },
    {
      "id": "Local_Optima_Issue",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Risk of converging to suboptimal solutions due to the nature of iterative optimization."
    },
    {
      "id": "Expectation_Maximization_Guarantees",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the convergence guarantees for EM algorithm."
    },
    {
      "id": "Jensens_Inequality",
      "type": "major",
      "parent": null,
      "description": "Introduction to Jensen's inequality and its applications in machine learning."
    },
    {
      "id": "Convex_Functions",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Functions where the line segment between any two points on its graph lies above or on the function."
    },
    {
      "id": "Theorem_Jensens_Inequality",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Statement and proof of Jensen's inequality theorem."
    },
    {
      "id": "Jensen's_Inequality",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Explanation and application of Jensen's inequality in deriving bounds."
    },
    {
      "id": "Concave_Functions",
      "type": "subnode",
      "parent": "Jensen's_Inequality",
      "description": "Negative of convex functions, with reversed inequality in Jensen's Inequality."
    },
    {
      "id": "EM_Algorithms",
      "type": "major",
      "parent": null,
      "description": "Expectation-Maximization algorithms for parameter estimation in latent variable models."
    },
    {
      "id": "Latent_Variable_Models",
      "type": "subnode",
      "parent": "EM_Algorithms",
      "description": "Models with unobserved variables that must be inferred from observed data."
    },
    {
      "id": "Log_Likelihood_Maximization",
      "type": "subnode",
      "parent": "EM_Algorithms",
      "description": "Process of finding parameters that maximize the likelihood of observing given data."
    },
    {
      "id": "Non_Convex_Optimization",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Difficulties arising from non-convex optimization problems."
    },
    {
      "id": "Latent_Variables",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Random variables that are not directly observed but inferred from other data."
    },
    {
      "id": "Likelihood_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for optimizing likelihood functions, especially in presence of latent variables."
    },
    {
      "id": "Single_Example_Optimization",
      "type": "subnode",
      "parent": "Likelihood_Optimization",
      "description": "Optimizing the log-likelihood for a single data point before extending to multiple examples."
    },
    {
      "id": "Probability_Distributions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on probability distributions in the context of machine learning."
    },
    {
      "id": "Log_Probability_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Derivation showing how to bound the log probability using a distribution Q."
    },
    {
      "id": "Evidence_Lower_Bound_(ELBO)",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "A lower bound on the log-likelihood used in variational inference and EM algorithm."
    },
    {
      "id": "Tightening_the_Bound",
      "type": "subnode",
      "parent": "Evidence_Lower_Bound_(ELBO)",
      "description": "Techniques for making the bound as close as possible to the true value."
    },
    {
      "id": "Posterior_Distribution",
      "type": "subnode",
      "parent": "Tightening_the_Bound",
      "description": "Key in setting Q(z) to achieve equality in ELBO derivation."
    },
    {
      "id": "Log_Likelihood_Optimization",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Optimizing log-likelihood of a single example under EM framework."
    },
    {
      "id": "Multiple_Examples",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Extending the EM algorithm to multiple training examples."
    },
    {
      "id": "Evidence_Lower_Bound",
      "type": "subnode",
      "parent": "Log_Likelihood_Optimization",
      "description": "Building ELBO for a single example in log-likelihood optimization."
    },
    {
      "id": "ELBO_Multiple_Examples",
      "type": "subnode",
      "parent": "Multiple_Examples",
      "description": "Summing ELBO over multiple examples to get a lower bound on the overall log-likelihood."
    },
    {
      "id": "LogLikelihood",
      "type": "major",
      "parent": "ModelParameters",
      "description": "Measure of how well a probability distribution or model explains a given set of observations"
    },
    {
      "id": "ELBO",
      "type": "major",
      "parent": "VariationalInference",
      "description": "Evidence Lower BOund used in variational inference and EM for optimizing latent variable models"
    },
    {
      "id": "Definition of ELBO",
      "type": "subnode",
      "parent": "ELBO",
      "description": "Mathematical definition of ELBO using Q and theta"
    },
    {
      "id": "Rewritten Formulations",
      "type": "subnode",
      "parent": "ELBO",
      "description": "Alternative formulations of ELBO including conditional likelihood"
    },
    {
      "id": "KL Divergence",
      "type": "subnode",
      "parent": "ELBO",
      "description": "Explanation of KL divergence in the context of ELBO"
    },
    {
      "id": "Marginal Distribution",
      "type": "subnode",
      "parent": "ELBO",
      "description": "Discussion on marginal distribution and its independence from theta"
    },
    {
      "id": "Conditional Distribution",
      "type": "subnode",
      "parent": "ELBO",
      "description": "Explanation of conditional distribution p(z|x) in ELBO context"
    },
    {
      "id": "Mixture of Gaussians",
      "type": "major",
      "parent": null,
      "description": "Revisiting the mixture of Gaussian models using EM algorithm"
    },
    {
      "id": "EM Algorithm",
      "type": "subnode",
      "parent": "Mixture of Gaussians",
      "description": "General definition and application to mixture of Gaussians"
    },
    {
      "id": "E-step Calculation",
      "type": "subnode",
      "parent": "Mixture of Gaussians",
      "description": "Calculation process and formula derivation for the E-step."
    },
    {
      "id": "Expectation-Maximization Algorithm",
      "type": "major",
      "parent": null,
      "description": "Overview of EM algorithm for parameter estimation in machine learning models."
    },
    {
      "id": "M-step Maximization",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Maximization step focusing on parameter updates in the M-step."
    },
    {
      "id": "Update Rule for mu_j",
      "type": "subnode",
      "parent": "M-step Maximization",
      "description": "Derivation and formula of update rule for μ_j parameters."
    },
    {
      "id": "Update Rule for phi_j",
      "type": "subnode",
      "parent": "M-step Maximization",
      "description": "Derivation process for the M-step update rule concerning φ_j parameters."
    },
    {
      "id": "Expectation_Maximization_Algorithm",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Algorithm for finding maximum likelihood or maximum a posteriori estimates of parameters in probabilistic models."
    },
    {
      "id": "M_Step_Update_Rule",
      "type": "subnode",
      "parent": "Expectation_Maximization_Algorithm",
      "description": "Rule for updating parameters during the maximization step of EM algorithm."
    },
    {
      "id": "Lagrangian_Method",
      "type": "subnode",
      "parent": "M_Step_Update_Rule",
      "description": "Mathematical method used to find maxima or minima under constraints."
    },
    {
      "id": "Variational_Inference",
      "type": "major",
      "parent": null,
      "description": "Technique for approximating probability distributions in Bayesian statistics and machine learning."
    },
    {
      "id": "Variational_Autoencoder",
      "type": "subnode",
      "parent": "Variational_Inference",
      "description": "Probabilistic model that uses an encoder-decoder architecture to learn a latent variable model for a set of high-dimensional data."
    },
    {
      "id": "VariationalAutoEncoder",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Family of algorithms extending EM to complex models using neural networks."
    },
    {
      "id": "VariationalInference",
      "type": "subnode",
      "parent": "VariationalAutoEncoder",
      "description": "Technique for approximating probability distributions using variational methods."
    },
    {
      "id": "ReparametrizationTrick",
      "type": "subnode",
      "parent": "VariationalAutoEncoder",
      "description": "Method allowing backpropagation through stochastic nodes in a neural network."
    },
    {
      "id": "GaussianMixtureModels",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Model using a mixture of multivariate Gaussian distributions to represent complex data sets."
    },
    {
      "id": "Variational Inference",
      "type": "major",
      "parent": null,
      "description": "Approximating posterior distributions using variational methods."
    },
    {
      "id": "ELBO Lower Bound",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Evidence lower bound used to optimize approximate posteriors."
    },
    {
      "id": "Mean Field Assumption",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Assumption for simplifying distributions over high-dimensional discrete variables."
    },
    {
      "id": "Continuous Latent Variables",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Techniques for dealing with continuous latent variables in variational inference."
    },
    {
      "id": "Gaussian_Distribution_Qi",
      "type": "subnode",
      "parent": "Latent_Variables",
      "description": "A Gaussian distribution assumed for Qi with specific mean and variance functions."
    },
    {
      "id": "Mean_and_Variance_Functions",
      "type": "subnode",
      "parent": "Gaussian_Distribution_Qi",
      "description": "Functions q(x;phi) and v(x;psi) that determine the mean and variance of Gaussian distribution Qi."
    },
    {
      "id": "Encoder_Decoder_Roles",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Roles of functions q and v as encoder, mapping data to latent space, and g(z;theta) as decoder, reconstructing the original data from latent variables."
    },
    {
      "id": "Efficient_Evaluation_ELBO",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Process of verifying efficient evaluation of Evidence Lower Bound (ELBO) for given Q distribution."
    },
    {
      "id": "ELBOOptimization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of optimizing Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "Q_iFormulation",
      "type": "subnode",
      "parent": "ELBOOptimization",
      "description": "Description and requirements for the form of Q_i used in ELBO calculation."
    },
    {
      "id": "EfficientEvaluation",
      "type": "subnode",
      "parent": "ELBOOptimization",
      "description": "Methods to efficiently evaluate the value of ELBO given fixed Q_i and theta."
    },
    {
      "id": "GradientAscent",
      "type": "subnode",
      "parent": "ELBOOptimization",
      "description": "Technique for optimizing ELBO using gradient ascent over parameters phi, psi, and theta."
    },
    {
      "id": "ReparameterizationTrick",
      "type": "subnode",
      "parent": "GradientComputation",
      "description": "Technique to simplify gradient computation by re-parameterizing the sampling process."
    },
    {
      "id": "ExpectationMaximization",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques for estimating parameters in statistical models."
    },
    {
      "id": "GradientEstimation",
      "type": "subnode",
      "parent": "ExpectationMaximization",
      "description": "Methods to estimate gradients of expectations with respect to model parameters."
    },
    {
      "id": "PrincipalComponentsAnalysis",
      "type": "major",
      "parent": null,
      "description": "Dimensionality reduction technique focusing on principal components of data."
    },
    {
      "id": "PCAComputationEfficiency",
      "type": "subnode",
      "parent": "PrincipalComponentsAnalysis",
      "description": "Computational methods for performing PCA efficiently."
    },
    {
      "id": "RedundancyDetection",
      "type": "major",
      "parent": null,
      "description": "Detecting and removing redundant attributes in data sets."
    },
    {
      "id": "LinearDependency",
      "type": "subnode",
      "parent": "RedundancyDetection",
      "description": "Identifying linearly dependent attributes within a dataset."
    },
    {
      "id": "PCAAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Principal Component Analysis for dimensionality reduction."
    },
    {
      "id": "DataNormalization",
      "type": "subnode",
      "parent": "PCAAlgorithm",
      "description": "Preprocessing step to normalize data before PCA application."
    },
    {
      "id": "MeanVarianceCalculation",
      "type": "subnode",
      "parent": "DataNormalization",
      "description": "Calculating mean and variance for each feature in the dataset."
    },
    {
      "id": "Zero Mean Adjustment",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Subtracting the mean from each feature."
    },
    {
      "id": "Unit Variance Scaling",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Dividing by standard deviation to achieve unit variance."
    },
    {
      "id": "Data Comparison",
      "type": "subnode",
      "parent": "Normalization Techniques",
      "description": "Ensuring attributes are comparable after normalization."
    },
    {
      "id": "Major Axis of Variation",
      "type": "major",
      "parent": null,
      "description": "Finding the direction that maximizes variance in projected data."
    },
    {
      "id": "Variance Maximization",
      "type": "subnode",
      "parent": "Major Axis of Variation",
      "description": "Choosing a unit vector to maximize variance of projections."
    },
    {
      "id": "Data Approximation",
      "type": "subnode",
      "parent": "Major Axis of Variation",
      "description": "Approximating data in the direction corresponding to major axis."
    },
    {
      "id": "PrincipalComponentAnalysis",
      "type": "major",
      "parent": null,
      "description": "Technique to reduce dimensionality while retaining variance."
    },
    {
      "id": "ProjectionOntoUnitVector",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Process of projecting data points onto a unit vector."
    },
    {
      "id": "VarianceMaximization",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Objective to maximize variance in projections."
    },
    {
      "id": "EmpiricalCovarianceMatrix",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Matrix representing data covariance for zero-mean data."
    },
    {
      "id": "PrincipalEigenvector",
      "type": "subnode",
      "parent": "VarianceMaximization",
      "description": "Vector maximizing variance in data projections."
    },
    {
      "id": "kDimensionalSubspace",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Projection of data into a lower-dimensional space using top k eigenvectors."
    },
    {
      "id": "Principal Component Analysis (PCA)",
      "type": "major",
      "parent": "Dimensionality Reduction Techniques",
      "description": "A method that transforms data into principal components for dimension reduction."
    },
    {
      "id": "Eigenvectors and Eigenvalues",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Top k eigenvectors form the basis for dimensionality reduction."
    },
    {
      "id": "Covariance Matrix Sigma",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Symmetric matrix whose eigenvectors are used in PCA."
    },
    {
      "id": "Dimensionality Reduction",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Process of reducing data dimensions while preserving variability."
    },
    {
      "id": "Principal Components",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "First k eigenvectors that maximize variance in the reduced space."
    },
    {
      "id": "Approximation Error Minimization",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Derivation of PCA by minimizing error from projection onto k-dimensional subspace."
    },
    {
      "id": "Data Visualization",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Plotting lower dimensional data for visual analysis and clustering."
    },
    {
      "id": "Preprocessing with PCA",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Using PCA to reduce dataset dimensions before further processing."
    },
    {
      "id": "Dimensionality Reduction Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques to reduce the number of random variables under consideration."
    },
    {
      "id": "Plotting Data Points",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Visualizing PCA-transformed data to identify similarities and clusters among car types."
    },
    {
      "id": "Preprocessing Dataset",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Using PCA to preprocess datasets for supervised learning, reducing dimensionality and complexity."
    },
    {
      "id": "Noise Reduction",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Applying PCA to reduce noise in data, such as estimating intrinsic piloting skill from noisy measures."
    },
    {
      "id": "Eigenfaces Method",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Using PCA on face images to find lower-dimensional representations that capture systematic variations between faces."
    },
    {
      "id": "Independent Components Analysis (ICA)",
      "type": "major",
      "parent": null,
      "description": "A technique for separating a multivariate signal into independent, non-Gaussian components."
    },
    {
      "id": "ICA_Overview",
      "type": "subnode",
      "parent": "Machine_Learning_Topic",
      "description": "Introduction to Independent Component Analysis (ICA) in the context of signal processing."
    },
    {
      "id": "Cocktail_Party_Problem",
      "type": "subnode",
      "parent": "ICA_Overview",
      "description": "An illustrative example using multiple microphones and speakers to demonstrate ICA's application."
    },
    {
      "id": "Mixing_Matrix_A",
      "type": "subnode",
      "parent": "Cocktail_Party_Problem",
      "description": "Matrix A representing the unknown mixing of signals from different sources."
    },
    {
      "id": "Unmixing_Matrix_W",
      "type": "subnode",
      "parent": "Cocktail_Party_Problem",
      "description": "Inverse matrix W used to recover original source signals from mixed data."
    },
    {
      "id": "ICA_Ambiguities",
      "type": "subnode",
      "parent": "ICA_Overview",
      "description": "Discussion on the limitations and ambiguities in recovering the unmixing matrix W without prior knowledge."
    },
    {
      "id": "ICA Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discusses inherent ambiguities in Independent Component Analysis recovery."
    },
    {
      "id": "Permutation Matrix",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Describes the role of permutation matrices in ICA ambiguities."
    },
    {
      "id": "Scaling Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Explains how scaling ambiguity affects the recovery process in ICA."
    },
    {
      "id": "Scaling_Signal",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Effect of scaling a signal by a positive factor"
    },
    {
      "id": "Sign_Changes_Ignored",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Explanation that sign changes in signals do not affect the outcome"
    },
    {
      "id": "Non_Gaussian_Sources",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Sources are non-Gaussian, eliminating ambiguity"
    },
    {
      "id": "Gaussian_Data_Issue",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Problem with Gaussian data in ICA due to rotational symmetry"
    },
    {
      "id": "Mixing_Matrix_Rotation",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Effect of rotating the mixing matrix on observed data distribution"
    },
    {
      "id": "ICA_On_Gaussian_Data",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the limitations of ICA when applied to Gaussian data."
    },
    {
      "id": "Rotational_Symmetry_Implication",
      "type": "subnode",
      "parent": "ICA_On_Gaussian_Data",
      "description": "Explanation of how rotational symmetry affects ICA results for Gaussian distributions."
    },
    {
      "id": "Non_Gaussian_Distributions",
      "type": "subnode",
      "parent": "ICA_On_Gaussian_Data",
      "description": "Discussion on the possibility of recovering independent sources from non-Gaussian data."
    },
    {
      "id": "Linear_Transformations_And_Densities",
      "type": "major",
      "parent": null,
      "description": "Exploration of how linear transformations affect probability densities in random variables."
    },
    {
      "id": "Effect_Of_Linear_Transformation",
      "type": "subnode",
      "parent": "Linear_Transformations_And_Densities",
      "description": "Explanation on the impact of linear transformation on density functions of random variables."
    },
    {
      "id": "Density Transformation",
      "type": "major",
      "parent": null,
      "description": "Transformation of probability density under linear transformation."
    },
    {
      "id": "1D Example",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Example in one dimension illustrating the concept."
    },
    {
      "id": "General Case",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Extension to vector-valued distributions and higher dimensions."
    },
    {
      "id": "Remark",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Additional explanation using hypercubes and determinants."
    },
    {
      "id": "ICA Algorithm",
      "type": "major",
      "parent": null,
      "description": "Derivation of an Independent Component Analysis algorithm based on maximum likelihood estimation."
    },
    {
      "id": "ICAIndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Assumption in Independent Component Analysis that sources are statistically independent."
    },
    {
      "id": "JointDistributionModeling",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Modeling the joint distribution of source signals as a product of marginals."
    },
    {
      "id": "DensityFunctionFormulation",
      "type": "subnode",
      "parent": "ICAIndependenceAssumption",
      "description": "Formulating density functions for individual sources in ICA."
    },
    {
      "id": "CumulativeDistributionFunction",
      "type": "subnode",
      "parent": "DensityFunctionFormulation",
      "description": "Definition and properties of cumulative distribution function (CDF)."
    },
    {
      "id": "SigmoidFunctionChoice",
      "type": "subnode",
      "parent": "DensityFunctionFormulation",
      "description": "Choosing the sigmoid function as a default density for sources in ICA."
    },
    {
      "id": "DataPreprocessing",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process of preparing data for analysis, including mean normalization."
    },
    {
      "id": "ProbabilityDistributions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Theoretical distributions used in modeling data."
    },
    {
      "id": "ModelParameters",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Parameters that define the model's structure, such as matrix W."
    },
    {
      "id": "StochasticGradientAscent",
      "type": "subnode",
      "parent": "GradientAscent",
      "description": "Variant of gradient ascent using single data points to update parameters iteratively."
    },
    {
      "id": "ConvergenceAndRecovery",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Process after convergence where the model is used to recover original sources."
    },
    {
      "id": "Stochastic Gradient Ascent",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A method for optimizing parameters in models using randomly selected examples from the dataset."
    },
    {
      "id": "Self-supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Training machine learning models with unlabeled data by predicting parts of the input as labels."
    },
    {
      "id": "Foundation Models",
      "type": "subnode",
      "parent": "Self-supervised Learning",
      "description": "Large-scale pre-trained models adaptable to various tasks with minimal labeled data."
    },
    {
      "id": "Pretraining and Adaptation",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Process of training a model on large unlabeled datasets followed by adapting it for specific tasks."
    },
    {
      "id": "Machine_Learning_Pretraining_and_Adaptation",
      "type": "major",
      "parent": null,
      "description": "Overview of pretraining and adaptation phases in machine learning."
    },
    {
      "id": "Pretraining_Phase",
      "type": "subnode",
      "parent": "Machine_Learning_Pretraining_and_Adaptation",
      "description": "Training a model on unlabeled data to learn good representations."
    },
    {
      "id": "Adaptation_Phase",
      "type": "subnode",
      "parent": "Machine_Learning_Pretraining_and_Adaptation",
      "description": "Customizing the pretrained model for specific downstream tasks using labeled data."
    },
    {
      "id": "Unlabeled_Dataset",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Dataset used in pretraining phase, consisting of unlabeled examples."
    },
    {
      "id": "Model_Parameterization",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Description of model ψ_θ and its parameterization."
    },
    {
      "id": "Embedding_Features",
      "type": "subnode",
      "parent": "Model_Parameterization",
      "description": "Representation or embedding produced by the model for an input example x."
    },
    {
      "id": "Pretraining_Loss_Function",
      "type": "subnode",
      "parent": "Pretraining_Phase",
      "description": "Loss function used to train the model during pretraining phase."
    },
    {
      "id": "Downstream_Task_Dataset",
      "type": "subnode",
      "parent": "Adaptation_Phase",
      "description": "Dataset with labeled examples specific to a downstream task."
    },
    {
      "id": "Machine Learning Adaptation Methods",
      "type": "major",
      "parent": null,
      "description": "Overview of methods used for adapting machine learning models to new tasks."
    },
    {
      "id": "Labeled Dataset",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Dataset containing labeled examples for a specific task."
    },
    {
      "id": "Zero-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Scenario where no labeled data is available for the new task."
    },
    {
      "id": "Few-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Situation with a small number of labeled examples (1-50)."
    },
    {
      "id": "Adaptation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Algorithm that takes pretrained model and downstream dataset to produce adapted model."
    },
    {
      "id": "Linear Probe",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Uses a linear head on top of the representation for prediction without changing the pretrained model."
    },
    {
      "id": "Finetuning",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Process of training a model on specific tasks with task-specific data after pretraining."
    },
    {
      "id": "Language-Specific Methods",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Methods specific to language problems introduced in section 14.3.2."
    },
    {
      "id": "Machine_Learning_Adaptation_Methods",
      "type": "major",
      "parent": null,
      "description": "Methods for adapting machine learning models to new tasks."
    },
    {
      "id": "Finetuning_Pretrained_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation_Methods",
      "description": "Process of fine-tuning pretrained models for downstream tasks."
    },
    {
      "id": "Prediction_Model_Structure",
      "type": "subnode",
      "parent": "Finetuning_Pretrained_Models",
      "description": "Structure of the prediction model using both fixed and trainable parameters."
    },
    {
      "id": "Optimization_Objective",
      "type": "subnode",
      "parent": "Prediction_Model_Structure",
      "description": "Objective function to optimize for fitting downstream data."
    },
    {
      "id": "Pretraining_Methods_Computer_Vision",
      "type": "major",
      "parent": null,
      "description": "Introduction to pretraining methods in computer vision."
    },
    {
      "id": "Supervised_Pretraining",
      "type": "subnode",
      "parent": "Pretraining_Methods_Computer_Vision",
      "description": "Training a model on labeled data for supervised learning tasks."
    },
    {
      "id": "Contrastive_Learning",
      "type": "subnode",
      "parent": "Pretraining_Methods_Computer_Vision",
      "description": "Technique for learning representations by contrasting positive pairs with negative pairs."
    },
    {
      "id": "Self-Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning method using unlabeled data to train models."
    },
    {
      "id": "Representation Function",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Function that maps images to representations based on semantics."
    },
    {
      "id": "Data Augmentation",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Technique for generating variations of an image to create positive pairs."
    },
    {
      "id": "Positive Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Augmented images from the same original image, expected to be semantically related."
    },
    {
      "id": "Negative Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Randomly selected augmented images from different original images, likely not semantically related."
    },
    {
      "id": "SIMCLR",
      "type": "subnode",
      "parent": "Contrastive_Learning",
      "description": "Algorithm that uses contrastive loss to learn useful representations from data."
    },
    {
      "id": "Loss_Function",
      "type": "subnode",
      "parent": "SIMCLR",
      "description": "Function used in SIMCLR to measure the difference between positive and negative pairs."
    },
    {
      "id": "Augmentation",
      "type": "subnode",
      "parent": "SIMCLR",
      "description": "Process of creating multiple versions of an image for training purposes."
    },
    {
      "id": "Pretrained_Language_Models",
      "type": "major",
      "parent": null,
      "description": "Models pre-trained on large datasets for natural language processing tasks."
    },
    {
      "id": "Language_Model_Probability_Distribution",
      "type": "subnode",
      "parent": "Pretrained_Language_Models",
      "description": "Probability distribution of a document in the context of language models."
    },
    {
      "id": "ConditionalProbabilityModeling",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Parameterizing conditional probabilities in sequence modeling tasks."
    },
    {
      "id": "EmbeddingsIntroduction",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Introducing word embeddings for numerical input handling in models."
    },
    {
      "id": "TransformerModelOverview",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Overview of the Transformer model's input-output interface and its autoregressive property."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning models including transformers."
    },
    {
      "id": "Transformer Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A type of model used in natural language processing and other tasks."
    },
    {
      "id": "Conditional Probability",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "Probability of a token given the previous tokens in sequence generation."
    },
    {
      "id": "Training Process",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "Process of training the transformer model using loss functions."
    },
    {
      "id": "Autoregressive Decoding",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "Process of generating text sequentially using conditional probabilities."
    },
    {
      "id": "Machine Learning Techniques",
      "type": "major",
      "parent": null,
      "description": "Various methods and algorithms used in machine learning."
    },
    {
      "id": "Language Models",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Models that generate text based on learned patterns from large datasets."
    },
    {
      "id": "Temperature Parameter",
      "type": "subnode",
      "parent": "Conditional Probability",
      "description": "Adjusts the randomness of generated text by scaling logits before applying softmax."
    },
    {
      "id": "Text Generation",
      "type": "subnode",
      "parent": "Language Models",
      "description": "Process of generating sequences of tokens based on learned language patterns."
    },
    {
      "id": "Adaptive Sampling",
      "type": "subnode",
      "parent": "Text Generation",
      "description": "Techniques to adjust the randomness in text generation using parameters like temperature."
    },
    {
      "id": "Zero-shot Learning",
      "type": "major",
      "parent": null,
      "description": "Approach where a model is adapted to new tasks without task-specific training data."
    },
    {
      "id": "In-context Learning",
      "type": "subnode",
      "parent": "Zero-shot Learning",
      "description": "Method of adapting models by providing context directly in the input during inference."
    },
    {
      "id": "Machine_Learning_Adaptation",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning adaptation techniques."
    },
    {
      "id": "Zero-Shot_Adaptation",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Technique where no input-output pairs are available for downstream tasks."
    },
    {
      "id": "In-Context_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Approach used in few-shot settings with a small number of labeled examples."
    },
    {
      "id": "Reinforcement Learning",
      "type": "major",
      "parent": null,
      "description": "Introduction to reinforcement learning and control problems."
    },
    {
      "id": "Sequential Decision Making",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Decision making in sequences without explicit supervision."
    },
    {
      "id": "Reward Function",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Function guiding learning agents through positive and negative rewards."
    },
    {
      "id": "MachineLearning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from data."
    },
    {
      "id": "ReinforcementLearning",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Area of ML where agents learn to make decisions in an environment through trial and error."
    },
    {
      "id": "MarkovDecisionProcesses",
      "type": "subnode",
      "parent": "ReinforcementLearning",
      "description": "Formal framework for modeling decision-making situations in reinforcement learning."
    },
    {
      "id": "States",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Set of all possible conditions or configurations of the system being modeled."
    },
    {
      "id": "Actions",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Possible decisions or moves that can be made in a given state."
    },
    {
      "id": "StateTransitionProbabilities",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Probability distribution over states after taking an action from the current state."
    },
    {
      "id": "DiscountFactor",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Parameter that determines the importance of future rewards relative to immediate ones."
    },
    {
      "id": "RewardFunction",
      "type": "subnode",
      "parent": "MarkovDecisionProcesses",
      "description": "Function assigning numerical values representing desirability of state-action pairs or states."
    },
    {
      "id": "Reinforcement Learning Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to reinforcement learning concepts and processes."
    },
    {
      "id": "Markov Decision Process (MDP)",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "A framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "State Transition",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "The process by which an agent moves from one state to another based on actions taken."
    },
    {
      "id": "Action Selection",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Choosing the next action in a sequence of states and transitions."
    },
    {
      "id": "Total Payoff Calculation",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "Calculating cumulative rewards over time with discounting."
    },
    {
      "id": "Discount Factor (γ)",
      "type": "subnode",
      "parent": "Total Payoff Calculation",
      "description": "A factor that discounts future rewards to reflect their reduced value compared to immediate rewards."
    },
    {
      "id": "Policy Definition",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "A strategy or rule that defines the action to be taken in each state."
    },
    {
      "id": "Value Function",
      "type": "subnode",
      "parent": "Policy Definition",
      "description": "The expected cumulative reward starting from a given state and following a policy."
    },
    {
      "id": "Policy Execution",
      "type": "major",
      "parent": null,
      "description": "Taking actions based on a policy in given states."
    },
    {
      "id": "Bellman Equations",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Set of equations that describe the relationship between value functions and optimal policies."
    },
    {
      "id": "Immediate Reward",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Reward received immediately upon entering state s."
    },
    {
      "id": "Future Discounted Rewards",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Expected sum of discounted rewards after the first step in MDP."
    },
    {
      "id": "Optimal Value Function",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Best possible value function attainable by any policy"
    },
    {
      "id": "Bellman's Equation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Equation that defines the optimal value function recursively"
    },
    {
      "id": "Policy",
      "type": "major",
      "parent": null,
      "description": "Strategy defining how to act in each state"
    },
    {
      "id": "Optimal Policy",
      "type": "subnode",
      "parent": "Policy",
      "description": "Policy that maximizes expected utility from a given state."
    },
    {
      "id": "Markov Decision Processes (MDPs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "Value Iteration",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDPs)",
      "description": "Algorithm to find optimal policies by iteratively updating value function."
    },
    {
      "id": "Policy Iteration",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDPs)",
      "description": "An iterative method to find optimal policies in reinforcement learning."
    },
    {
      "id": "Synchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Update all state values simultaneously before applying the Bellman backup operator."
    },
    {
      "id": "Asynchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Update state values one at a time in some order."
    },
    {
      "id": "Value_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Iterative method to find optimal value function for an MDP using estimated transition probabilities and rewards."
    },
    {
      "id": "Policy_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Method to iteratively improve policies in an MDP based on the current value function estimate."
    },
    {
      "id": "Convergence",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Process of reaching optimal values in value or policy iteration."
    },
    {
      "id": "Bellman_Equations",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Equations used to define the value of a state or action in reinforcement learning."
    },
    {
      "id": "Greedy_Policy",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Policy that selects actions based on highest expected immediate reward."
    },
    {
      "id": "Learning_Model_for_MDP",
      "type": "major",
      "parent": null,
      "description": "Process of learning state transition probabilities and rewards in MDPs from data."
    },
    {
      "id": "Inverted_Pendulum_Problem",
      "type": "subnode",
      "parent": "Learning_Model_for_MDP",
      "description": "Example problem used to illustrate the process of estimating transition probabilities and rewards."
    },
    {
      "id": "State_Transition_Probabilities",
      "type": "subnode",
      "parent": "Learning_Model_for_MDP",
      "description": "Estimating probabilities of transitioning from one state to another based on observed experience."
    },
    {
      "id": "Maximum_Likelihood_Estimates",
      "type": "subnode",
      "parent": "State_Transition_Probabilities",
      "description": "Method used to estimate parameters in a model by maximizing the likelihood function given the observed data."
    },
    {
      "id": "MDP_Model_Learning",
      "type": "major",
      "parent": null,
      "description": "Process of learning a model for an MDP with unknown state transitions and rewards."
    },
    {
      "id": "Expected_Immediate_Reward",
      "type": "subnode",
      "parent": "MDP_Model_Learning",
      "description": "Calculating the expected reward in a given state as the average observed reward."
    },
    {
      "id": "Optimization_Technique",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Using previous iteration's solution as initial guess for faster convergence of value iteration."
    },
    {
      "id": "Continuous_State_MDPs",
      "type": "major",
      "parent": null,
      "description": "Discussion on Markov Decision Processes with infinite state spaces."
    },
    {
      "id": "Discretization_Method",
      "type": "subnode",
      "parent": "Continuous_State_MDPs",
      "description": "Technique to convert continuous-state MDPs into discrete ones for easier computation."
    },
    {
      "id": "Discretization_in_MDPs",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of converting continuous state spaces into discrete ones for easier computation."
    },
    {
      "id": "Continuous_State_Space",
      "type": "subnode",
      "parent": "Discretization_in_MDPs",
      "description": "State space where states are not restricted to a finite set of values."
    },
    {
      "id": "Discrete_State_Space",
      "type": "subnode",
      "parent": "Discretization_in_MDPs",
      "description": "State space with distinct, separate states."
    },
    {
      "id": "Supervised_Learning",
      "type": "major",
      "parent": null,
      "description": "Type of machine learning where models are trained on labeled data."
    },
    {
      "id": "Piecewise_Constant_Representation",
      "type": "subnode",
      "parent": "Discretization_in_MDPs",
      "description": "Method that assumes constant values within each discretized interval."
    },
    {
      "id": "Curse_of_Dimensionality",
      "type": "subnode",
      "parent": "Discretization_in_MDPs",
      "description": "Exponential increase in complexity with the number of dimensions in a problem."
    },
    {
      "id": "Curse of Dimensionality",
      "type": "major",
      "parent": null,
      "description": "Downside of discretization in high-dimensional spaces."
    },
    {
      "id": "Discretization Limitations",
      "type": "subnode",
      "parent": "Curse of Dimensionality",
      "description": "Works well for low dimensions but fails at higher ones due to exponential growth."
    },
    {
      "id": "Value Function Approximation",
      "type": "major",
      "parent": "Fitted Value Iteration",
      "description": "Alternative method for finding policies in continuous-state MDPs without discretization."
    },
    {
      "id": "Model or Simulator",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Assumption of having a model or simulator for the MDP to develop approximation algorithms."
    },
    {
      "id": "Model Creation Methods",
      "type": "major",
      "parent": null,
      "description": "Various methods to create a model for predicting next states in an MDP."
    },
    {
      "id": "Physics Simulation",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Using physical laws or software to simulate system behavior."
    },
    {
      "id": "Off-the-Shelf Software",
      "type": "subnode",
      "parent": "Physics Simulation",
      "description": "Utilizing existing physics simulation tools like Open Dynamics Engine."
    },
    {
      "id": "Learning from Data",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Deriving models by analyzing data collected through MDP trials."
    },
    {
      "id": "Data Collection",
      "type": "subnode",
      "parent": "Learning from Data",
      "description": "Executing actions in an MDP to gather state sequences for model learning."
    },
    {
      "id": "StateTransitionModeling",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Predicting the next state based on current state and action."
    },
    {
      "id": "LinearModelPrediction",
      "type": "subnode",
      "parent": "StateTransitionModeling",
      "description": "Using a linear model to predict future states from past data."
    },
    {
      "id": "LossFunctionOptimization",
      "type": "subnode",
      "parent": "LinearModelPrediction",
      "description": "Minimizing the loss function to estimate parameters A and B."
    },
    {
      "id": "DeterministicVsStochasticModels",
      "type": "subnode",
      "parent": "StateTransitionModeling",
      "description": "Comparison between deterministic and stochastic models for state transitions."
    },
    {
      "id": "NonLinearFeatureMappings",
      "type": "subnode",
      "parent": "StateTransitionModeling",
      "description": "Using non-linear feature mappings to enhance model flexibility."
    },
    {
      "id": "AlternativeLearningAlgorithms",
      "type": "subnode",
      "parent": "StateTransitionModeling",
      "description": "Exploring other learning algorithms for state transition prediction."
    },
    {
      "id": "Nonlinear_Feature_Mappings",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "Mappings that transform raw data into a higher dimensional space to capture non-linear relationships."
    },
    {
      "id": "Locally_Weighted_Linear_Regression",
      "type": "subnode",
      "parent": "Machine_Learning_Models",
      "description": "A method for learning models in continuous state spaces using weighted regression."
    },
    {
      "id": "Fitted_Value_Iteration",
      "type": "major",
      "parent": null,
      "description": "Algorithm to approximate value functions in continuous state MDPs."
    },
    {
      "id": "Fitted Value Iteration",
      "type": "major",
      "parent": null,
      "description": "Algorithm using supervised learning to approximate the value function in reinforcement learning."
    },
    {
      "id": "Supervised Learning Algorithm",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Uses linear regression to fit the value function based on state samples."
    },
    {
      "id": "State Sampling",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Randomly samples states to approximate the value function."
    },
    {
      "id": "Regression Algorithms",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Set of methods for predicting continuous values from input data."
    },
    {
      "id": "Deterministic Simulator",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Model that predicts exact outcomes without randomness."
    },
    {
      "id": "Convergence Issues",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Challenges in proving convergence of the algorithm."
    },
    {
      "id": "Value_Iteration_Approximations",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Approximation methods for value iteration in reinforcement learning."
    },
    {
      "id": "Expectation_Computation",
      "type": "subnode",
      "parent": "Value_Iteration_Approximations",
      "description": "Methods to compute or approximate the expectation of future states and rewards."
    },
    {
      "id": "Deterministic_Simulator",
      "type": "subnode",
      "parent": "Expectation_Computation",
      "description": "Using a deterministic simulator for state transitions without noise."
    },
    {
      "id": "Gaussian_Noise_Approximation",
      "type": "subnode",
      "parent": "Expectation_Computation",
      "description": "Approximating the expectation when the transition function includes Gaussian noise."
    },
    {
      "id": "Policy_Iteration_Connections",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Connections between policy iteration and value iteration methods."
    },
    {
      "id": "Algorithm 6",
      "type": "major",
      "parent": null,
      "description": "Variant of policy iteration that uses value evaluation procedure VE."
    },
    {
      "id": "VE Procedure",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Evaluates the state-value function for a given policy and hyperparameter k."
    },
    {
      "id": "Hyperparameter k",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Controls the number of value updates before policy improvement."
    },
    {
      "id": "Chapter_15_Summary",
      "type": "major",
      "parent": null,
      "description": "Summary of Chapter 15 on MDPs and value/policy iteration."
    },
    {
      "id": "k_steps_update_frequency",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Discussion on optimal update frequency k for faster convergence."
    },
    {
      "id": "Policy_Iteration_Speedup",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Explanation of how policy iteration can speed up computation using linear system solvers."
    },
    {
      "id": "Value_Iteration_Preference",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Conditions under which value iteration is preferred over policy iteration."
    },
    {
      "id": "Chapter_16_LQR_DDP_LQG",
      "type": "major",
      "parent": null,
      "description": "Introduction to Chapter 16 on LQR, DDP and LQG in MDP context."
    },
    {
      "id": "Finite_Horizon_MDPs",
      "type": "subnode",
      "parent": "Chapter_16_LQR_DDP_LQG",
      "description": "Overview of finite-horizon Markov Decision Processes (MDPs)."
    },
    {
      "id": "Optimal_Bellman_Equation",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Definition and explanation of the optimal Bellman equation."
    },
    {
      "id": "Recovering_Optimal_Policy",
      "type": "subnode",
      "parent": "Optimal_Bellman_Equation",
      "description": "Method to recover the optimal policy from the optimal value function."
    },
    {
      "id": "ExpectationRewriting",
      "type": "major",
      "parent": null,
      "description": "Explains how expectation is rewritten in finite and continuous cases."
    },
    {
      "id": "OptimalActionComputation",
      "type": "subnode",
      "parent": "RewardFunction",
      "description": "Details how to compute the optimal action given a state under the new reward framework."
    },
    {
      "id": "FiniteHorizonMDP",
      "type": "major",
      "parent": null,
      "description": "Introduces finite horizon Markov Decision Processes (MDPs)."
    },
    {
      "id": "TimeHorizonDefinition",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Defines the time horizon in a finite MDP."
    },
    {
      "id": "PayoffDefinition",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Explains how payoff is defined for finite and infinite horizons."
    },
    {
      "id": "DiscountFactorRole",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Discusses the role of the discount factor in ensuring the convergence of an infinite sum, which is not necessary in a finite horizon setting."
    },
    {
      "id": "NonStationaryPolicy",
      "type": "major",
      "parent": null,
      "description": "Explains that policies can be non-stationary in a finite horizon MDP."
    },
    {
      "id": "Non-Stationary Policies",
      "type": "major",
      "parent": null,
      "description": "Policies that change over time in a finite-horizon MDP."
    },
    {
      "id": "Time Dependent Dynamics",
      "type": "subnode",
      "parent": "Non-Stationary Policies",
      "description": "Transition probabilities and rewards vary with time."
    },
    {
      "id": "Finite Horizon MDP",
      "type": "major",
      "parent": null,
      "description": "Markov Decision Process with a limited number of steps."
    },
    {
      "id": "Optimal Policy in Finite-Horizon",
      "type": "subnode",
      "parent": "Non-Stationary Policies",
      "description": "The optimal policy can be non-stationary due to time constraints and changing goals."
    },
    {
      "id": "Policy at Time t",
      "type": "subnode",
      "parent": "Finite Horizon MDP",
      "description": "A policy that changes according to the current time step (t)."
    },
    {
      "id": "Time in State Representation",
      "type": "subnode",
      "parent": "Finite Horizon MDP",
      "description": "Incorporating time as part of the state representation to account for non-stationary dynamics."
    },
    {
      "id": "ValueFunctionDefinition",
      "type": "subnode",
      "parent": "ReinforcementLearning",
      "description": "Definition of value function at time t for a policy pi, representing expected cumulative reward starting from state s."
    },
    {
      "id": "OptimalValueFunction",
      "type": "subnode",
      "parent": "ValueFunctionDefinition",
      "description": "Finding the optimal value function by maximizing over all policies."
    },
    {
      "id": "BellmanEquation",
      "type": "subnode",
      "parent": "OptimalValueFunction",
      "description": "Key equation in reinforcement learning that defines the relationship between state values and future rewards."
    },
    {
      "id": "DynamicProgramming",
      "type": "subnode",
      "parent": "ReinforcementLearning",
      "description": "Technique for solving complex problems by breaking them down into simpler subproblems, used extensively in RL."
    },
    {
      "id": "ValueIterationAlgorithm",
      "type": "subnode",
      "parent": "BellmanEquation",
      "description": "Iterative algorithm to compute the optimal value function using Bellman's equation."
    },
    {
      "id": "Machine_Learning_Overview",
      "type": "major",
      "parent": null,
      "description": "General introduction to machine learning concepts and theories."
    },
    {
      "id": "Geometric_Convergence",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Rate at which value iteration converges towards optimal values."
    },
    {
      "id": "Bellman_Operator",
      "type": "subnode",
      "parent": "Bellman_Equations",
      "description": "Operator that maps a value function to another value function in reinforcement learning."
    },
    {
      "id": "LQR_Overview",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Regulation theory and its applications in robotics and control systems."
    },
    {
      "id": "Finite_Horizon_Setting",
      "type": "subnode",
      "parent": "LQR_Overview",
      "description": "Special case of finite-horizon setting where exact solutions are tractable."
    },
    {
      "id": "Linear_Transitions",
      "type": "subnode",
      "parent": "LQR_Overview",
      "description": "Model assumptions for linear transitions in state space with Gaussian noise."
    },
    {
      "id": "Quadratic_Rewards",
      "type": "subnode",
      "parent": "LQR_Overview",
      "description": "Assumptions about quadratic rewards that ensure the reward is always negative and encourages states close to origin."
    },
    {
      "id": "LQRModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Assumptions of the Linear Quadratic Regulator (LQR) model in control theory."
    },
    {
      "id": "LQRAlgorithmSteps",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Two steps to implement the LQR algorithm: estimation and policy derivation."
    },
    {
      "id": "Step1Estimation",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "Collect data, use linear regression for matrix estimation, and Gaussian Discriminant Analysis for covariance learning."
    },
    {
      "id": "Step2PolicyDerivation",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "Determine the optimal policy using dynamic programming given known model parameters."
    },
    {
      "id": "DynamicProgrammingApplication",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Use of dynamic programming to compute the optimal value function in LQR context."
    },
    {
      "id": "Optimal_Control",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques for determining control policies that optimize a performance criterion."
    },
    {
      "id": "Bellman_Equation",
      "type": "subnode",
      "parent": "Optimal_Control",
      "description": "Equation used to solve dynamic optimization problems by breaking them down into simpler subproblems."
    },
    {
      "id": "Value_Function",
      "type": "subnode",
      "parent": "Bellman_Equation",
      "description": "Function that represents the long-term reward an agent can achieve from a given state under optimal control policies."
    },
    {
      "id": "Quadratic_Value_Functions",
      "type": "subnode",
      "parent": "Value_Function",
      "description": "Value functions expressed as quadratic forms in states, simplifying optimization problems."
    },
    {
      "id": "Optimal_Policy",
      "type": "subnode",
      "parent": "Bellman_Equation",
      "description": "Policy that maximizes the expected value of the cumulative reward over time."
    },
    {
      "id": "Linear_Policies",
      "type": "subnode",
      "parent": "Optimal_Policy",
      "description": "Policies that are linear functions of state variables, facilitating computation and analysis."
    },
    {
      "id": "Optimal Policy in LQR",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Discussion on optimal policy formulation within the Linear Quadratic Regulator framework."
    },
    {
      "id": "Discrete Ricatti Equations",
      "type": "subnode",
      "parent": "Optimal Policy in LQR",
      "description": "Mathematical equations used to iteratively update control parameters in an optimal policy setting."
    },
    {
      "id": "LQR Algorithm Steps",
      "type": "subnode",
      "parent": "Optimal Policy in LQR",
      "description": "Step-by-step procedure for implementing the Linear Quadratic Regulator algorithm."
    },
    {
      "id": "Non-linear Dynamics and LQR",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Exploration of how non-linear systems can be approximated using linear methods like LQR."
    },
    {
      "id": "Inverted_Pendulum_System",
      "type": "subnode",
      "parent": "Dynamics_and_Control_Problems",
      "description": "Example system used to illustrate dynamics and control problems."
    },
    {
      "id": "State_Transitions",
      "type": "subnode",
      "parent": "Inverted_Pendulum_System",
      "description": "Description of state transitions in the inverted pendulum system."
    },
    {
      "id": "Linearization_of_Dynamics",
      "type": "major",
      "parent": null,
      "description": "Process of linearizing complex dynamics for analysis and control."
    },
    {
      "id": "Taylor_Expansion_Method",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Use of Taylor expansion to approximate non-linear systems with linear ones."
    },
    {
      "id": "LQR_Assumptions_Similarity",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Similarity between linearized dynamics and assumptions in Linear Quadratic Regulator (LQR) theory."
    },
    {
      "id": "Differential_Dynamic_Programming",
      "type": "major",
      "parent": null,
      "description": "Optimization technique for solving control problems by iteratively improving the solution."
    },
    {
      "id": "Nominal Trajectory",
      "type": "subnode",
      "parent": "Differential Dynamic Programming (DDP)",
      "description": "Initial approximation of the desired trajectory using simple control methods."
    },
    {
      "id": "Linearization Around Trajectory Points",
      "type": "subnode",
      "parent": "Differential Dynamic Programming (DDP)",
      "description": "Process of approximating dynamics around each point on a nominal trajectory."
    },
    {
      "id": "Non-Stationary Dynamics",
      "type": "subnode",
      "parent": "Linearization Around Trajectory Points",
      "description": "Dynamic system where parameters vary over time or with state."
    },
    {
      "id": "Optimization_Frameworks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Frameworks used for optimization in machine learning problems."
    },
    {
      "id": "Linear_Quadratic_Regulator_(LQR)",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "A control strategy that minimizes a quadratic cost function over time."
    },
    {
      "id": "Hessian_Matrix",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "Matrix of second-order partial derivatives used in optimization problems."
    },
    {
      "id": "Linear_Quadratic_Gaussian_(LQG)",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Extension of LQR to stochastic systems with noisy observations."
    },
    {
      "id": "Partially Observable MDPs (POMDP)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "MDPs with an additional observation layer to handle partial observability."
    },
    {
      "id": "Observation Layer",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Introduces observations as a conditional distribution given the state."
    },
    {
      "id": "Belief State",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Maintains a belief state based on past observations to predict future states."
    },
    {
      "id": "Policy in POMDP",
      "type": "subnode",
      "parent": "Belief State",
      "description": "Maps the belief state to actions for decision-making under uncertainty."
    },
    {
      "id": "LQR Extension",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Extends Linear Quadratic Regulator (LQR) to handle partial observability scenarios."
    },
    {
      "id": "Kalman Filter",
      "type": "subnode",
      "parent": "LQR Extension",
      "description": "A recursive algorithm for estimating the state of a system over time."
    },
    {
      "id": "Step 1",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Initial step to set up the system dynamics"
    },
    {
      "id": "Dynamics Model",
      "type": "subnode",
      "parent": "Step 1",
      "description": "Model describing state transitions and observations"
    },
    {
      "id": "Gaussian Distribution",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Distribution used due to Gaussian noise in system"
    },
    {
      "id": "Predict Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Estimates the next state based on current belief and transition model."
    },
    {
      "id": "Update Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Refines the prediction using new observation data."
    },
    {
      "id": "Belief States",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Represents the probability distribution of states given observations."
    },
    {
      "id": "Transition Model",
      "type": "subnode",
      "parent": "Predict Step",
      "description": "Describes how the system evolves over time."
    },
    {
      "id": "Kalman Gain",
      "type": "subnode",
      "parent": "Update Step",
      "description": "Weighted average factor between prediction and observation update."
    },
    {
      "id": "LQR Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Linear Quadratic Regulator algorithm for optimal control problems."
    },
    {
      "id": "Policy Gradient Methods",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Model-free reinforcement learning methods that optimize policy directly."
    },
    {
      "id": "REINFORCE Algorithm",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Specific model-free algorithm for optimizing randomized policies."
    },
    {
      "id": "Optimal Policy Recovery",
      "type": "subnode",
      "parent": "LQR Algorithm",
      "description": "Process of recovering the optimal policy from computed quantities."
    },
    {
      "id": "Finite Horizon Trajectories",
      "type": "subnode",
      "parent": "REINFORCE Algorithm",
      "description": "Trajectories with a fixed length T used in REINFORCE algorithm."
    },
    {
      "id": "Randomized Policy Learning",
      "type": "subnode",
      "parent": "REINFORCE Algorithm",
      "description": "Learning policies that output actions probabilistically based on states."
    },
    {
      "id": "Policy_Gradient_Methods",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Optimization techniques for improving policies in reinforcement learning."
    },
    {
      "id": "Gradient_Ascend_Optimization",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Optimizing policy parameters by maximizing the expected reward gradient."
    },
    {
      "id": "Reward_Function_Estimation",
      "type": "subnode",
      "parent": "Gradient_Ascend_Optimization",
      "description": "Estimating gradients without knowing the exact form of the reward function."
    },
    {
      "id": "Reparametrization_Technique",
      "type": "subnode",
      "parent": "Variational_Autoencoder",
      "description": "Method used in VAEs to compute gradients through stochastic nodes."
    },
    {
      "id": "REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Algorithm for estimating policy gradients using likelihood ratios."
    },
    {
      "id": "Policy_Gradient_Theorem",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "A theorem that describes the gradient of a policy parameter with respect to expected reward."
    },
    {
      "id": "Expectation_Maximization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Estimation of expected values using samples from a distribution."
    },
    {
      "id": "Log_Probability_Computation",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Calculation and differentiation of log probabilities for policy gradients."
    },
    {
      "id": "Policy_Gradient_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of policy gradient methods in reinforcement learning."
    },
    {
      "id": "Log_Probability_Gradients",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Derivation and computation of gradients for log probabilities of actions given states."
    },
    {
      "id": "Vanilla_REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Description of the basic REINFORCE algorithm using policy gradient estimates."
    },
    {
      "id": "Trajectory_Probability_Change",
      "type": "subnode",
      "parent": "Log_Probability_Gradients",
      "description": "Explanation of how gradients affect trajectory probabilities in a policy."
    },
    {
      "id": "Empirical_Estimation",
      "type": "subnode",
      "parent": "Vanilla_REINFORCE_Algorithm",
      "description": "Methodology for estimating the right-hand side of the policy gradient equation using sample trajectories."
    },
    {
      "id": "Reinforcement_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Overview",
      "description": "Learning through interaction with an environment to maximize cumulative reward."
    },
    {
      "id": "Trajectory_Probability",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Probability of a sequence of actions and states under a policy."
    },
    {
      "id": "Reward_Function",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Function defining the reward for each state-action pair in reinforcement learning."
    },
    {
      "id": "Gradient_Estimation",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Estimating gradients of policy parameters with respect to expected rewards."
    },
    {
      "id": "Law_of_Total_Expectation",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Statistical rule used in deriving simplifications within the policy gradient theorem."
    },
    {
      "id": "Estimator_Simplification",
      "type": "subnode",
      "parent": "Law_of_Total_Expectation",
      "description": "Simplifying the estimator by applying the law of total expectation."
    },
    {
      "id": "Value_Functions",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Functions that represent expected cumulative rewards from a given state under a policy."
    },
    {
      "id": "Baseline_Estimator",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Reduces variance of policy gradient estimates by subtracting a baseline from rewards."
    },
    {
      "id": "Algorithm_7_Vanilla_Policy_Gradient",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Describes a method for updating policy parameters using gradients and baselines."
    },
    {
      "id": "Machine_Learning_Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of recent papers in machine learning and related fields."
    },
    {
      "id": "Double_Descent_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Models explaining double descent phenomenon for weak features in machine learning tasks."
    },
    {
      "id": "Variational_Inference_Review",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Comprehensive review of variational inference techniques from a statistical perspective."
    },
    {
      "id": "Foundation_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Overview and analysis of opportunities and risks associated with foundation models in AI."
    },
    {
      "id": "Few_Shot_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Research on the capability of language models to perform few-shot learning tasks."
    },
    {
      "id": "Contrastive_Learning_Visual_Representations",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Framework for contrastive learning applied to visual representation in machine learning."
    },
    {
      "id": "BERT_Model",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Introduction of BERT model for pre-training deep bidirectional transformers for language understanding."
    },
    {
      "id": "Implicit_Bias_Noise_Covariance",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Study on the implicit bias introduced by noise covariance in machine learning models."
    },
    {
      "id": "High_Dimensional_Statistics",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Discussion on unexpected phenomena and challenges in high-dimensional statistical analysis."
    },
    {
      "id": "Machine Learning Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of papers related to machine learning research and theory."
    },
    {
      "id": "Implicit Bias in Machine Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Studies on the implicit biases in machine learning models and algorithms."
    },
    {
      "id": "High-Dimensional Interpolation",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on interpolation properties of high-dimensional data sets."
    },
    {
      "id": "Deep Residual Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Paper discussing deep residual learning for image recognition tasks."
    },
    {
      "id": "Statistical Learning Textbook",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Introduction to statistical learning, covering various machine learning topics."
    },
    {
      "id": "Optimization Methods",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Methods for stochastic optimization in machine learning models."
    },
    {
      "id": "Variational Bayes",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on auto-encoding variational bayes techniques."
    },
    {
      "id": "Model-Based Deep RL",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Framework for model-based deep reinforcement learning with theoretical guarantees."
    },
    {
      "id": "Generalization Error Analysis",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Analysis of the generalization error in random features regression models."
    },
    {
      "id": "Statistical Mechanics of Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on statistical mechanics principles applied to learning theory."
    },
    {
      "id": "double_descent",
      "type": "major",
      "parent": null,
      "description": "Machine learning phenomenon where performance improves after an initial decline."
    },
    {
      "id": "statistical_mechanics_of_learning",
      "type": "subnode",
      "parent": "double_descent",
      "description": "Application of statistical mechanics principles to understand generalization in machine learning models."
    },
    {
      "id": "generalization",
      "type": "subnode",
      "parent": "statistical_mechanics_of_learning",
      "description": "Process by which a model learns from training data and applies knowledge to unseen data."
    },
    {
      "id": "learning_to_generalize",
      "type": "subnode",
      "parent": "double_descent",
      "description": "Exploration of how learning processes enable generalization in machine learning models."
    }
  ],
  "edges": [
    {
      "from": "Major Axis of Variation",
      "to": "Data Approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Supervised Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "State Sampling",
      "relationship": "subtopic"
    },
    {
      "from": "GradientDescent",
      "to": "UpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "II Deep learning",
      "to": "7 Deep learning",
      "relationship": "contains"
    },
    {
      "from": "M_Step",
      "to": "ELBO",
      "relationship": "uses"
    },
    {
      "from": "Validation_Set",
      "to": "Hold_Out_Cross_Validation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Cross_Entropy_Loss",
      "relationship": "has_subtopic"
    },
    {
      "from": "Learning Settings",
      "to": "Domain Shift",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Least_Squares_Regression",
      "relationship": "contains"
    },
    {
      "from": "Feature_Vector_Selection",
      "to": "Stop_Words_Exclusion",
      "relationship": "subtopic_of"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "LearningRateAdjustment",
      "relationship": "technique_to_improve"
    },
    {
      "from": "PrimalProblem",
      "to": "GeneralizedLagrangian",
      "relationship": "defines"
    },
    {
      "from": "Foundation Models",
      "to": "Pretraining and Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "KernelMethods",
      "to": "FeatureMaps",
      "relationship": "subtopic"
    },
    {
      "from": "ParameterScalingInvariance",
      "to": "DecisionBoundary",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "LinearRegressionLimitations",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianKernel",
      "to": "KernelsAsSimilarityMetrics",
      "relationship": "subtopic"
    },
    {
      "from": "DoubleDescentPhenomenon",
      "to": "ModelComplexityMeasures",
      "relationship": "subtopic"
    },
    {
      "from": "TrainingExamplesMatrixNotation",
      "to": "Vectorization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Optimal_Control",
      "relationship": "related_to"
    },
    {
      "from": "RegressionProblems",
      "to": "LeastSquareCostFunction",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.1 Logistic regression",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Sample-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "GenerativeAlgorithms",
      "to": "BayesRule",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningBasics",
      "to": "TrainingExamplesMatrixNotation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Binary Classification",
      "to": "Learning Theory Proofs",
      "relationship": "subtopic"
    },
    {
      "from": "Reward Function",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Overview",
      "to": "Chain Rule Perspective",
      "relationship": "subtopic"
    },
    {
      "from": "UpperBoundOnGeneralizationError",
      "to": "GeneralizationErrorGuarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Chain_Rule_Application",
      "to": "Gradient_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "StateTransitionModeling",
      "to": "DeterministicVsStochasticModels",
      "relationship": "subtopic"
    },
    {
      "from": "1.3 Probabilistic interpretation",
      "to": "Gaussian distribution",
      "relationship": "assumes"
    },
    {
      "from": "Non-Stationary Policies",
      "to": "Optimal Policy in Finite-Horizon",
      "relationship": "subtopic"
    },
    {
      "from": "High-Dimensional Interpolation",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "StateTransitionModeling",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Classification Problem",
      "relationship": "related_to"
    },
    {
      "from": "TrainingErrorMean",
      "to": "UpperBoundOnGeneralizationError",
      "relationship": "subtopic"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "Preliminaries",
      "relationship": "depends_on"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "BatchGradientDescent",
      "relationship": "contrasts_with"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "Taylor_Expansion_Method",
      "relationship": "subtopic"
    },
    {
      "from": "9.1Regularization",
      "to": "RegularizerFunction",
      "relationship": "subtopic"
    },
    {
      "from": "DynamicProgrammingApplication",
      "to": "Step2PolicyDerivation",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Challenges",
      "to": "Data_Scarcity_Impact",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement learning",
      "to": "Markov decision processes",
      "relationship": "depends_on"
    },
    {
      "from": "Convolutional_Layers",
      "to": "Efficiency_of_Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "Evidence_Lower_Bound_(ELBO)",
      "to": "Jensen's_Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "ICA_On_Gaussian_Data",
      "relationship": "has_subtopic"
    },
    {
      "from": "Non-Stationary Policies",
      "to": "Time Dependent Dynamics",
      "relationship": "depends_on"
    },
    {
      "from": "Prediction_Equation",
      "to": "Support_Vectors",
      "relationship": "related_to"
    },
    {
      "from": "Value Function Approximation",
      "to": "Model or Simulator",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "CostFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Conv1D_Simplified",
      "to": "Bias_Scalar",
      "relationship": "has_subcomponent"
    },
    {
      "from": "StateTransitionModeling",
      "to": "LinearModelPrediction",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Gradient_Descent",
      "relationship": "contains"
    },
    {
      "from": "L2 Regularization",
      "to": "Regularizer R(θ)",
      "relationship": "subtopic"
    },
    {
      "from": "Test Error vs Training Error",
      "to": "Generalization Gap",
      "relationship": "subtopic"
    },
    {
      "from": "1D_Convolution",
      "to": "Conv1D_Simplified",
      "relationship": "subtopic"
    },
    {
      "from": "Adaptation_Phase",
      "to": "Downstream_Task_Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "2.3 Multi-class classification",
      "to": "Response Variable",
      "relationship": "depends_on"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.1 Preliminaries on partial derivatives",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimization Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Vectorization over training examples",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "GradientAscentRule",
      "relationship": "subtopic"
    },
    {
      "from": "1 Linear regression",
      "to": "1.3 Probabilistic interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "2 Classification and logistic regression",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ConvergenceAndRecovery",
      "relationship": "subtopic"
    },
    {
      "from": "SigmoidFunction",
      "to": "GradientVanishing",
      "relationship": "depends_on"
    },
    {
      "from": "KernelFunctions",
      "to": "ComputationalEfficiency",
      "relationship": "has_subtopic"
    },
    {
      "from": "E_Step",
      "to": "LogLikelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Coordinate_Ascend_Method",
      "relationship": "depends_on"
    },
    {
      "from": "Union Bound Lemma",
      "to": "Hoeffding Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Model Selection Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Training_Error_Generalization_Error",
      "to": "Uniform_Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "ICA_Overview",
      "relationship": "contains"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.3 LMS with the kernel trick",
      "relationship": "subtopic"
    },
    {
      "from": "Conv1D_Simplified",
      "to": "Matrix_Multiplication_Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Partially Observable MDPs (POMDP)",
      "relationship": "contains"
    },
    {
      "from": "TrainingExamplesMatrixNotation",
      "to": "LayerActivations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Locally_weighted_linear_regression",
      "relationship": "contains"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Gradient_Calculation",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Markov Decision Processes (MDPs)",
      "relationship": "subtopic"
    },
    {
      "from": "UniformConvergence",
      "to": "TrainingErrorGeneralizationGap",
      "relationship": "depends_on"
    },
    {
      "from": "LQR Algorithm",
      "to": "Optimal Policy Recovery",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Functional Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning Overview",
      "to": "Policy Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization_in_MDPs",
      "to": "Curse_of_Dimensionality",
      "relationship": "related_to"
    },
    {
      "from": "Kernel_Trick",
      "to": "Dual_Formulation",
      "relationship": "enables"
    },
    {
      "from": "MachineLearningModels",
      "to": "DecisionBoundariesComparison",
      "relationship": "has_subtopic"
    },
    {
      "from": "HighDimensionalFeatures",
      "to": "FeatureMapping",
      "relationship": "related_to"
    },
    {
      "from": "PAC_Framework",
      "to": "Machine_Learning_Basics",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Primal_Dual_Problems",
      "relationship": "depends_on"
    },
    {
      "from": "PredictionCalculation",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Locally_Weighted_Linear_Regression",
      "relationship": "related_to"
    },
    {
      "from": "Double_Descent_Models",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "BellmanEquation",
      "to": "ValueIterationAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "assigning_step",
      "to": "inner_loop",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Non-separable Case",
      "relationship": "subtopic"
    },
    {
      "from": "BestPossibleHypothesis",
      "to": "UniformConvergence",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization_Method",
      "to": "Continuous_State_MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "Machine_Learning_Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "UnifiedTreatment",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "DensityFunctionFormulation",
      "to": "CumulativeDistributionFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Conv2D",
      "to": "Conv1D",
      "relationship": "extends"
    },
    {
      "from": "GaussianDistribution",
      "to": "StandardNormalDistribution",
      "relationship": "related_to"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "VarianceMaximization",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "HypothesisFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "TwoLayerNetworkExample",
      "to": "VectorizationInMachineLearning",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Reward_Function",
      "relationship": "related_to"
    },
    {
      "from": "Generalization and regularization",
      "to": "Bias-variance tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Hypothesis_Class_Expansion",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.2 Digression: the perceptron learning algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Quadratic Gaussian (LQG)",
      "to": "LQR, DDP and LQG",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDistributions",
      "to": "CovarianceMatrixManipulation",
      "relationship": "subtopic"
    },
    {
      "from": "Deep_Learning",
      "to": "Learned_Features",
      "relationship": "subtopic"
    },
    {
      "from": "Mean Squared Error (MSE)",
      "to": "Bias-Variance Decomposition",
      "relationship": "subtopic"
    },
    {
      "from": "ModulesInNetwork",
      "to": "ForwardPass",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Non-linear Dynamics and LQR",
      "relationship": "related_to"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Heuristic_Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Hold-out Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Log_Likelihood_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Variational_Autoencoder",
      "relationship": "related_to"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Unlabeled_Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "GradientDescent",
      "to": "CostFunctionJ",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Sample_Complexity_Bound",
      "relationship": "subtopic"
    },
    {
      "from": "1.3 Probabilistic interpretation",
      "to": "Input variables x^(i)",
      "relationship": "defines"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.2 Logistic regression",
      "relationship": "contains"
    },
    {
      "from": "ScalingInvariantProperty",
      "to": "LN-S",
      "relationship": "depends_on"
    },
    {
      "from": "LQR_Overview",
      "to": "Linear_Transitions",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Positive Semi-Definiteness",
      "relationship": "depends_on"
    },
    {
      "from": "KernelTrickIntroduction",
      "to": "RuntimeOptimization",
      "relationship": "leads_to"
    },
    {
      "from": "Update Step",
      "to": "Kalman Gain",
      "relationship": "has_subtopic"
    },
    {
      "from": "Belief State",
      "to": "Policy in POMDP",
      "relationship": "depends_on"
    },
    {
      "from": "E_Step",
      "to": "M_Step",
      "relationship": "followed_by"
    },
    {
      "from": "Backpropagation",
      "to": "Backward functions for basic modules",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization_in_MDPs",
      "to": "Piecewise_Constant_Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Bellman_Equation",
      "to": "Recovering_Optimal_Policy",
      "relationship": "depends_on"
    },
    {
      "from": "Back-propagation for MLPs",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Step 1",
      "to": "Dynamics Model",
      "relationship": "depends_on"
    },
    {
      "from": "Algorithm 6",
      "to": "VE Procedure",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Model Error Analysis",
      "relationship": "contains"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.1 Gaussian discriminant analysis",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Convergence_Criteria",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "PoissonDataExample",
      "relationship": "illustrates"
    },
    {
      "from": "RegressionProblems",
      "to": "MeanSquaredCostFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "1D_Convolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Discretization",
      "to": "Continuous state MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Linear Regression Models",
      "relationship": "subtopic"
    },
    {
      "from": "Hidden_Units",
      "to": "ReLU_Activation",
      "relationship": "uses"
    },
    {
      "from": "7 Deep learning",
      "to": "7.3 Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "ICA_On_Gaussian_Data",
      "to": "Rotational_Symmetry_Implication",
      "relationship": "subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Batch Normalization (BN)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Calculation Example",
      "to": "Matrix Derivatives",
      "relationship": "is_subtopic_of"
    },
    {
      "from": "DiscriminativeAlgorithms",
      "to": "PerceptronAlgorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Mini-batch Stochastic Gradient Descent",
      "relationship": "contains"
    },
    {
      "from": "Mixing_Matrix_Rotation",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "ReinforcementLearning",
      "to": "ValueFunctionDefinition",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient_Ascend_Optimization",
      "to": "Reward_Function_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Deep_Learning_Concepts",
      "to": "Black_Box_Models",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "GaussianDistributions",
      "to": "MeanVectorVariation",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticFunction",
      "to": "ProbabilityDistributions",
      "relationship": "related_to"
    },
    {
      "from": "moving_step",
      "to": "inner_loop",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Computational Challenges",
      "relationship": "depends_on"
    },
    {
      "from": "Loss_Functions",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "ConditionalProbability",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Learning from Data",
      "to": "Data Collection",
      "relationship": "depends_on"
    },
    {
      "from": "BackPropagationAlgorithm",
      "to": "IntermediateValuesStorage",
      "relationship": "related_to"
    },
    {
      "from": "Alpha Update Process",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical_Risk_Minimization_(ERM)",
      "to": "Sample_Complexity",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "TanhFunction",
      "relationship": "contains"
    },
    {
      "from": "Optimizers",
      "to": "Generalization",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Action Selection",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Gaussian Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Probability Estimation",
      "to": "Naive Bayes Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "KKT_Conditions",
      "to": "Convergence_Tolerance",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Batch Size",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Pretraining_and_Adaptation",
      "to": "Adaptation_Phase",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Training Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "Partial_Derivatives",
      "to": "Mathematical_Notations",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization and model selection",
      "to": "Model selection via cross validation",
      "relationship": "subtopic"
    },
    {
      "from": "Law_of_Total_Expectation",
      "to": "Estimator_Simplification",
      "relationship": "leads_to"
    },
    {
      "from": "Feature Mapping",
      "to": "Kernel Matrix Properties",
      "relationship": "related_to"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Optimizer Impact on Generalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "NeuralNetworks",
      "relationship": "contains"
    },
    {
      "from": "Maximizing Functions",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "Convergence Issues",
      "to": "Fitted Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Original Loss J(θ)",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "SMO_Algorithm",
      "to": "KKT_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LMSAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "ThetaVectorRepresentation",
      "to": "UpdateRuleDerivation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "SigmoidFunction",
      "relationship": "defines"
    },
    {
      "from": "Inverted_Pendulum_System",
      "to": "State_Transitions",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "FittingTheta",
      "relationship": "subtopic_of"
    },
    {
      "from": "CanonicalResponseFunction",
      "to": "CanonicalLinkFunction",
      "relationship": "inverse_of"
    },
    {
      "from": "Hold-out Cross Validation",
      "to": "Validation Set (S_cv)",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "FeatureSelection",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Notation for SVMs",
      "relationship": "depends_on"
    },
    {
      "from": "GradientEstimation",
      "to": "ReparameterizationTrick",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Implicit Regularization Effect",
      "relationship": "has_subtopic"
    },
    {
      "from": "Log_Probability_Bound",
      "to": "Jensen's_Inequality",
      "relationship": "uses"
    },
    {
      "from": "Posterior Distribution Approximation",
      "to": "MLE Estimate",
      "relationship": "related_to"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Observation Layer",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Logistic Loss Function",
      "relationship": "contains"
    },
    {
      "from": "Optimizers",
      "to": "Initialization",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Layers",
      "to": "1-D Convolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "ModelParameters",
      "to": "LogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "RedundancyDetection",
      "to": "LinearDependency",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Policy_Iteration_Speedup",
      "relationship": "has_subtopic"
    },
    {
      "from": "SIMCLR",
      "to": "Augmentation",
      "relationship": "uses"
    },
    {
      "from": "EfficiencyConcerns",
      "to": "VectorizationInMachineLearning",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Feature_Engineering",
      "to": "Feature_Maps",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Error_Term",
      "relationship": "subtopic"
    },
    {
      "from": "Mixture of Gaussians",
      "to": "E-step Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Hypothesis",
      "relationship": "depends_on"
    },
    {
      "from": "ParallelismAndGPUs",
      "to": "VectorizationInMachineLearning",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Trajectory_Probability",
      "relationship": "depends_on"
    },
    {
      "from": "Generative_Modeling",
      "to": "Naive_Bayes_Assumption",
      "relationship": "has_subtopic"
    },
    {
      "from": "VC_Dimension_Theory",
      "to": "Shattering",
      "relationship": "defines"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "LikelihoodFunction",
      "to": "IndependenceAssumption",
      "relationship": "depends_on"
    },
    {
      "from": "Remark",
      "to": "Density Transformation",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Function Properties",
      "to": "Sufficient Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "statistical_mechanics_of_learning",
      "to": "generalization",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization and model selection",
      "to": "Implicit regularization effect (optional reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Bayesian statistics and regularization",
      "relationship": "subtopic"
    },
    {
      "from": "GammaCalculation",
      "to": "GeometricMargin",
      "relationship": "related_to"
    },
    {
      "from": "Policy Execution",
      "to": "Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Few_Shot_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Model_Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep Residual Learning",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Continuous state MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Definition",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "Densities and linear transformations",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional Probability",
      "to": "Softmax Function",
      "relationship": "depends_on"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "NaturalParameter",
      "relationship": "depends_on"
    },
    {
      "from": "BiasVarianceTradeoff",
      "to": "MSECalculation",
      "relationship": "subtopic"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Feature_Map_Phi",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "subtopic"
    },
    {
      "from": "Least_Squares_Regression",
      "to": "Probabilistic_Assumptions",
      "relationship": "requires"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "K_Means_Clustering",
      "relationship": "contrasts_with"
    },
    {
      "from": "LQRModelAssumptions",
      "to": "Step1Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Evidence_Lower_Bound_(ELBO)",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "ConditionalDistributionModeling",
      "relationship": "contains"
    },
    {
      "from": "Naive_Bayes_Assumption",
      "to": "Conditional_Independence",
      "relationship": "explains"
    },
    {
      "from": "Optimization Problem",
      "to": "Geometric Margin",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Model Complexity",
      "relationship": "related_to"
    },
    {
      "from": "Noise (σ^2)",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "\\(\\ell_{1}\\) Regularization",
      "relationship": "related_to"
    },
    {
      "from": "NaiveBayesClassifier",
      "to": "ParameterEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "MLPArchitecture",
      "to": "NonlinearActivationModule",
      "relationship": "depends_on"
    },
    {
      "from": "Cocktail_Party_Problem",
      "to": "Unmixing_Matrix_W",
      "relationship": "related_to"
    },
    {
      "from": "Bellman Equations",
      "to": "Future Discounted Rewards",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GaussianMixtureModels",
      "relationship": "related_to"
    },
    {
      "from": "Other Activation Functions",
      "to": "ReLU Activation Function",
      "relationship": "replaces"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.2 Constructing GLMs",
      "relationship": "subtopic"
    },
    {
      "from": "Infinite_Hypothesis_Classes",
      "to": "Finite_Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningBasics",
      "to": "CostFunction",
      "relationship": "related_to"
    },
    {
      "from": "ProbabilityFormula",
      "to": "LikelihoodFunction",
      "relationship": "related_to"
    },
    {
      "from": "Sparsity of Model Parameters",
      "to": "Gradient Descent Compatibility",
      "relationship": "related_to"
    },
    {
      "from": "Backward_Functions_Basic_Modules",
      "to": "Activation_Backward_Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "M_Step_Update_Rule",
      "to": "Lagrangian_Method",
      "relationship": "uses"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "School_Quality",
      "relationship": "depends_on"
    },
    {
      "from": "Bellman Equations",
      "to": "Markov Decision Processes (MDPs)",
      "relationship": "related_to"
    },
    {
      "from": "Transformer Model",
      "to": "Training Process",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Gaussian_Mixture_Models",
      "relationship": "related_to"
    },
    {
      "from": "7 Deep learning",
      "to": "7.1 Supervised learning with non-linear models",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Non-Convex Constraint",
      "relationship": "has_subtopic"
    },
    {
      "from": "RegularizerFunction",
      "to": "RegularizedLoss",
      "relationship": "defines"
    },
    {
      "from": "Loss_Functions",
      "to": "Training_Loss",
      "relationship": "subtopic"
    },
    {
      "from": "ModulesInNetwork",
      "to": "BackwardPass",
      "relationship": "contains"
    },
    {
      "from": "Learning Theory",
      "to": "Union Bound Lemma",
      "relationship": "contains"
    },
    {
      "from": "Supervised Learning",
      "to": "Regression Problem",
      "relationship": "related_to"
    },
    {
      "from": "Connections between Policy and Value Iteration (Optional)",
      "to": "Learning a model for an MDP",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilityOfData",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "Logit",
      "relationship": "depends_on"
    },
    {
      "from": "Variational_Inference_Review",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianMixtureModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "SMO_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "ReparametrizationTrick",
      "to": "VariationalAutoEncoder",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Overfitting",
      "relationship": "subtopic"
    },
    {
      "from": "RewardFunction",
      "to": "OptimalActionComputation",
      "relationship": "subtopic"
    },
    {
      "from": "Predict Step",
      "to": "Gaussian Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Iteration",
      "to": "Markov Decision Processes (MDPs)",
      "relationship": "subtopic"
    },
    {
      "from": "Properties_of_Kernels",
      "to": "Kernel_Characterization",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianMixtureModel",
      "to": "EMAlgorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "LossFunction",
      "relationship": "defines"
    },
    {
      "from": "EM_Algorithms",
      "to": "Log_Likelihood_Maximization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "LQR Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "contains"
    },
    {
      "from": "Margins_Intuition",
      "to": "Logistic_Regression_Analogy",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Deep_Learning",
      "relationship": "related_to"
    },
    {
      "from": "Deep_Learning",
      "to": "Neural_Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel_Trick",
      "to": "Inner_Products",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Bias_Variance_Tradeoff",
      "to": "Infinite_Hypothesis_Classes",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "SMO_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Overview",
      "relationship": "depends_on"
    },
    {
      "from": "ActivationFunctions",
      "to": "IdentityFunction",
      "relationship": "contains"
    },
    {
      "from": "NeuralNetworkBasics",
      "to": "WeightMatrices",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "BinaryFeatures",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "4 Generative learning algorithms",
      "relationship": "contains"
    },
    {
      "from": "BLASOptimization",
      "to": "VectorizationInMachineLearning",
      "relationship": "subtopic"
    },
    {
      "from": "joint_distribution_modeling",
      "to": "unsupervised_learning",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman Equations",
      "to": "Immediate Reward",
      "relationship": "related_to"
    },
    {
      "from": "NormalEquations",
      "to": "MatrixDerivatives",
      "relationship": "uses"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "OverparameterizedRegime",
      "relationship": "related_to"
    },
    {
      "from": "Step1Estimation",
      "to": "Step2PolicyDerivation",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Error_Bound",
      "to": "Uniform_Convergence_Assumption",
      "relationship": "depends_on"
    },
    {
      "from": "BernoulliDistribution",
      "to": "ExponentialFamilyDistributions",
      "relationship": "part_of"
    },
    {
      "from": "Optimal Margin Classifier",
      "to": "Linear Separability",
      "relationship": "depends_on"
    },
    {
      "from": "BiasVarianceTradeoff",
      "to": "DecompositionOfMSE",
      "relationship": "subtopic"
    },
    {
      "from": "Training Process",
      "to": "Cross-Entropy Loss",
      "relationship": "uses"
    },
    {
      "from": "Value_Iteration",
      "to": "Geometric_Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Sequential Decision Making",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "PolynomialModelFitting",
      "to": "OverfittingExample",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Loss Function Computation",
      "relationship": "contains"
    },
    {
      "from": "Jensen's_Inequality",
      "to": "Convex_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworkBasics",
      "to": "TwoLayerNN",
      "relationship": "subtopic"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Hyperparameters",
      "relationship": "depends_on"
    },
    {
      "from": "E-step Calculation",
      "to": "Expectation-Maximization Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Statistical Mechanics of Learning",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Inverted_Pendulum_System",
      "relationship": "depends_on"
    },
    {
      "from": "Primal_Dual_Problems",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "VariationalInference",
      "to": "ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Classical Theoretical Results",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Feature_Vector_Selection",
      "relationship": "subtopic_of"
    },
    {
      "from": "LogisticRegression",
      "to": "TotalLossFunction",
      "relationship": "subtopic"
    },
    {
      "from": "DataNormalization",
      "to": "MeanVarianceCalculation",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Value_Iteration_Preference",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Sample_Complexity",
      "to": "Hypothesis_Class_Size",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "VC_Dimension_Theory",
      "relationship": "contains"
    },
    {
      "from": "Asynchronous Updates",
      "to": "Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Spam_Filtering",
      "to": "Training_Set",
      "relationship": "depends_on"
    },
    {
      "from": "LQR_Overview",
      "to": "Finite_Horizon_Setting",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman_Equations",
      "to": "Bellman_Operator",
      "relationship": "related_to"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Noise Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "\\(\\mathcal{L}(w,b,\\xi,\\alpha,r)\\)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Fundamentals",
      "to": "Loss_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Stochastic Gradient Ascent",
      "relationship": "contains"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "The case of finite H",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "NeuralNetworkBasics",
      "relationship": "depends_on"
    },
    {
      "from": "BernoulliDistribution",
      "to": "NaturalParameterForBernoulli",
      "relationship": "subtopic"
    },
    {
      "from": "KKT_Conditions",
      "to": "Dual_Complementarity",
      "relationship": "subtopic"
    },
    {
      "from": "String Classification Example",
      "to": "Kernel Matrix Properties",
      "relationship": "subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Layer Normalization (LN)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Optimization_Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Backpropagation",
      "to": "Efficient_Backward_Propagation",
      "relationship": "related_to"
    },
    {
      "from": "Optimal_Policy",
      "to": "Linear_Policies",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Fisher Scoring",
      "relationship": "depends_on"
    },
    {
      "from": "Latent_Variables",
      "to": "Gaussian_Distribution_Qi",
      "relationship": "depends_on"
    },
    {
      "from": "Finite Horizon MDP",
      "to": "Time in State Representation",
      "relationship": "subtopic"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "Value_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "SingleNeuronModel",
      "to": "HousingPricePrediction",
      "relationship": "example_of"
    },
    {
      "from": "Machine_Learning",
      "to": "Contrastive_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "MAP Estimate",
      "to": "Prior Choice",
      "relationship": "subtopic"
    },
    {
      "from": "FunctionRepresentation",
      "to": "LinearHypothesis",
      "relationship": "depends_on"
    },
    {
      "from": "1.3 Probabilistic interpretation",
      "to": "Target variable y^(i)",
      "relationship": "defines"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "Spam_Filtering",
      "to": "Feature_Vector",
      "relationship": "uses"
    },
    {
      "from": "Maximum_Likelihood_Estimation",
      "to": "Likelihood_Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Hypothesis_Class_Parameterization",
      "relationship": "contains"
    },
    {
      "from": "Statistical Learning Textbook",
      "to": "Machine Learning Papers",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelTrickIntroduction",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Primal Constraints",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "HypothesisFormulation",
      "relationship": "follows_from"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Efficient_Computation_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.2 Least squares revisited",
      "relationship": "contains"
    },
    {
      "from": "Policy_Iteration",
      "to": "Greedy_Policy",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Prior Distribution",
      "relationship": "includes_concept"
    },
    {
      "from": "Vocabulary",
      "to": "Stop_Words",
      "relationship": "includes"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Mapping",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "contains"
    },
    {
      "from": "SpamNonSpamClassification",
      "to": "WordGenerationProcess",
      "relationship": "depends_on"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Gradient Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Model-Based Deep RL",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "b Variable Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "distortion_function",
      "to": "k-means_algorithm",
      "relationship": "related_to"
    },
    {
      "from": "BERT_Model",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "ICA_Overview",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "TrainingErrorGeneralizationGap",
      "to": "SampleComplexityDefinition",
      "relationship": "related_to"
    },
    {
      "from": "Model Creation Methods",
      "to": "Learning from Data",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation_Methods",
      "to": "Pretraining_Methods_Computer_Vision",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Backpropagation",
      "to": "Chain_Rule_Application",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Value_Functions",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConferences",
      "to": "NeurIPSConference",
      "relationship": "contains"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Weight Matrices and Biases",
      "relationship": "depends_on"
    },
    {
      "from": "TwoLayerNN",
      "to": "IdentityFunction",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Function_Margin",
      "relationship": "subtopic"
    },
    {
      "from": "BiasVarianceTradeoff",
      "to": "Claim8.1.1",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Efficient_Update",
      "relationship": "related_to"
    },
    {
      "from": "ResNetArchitecture",
      "to": "ResidualBlock",
      "relationship": "depends_on"
    },
    {
      "from": "VarianceMaximization",
      "to": "LagrangeMultipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Constrained_Optimization_Problems",
      "to": "Lagrangian_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Tightening_the_Bound",
      "to": "Posterior_Distribution",
      "relationship": "related_to"
    },
    {
      "from": "MultiClassClassification",
      "to": "Logits",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningArchitectures",
      "to": "TransformerArchitecture",
      "relationship": "contains"
    },
    {
      "from": "Posterior Distribution Approximation",
      "to": "MAP Estimate",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NonLinearModels",
      "relationship": "contains"
    },
    {
      "from": "Neural Networks",
      "to": "Bias",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "Linear_Quadratic_Regulator_(LQR)",
      "relationship": "depends_on"
    },
    {
      "from": "Loss_Functions",
      "to": "Test_Error",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "SoftmaxFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Learning Rate Schedules",
      "relationship": "related_to"
    },
    {
      "from": "Major Axis of Variation",
      "to": "Variance Maximization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Newton's Method",
      "relationship": "compared_with"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Kernel_Function_Defined",
      "relationship": "related_to"
    },
    {
      "from": "Hypothesis_Function",
      "to": "Training_Error",
      "relationship": "related_to"
    },
    {
      "from": "FiniteHorizonMDP",
      "to": "TimeHorizonDefinition",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Chain_Rule",
      "relationship": "subtopic"
    },
    {
      "from": "GradientDescentOptimizer",
      "to": "MinimumNormSolution",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Function",
      "to": "Positive Pair",
      "relationship": "related_to"
    },
    {
      "from": "State_Transition_Probabilities",
      "to": "Maximum_Likelihood_Estimates",
      "relationship": "uses"
    },
    {
      "from": "NonLinearModel",
      "to": "TrainingExamples",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "GDA",
      "relationship": "contains"
    },
    {
      "from": "Backpropagation Overview",
      "to": "General Backprop Strategy",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Learning a model for an MDP",
      "relationship": "subtopic"
    },
    {
      "from": "Transformer Model",
      "to": "Autoregressive Decoding",
      "relationship": "has_subtopic"
    },
    {
      "from": "Vapnik's Theorem",
      "to": "Generalization Error Bound",
      "relationship": "provides"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backward Function Overview",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningBasics",
      "to": "BinaryClassification",
      "relationship": "related_to"
    },
    {
      "from": "Pretraining_Methods_Computer_Vision",
      "to": "Supervised_Pretraining",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "LQR_Assumptions_Similarity",
      "relationship": "related_to"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Data Augmentation",
      "relationship": "depends_on"
    },
    {
      "from": "GradientDescent",
      "to": "LearningRate",
      "relationship": "depends_on"
    },
    {
      "from": "REINFORCE Algorithm",
      "to": "Finite Horizon Trajectories",
      "relationship": "subtopic_of"
    },
    {
      "from": "Posterior Distribution on Parameters",
      "to": "Predictive Posterior Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Variational_Autoencoder",
      "relationship": "related_to"
    },
    {
      "from": "Hypothesis_Class_Expansion",
      "to": "Variance_Increase",
      "relationship": "related_to"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Softmax_Function",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworkBasics",
      "to": "BiasVectors",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDPs)",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "9.1Regularization",
      "to": "ModelComplexityMeasure",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Explanation and Mitigation Strategy",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "ReliableEstimateOfEpsilon",
      "to": "GeneralizationErrorGuarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining and adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "double_descent",
      "to": "learning_to_generalize",
      "relationship": "subtopic"
    },
    {
      "from": "Test_Error",
      "to": "Population_Distribution",
      "relationship": "related_to"
    },
    {
      "from": "CostFunctionJ",
      "to": "PartialDerivative",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "Loss Function Computation",
      "to": "Gradient Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "KKT_Conditions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Zero Mean Adjustment",
      "relationship": "has_subtopic"
    },
    {
      "from": "Likelihood_Maximization",
      "to": "Maximum_Likelihood_Estimation",
      "relationship": "subtopic_of"
    },
    {
      "from": "GenerativeAlgorithms",
      "to": "ClassPriors",
      "relationship": "depends_on"
    },
    {
      "from": "LQR_Overview",
      "to": "Quadratic_Rewards",
      "relationship": "subtopic"
    },
    {
      "from": "Modern_Neural_Network_Modules",
      "to": "MLP_Architecture",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation_Maximization_Algorithm",
      "to": "M_Step_Update_Rule",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDistribution",
      "to": "CovarianceMatrix",
      "relationship": "depends_on"
    },
    {
      "from": "OrdinaryLeastSquares",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "Linear_Classifiers",
      "to": "Machine_Learning_Theory",
      "relationship": "subtopic"
    },
    {
      "from": "Learning_Model_for_MDP",
      "to": "State_Transition_Probabilities",
      "relationship": "depends_on"
    },
    {
      "from": "Deep Learning Introduction",
      "to": "Supervised Learning with Non-Linear Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Few-Shot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum_Likelihood_Estimation",
      "to": "Gradient_Ascend_Method",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization_in_MDPs",
      "to": "Policy_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Weight Vector",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "DataPreprocessing",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Baseline_Estimator",
      "relationship": "has_subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "5 Kernel methods",
      "relationship": "contains"
    },
    {
      "from": "Gradient_Descent",
      "to": "Batch_Gradient_Descent",
      "relationship": "subtopic"
    },
    {
      "from": "7 Deep learning",
      "to": "7.4 Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Functions Backward Pass",
      "to": "Logistic Loss",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "FullyConnectedNN",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "Sequential_Minimal_Optimization_SMO",
      "relationship": "depends_on"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "DiscriminativeAlgorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "VC Dimension",
      "to": "Shattering",
      "relationship": "defines"
    },
    {
      "from": "Relationship to Logistic Regression",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "related_to"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Test Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "Linear_Transformations_And_Densities",
      "to": "Effect_Of_Linear_Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Implementation",
      "to": "Data Representation",
      "relationship": "related_to"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "IncrementalGradientDescent",
      "relationship": "synonym_of"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "SMO_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Loss Function Composition",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "State Transition",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GradientAscent",
      "relationship": "subtopic"
    },
    {
      "from": "Mean Field Assumption",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "BinaryClassificationProblem",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Geometric Margin",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworkParameters",
      "to": "BiologicalInspiration",
      "relationship": "related_to"
    },
    {
      "from": "EM_Algorithms",
      "to": "Latent_Variable_Models",
      "relationship": "subtopic"
    },
    {
      "from": "d_star_p_star_Equality",
      "to": "Convexity_Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Convergence",
      "relationship": "has_subtopic"
    },
    {
      "from": "PrincipalComponentsAnalysis",
      "to": "PCAComputationEfficiency",
      "relationship": "subtopic"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Belief State",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Policy_Gradient_Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "gaussian_distributions",
      "to": "joint_distribution_modeling",
      "relationship": "related_to"
    },
    {
      "from": "Model_Parameterization",
      "to": "Embedding_Features",
      "relationship": "describes"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Log_Probability_Gradients",
      "relationship": "has_subtopic"
    },
    {
      "from": "1 Linear regression",
      "to": "1.1 LMS algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Finding Roots",
      "to": "Newton's Method",
      "relationship": "depends_on"
    },
    {
      "from": "maximum_likelihood_estimation",
      "to": "model_parameters",
      "relationship": "depends_on"
    },
    {
      "from": "EM algorithms",
      "to": "Jensen's inequality",
      "relationship": "related_to"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Permutation Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "PCAAlgorithm",
      "to": "DataNormalization",
      "relationship": "subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "SufficientStatistic",
      "relationship": "depends_on"
    },
    {
      "from": "LinearModelLimitations",
      "to": "UnderfittingExample",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining methods in computer vision",
      "relationship": "subtopic"
    },
    {
      "from": "SingleNeuronModel",
      "to": "ReLUActivationFunction",
      "relationship": "uses"
    },
    {
      "from": "LogisticRegression",
      "to": "ProbabilityFunction",
      "relationship": "related_to"
    },
    {
      "from": "FiniteHorizonMDP",
      "to": "DiscountFactorRole",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Conv1D",
      "relationship": "contains"
    },
    {
      "from": "Scaling_Signal",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "SIMCLR",
      "to": "Loss_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Pretraining_Loss_Function",
      "relationship": "depends_on"
    },
    {
      "from": "initialization_method",
      "to": "k-means_algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "LearnableParameters",
      "relationship": "has_component"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "MLPArchitecture",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Activation Functions Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Chapter9",
      "to": "9.1Regularization",
      "relationship": "contains"
    },
    {
      "from": "Value_Function",
      "to": "Quadratic_Value_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Dual_Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Target Vector y",
      "to": "Least Squares Revisited",
      "relationship": "is_subtopic_of"
    },
    {
      "from": "Zero Mean Adjustment",
      "to": "Unit Variance Scaling",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Natural_Language_Processing",
      "relationship": "contains"
    },
    {
      "from": "Synchronous Updates",
      "to": "Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Self-supervised learning and foundation models",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ELBOOptimization",
      "relationship": "contains"
    },
    {
      "from": "model_parameters",
      "to": "mixture_of_gaussians",
      "relationship": "related_to"
    },
    {
      "from": "VE Procedure",
      "to": "Hyperparameter k",
      "relationship": "depends_on"
    },
    {
      "from": "Corollary on Training Examples",
      "to": "VC Dimension",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal_Control",
      "to": "Bellman_Equation",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModelsGLM",
      "to": "ExponentialFamilyDistributions",
      "relationship": "related_to"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "GDA",
      "relationship": "compares_to"
    },
    {
      "from": "VectorizationTrainingExamples",
      "to": "ParallelismAcrossExamples",
      "relationship": "subtopic"
    },
    {
      "from": "FiniteHorizonMDP",
      "to": "PayoffDefinition",
      "relationship": "has_subtopic"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "NewWordChallenge",
      "relationship": "related_to"
    },
    {
      "from": "Test Error vs Training Error",
      "to": "Overfitting",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworkBasics",
      "to": "LayerOperations",
      "relationship": "subtopic"
    },
    {
      "from": "Transformer Model",
      "to": "Conditional Probability",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Expectation_Maximization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization Parameter λ",
      "to": "Regularized Loss",
      "relationship": "related_to"
    },
    {
      "from": "StateTransitionModeling",
      "to": "NonLinearFeatureMappings",
      "relationship": "subtopic"
    },
    {
      "from": "Least_Squares_Regression",
      "to": "Probabilistic_Assumptions_Classification",
      "relationship": "related_to"
    },
    {
      "from": "Expectation_Computation",
      "to": "Deterministic_Simulator",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Gradient_Ascent_Newtons_Method",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworkBasics",
      "to": "SingleNeuronModel",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Training Set (S)",
      "relationship": "depends_on"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "TrainingSetSizeImpact",
      "relationship": "advantage_over"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Expectation_Maximization_Algorithm",
      "relationship": "includes"
    },
    {
      "from": "Optimization_Problems",
      "to": "Non_Convex_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Algorithm",
      "to": "Gradient_Descent_Update",
      "relationship": "subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "EmpiricalCovarianceMatrix",
      "relationship": "related_to"
    },
    {
      "from": "I Supervised learning",
      "to": "1 Linear regression",
      "relationship": "contains"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Historical Context",
      "relationship": "related_to"
    },
    {
      "from": "Deep_Learning_Concepts",
      "to": "House_Price_Prediction",
      "relationship": "subtopic"
    },
    {
      "from": "Sequential_Minimal_Optimization_SMO",
      "to": "Coordinate_Ascend_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrick",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "Q_iFormulation",
      "to": "GaussianDistribution",
      "relationship": "related_to"
    },
    {
      "from": "EM_Algorithm",
      "to": "Local_Optima_Issue",
      "relationship": "has_consequence"
    },
    {
      "from": "Expectation_Computation",
      "to": "Gaussian_Noise_Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Training Data Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization",
      "to": "Broadcasting",
      "relationship": "depends_on"
    },
    {
      "from": "IntermediateVariables",
      "to": "LossFunction",
      "relationship": "defines"
    },
    {
      "from": "gaussian_mixture_models",
      "to": "em_algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Locally Weighted Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "MultinomialFeatures",
      "to": "Discretization",
      "relationship": "depends_on"
    },
    {
      "from": "W Variable Backward Function",
      "to": "Computational Efficiency",
      "relationship": "has_subtopic"
    },
    {
      "from": "Overfitting",
      "to": "5th Degree Polynomial",
      "relationship": "depends_on"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Frequentist Approach",
      "relationship": "contrasts_with"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Sparsity of Model Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "DataPreprocessing",
      "to": "ProbabilityDistributions",
      "relationship": "depends_on"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "BernoulliEventModel",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "Family_Size",
      "relationship": "depends_on"
    },
    {
      "from": "IndependenceAssumption",
      "to": "LikelihoodExpression",
      "relationship": "subtopic"
    },
    {
      "from": "ICA_Overview",
      "to": "Cocktail_Party_Problem",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilisticModel",
      "to": "NegativeLogLikelihood",
      "relationship": "uses"
    },
    {
      "from": "StochasticGradientDescentAlgorithm",
      "to": "LinearRegressionOptimization",
      "relationship": "related_to"
    },
    {
      "from": "Prediction_Model_Structure",
      "to": "Optimization_Objective",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Policy_Gradient_Methods",
      "relationship": "contains"
    },
    {
      "from": "ValueFunctionDefinition",
      "to": "OptimalValueFunction",
      "relationship": "subtopic"
    },
    {
      "from": "double_descent",
      "to": "statistical_mechanics_of_learning",
      "relationship": "related_to"
    },
    {
      "from": "Pretrained large language models",
      "to": "Zero-shot learning and in-context learning",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Overview",
      "to": "Concrete Backprop Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Parameters Estimation",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.2 General strategy of backpropagation",
      "relationship": "contains"
    },
    {
      "from": "Modern_Neural_Network_Modules",
      "to": "Matrix_Multiplication_Module",
      "relationship": "subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LogLikelihoodFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Linear_Functions",
      "to": "Cubic_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Floating_Point_Precision",
      "to": "Machine_Learning_Theory",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Expectation_Maximization_Guarantees",
      "relationship": "depends_on"
    },
    {
      "from": "Coordinate_Ascend_Method",
      "to": "Quadratic_Function_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "EmbeddingsIntroduction",
      "to": "TransformerModelOverview",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "Optimal_Margin_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning Overview",
      "to": "Total Payoff Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Representation Function",
      "relationship": "depends_on"
    },
    {
      "from": "Vapnik's Theorem",
      "to": "Uniform Convergence",
      "relationship": "implies"
    },
    {
      "from": "Non_Gaussian_Sources",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization Parameters",
      "to": "Model Selection",
      "relationship": "subtopic"
    },
    {
      "from": "Policy",
      "to": "Optimal Policy",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Policy_Gradient_Methods",
      "relationship": "subtopic"
    },
    {
      "from": "MatrixAlgebra",
      "to": "VectorizationInMachineLearning",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NaiveBayesClassifier",
      "relationship": "contains"
    },
    {
      "from": "Support_Vectors",
      "to": "Alpha_i",
      "relationship": "corresponds_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Scaling Constraint",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization_Problems",
      "to": "SMO_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Linearization of dynamics",
      "to": "From non-linear dynamics to LQR",
      "relationship": "subtopic"
    },
    {
      "from": "Pretrained large language models",
      "to": "Open up the blackbox of Transformers",
      "relationship": "depends_on"
    },
    {
      "from": "MLPArchitecture",
      "to": "MatrixMultiplicationModule",
      "relationship": "depends_on"
    },
    {
      "from": "Finite Horizon MDP",
      "to": "Policy at Time t",
      "relationship": "subtopic"
    },
    {
      "from": "Properties_of_Kernels",
      "to": "Feature_Maps_and_Kernels",
      "relationship": "follows"
    },
    {
      "from": "Unsupervised learning",
      "to": "EM algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Least_Squares_Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Methods",
      "to": "Machine Learning Papers",
      "relationship": "depends_on"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "ActivationFunctions",
      "to": "ReLUFunction",
      "relationship": "contains"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "kDimensionalSubspace",
      "relationship": "related_to"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "ReLU Activation Function",
      "relationship": "uses"
    },
    {
      "from": "likelihood_function",
      "to": "model_parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Overview",
      "to": "Reinforcement_Learning",
      "relationship": "contains"
    },
    {
      "from": "Supervised_Learning",
      "to": "Piecewise_Constant_Representation",
      "relationship": "depends_on"
    },
    {
      "from": "Mercer's Theorem",
      "to": "Kernel Matrix Properties",
      "relationship": "depends_on"
    },
    {
      "from": "Posterior Distribution on Parameters",
      "to": "Model Specification",
      "relationship": "subtopic"
    },
    {
      "from": "Approximation Error Minimization",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Eigenvectors and Eigenvalues",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "k_steps_update_frequency",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization_Problems",
      "to": "Support_Vectors_SVM",
      "relationship": "related_to"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Nominal Trajectory",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Machine Learning Papers",
      "relationship": "related_to"
    },
    {
      "from": "Zero-Shot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm 6",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Total Payoff Calculation",
      "to": "Discount Factor (γ)",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "Walkability",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDistribution",
      "to": "DensityPlots",
      "relationship": "related_to"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.3 Backward functions for basic modules",
      "relationship": "contains"
    },
    {
      "from": "OptimizationInML",
      "to": "AlphaConstraints",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis_Function",
      "to": "Generalization_Error",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "General strategy of backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrickIntroduction",
      "to": "ThetaVectorRepresentation",
      "relationship": "related_to"
    },
    {
      "from": "inner_loop",
      "to": "k-means_algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "1D Example",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "Jacobian_Matrix_Implications",
      "to": "Chain_Rule_Application",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Independent components analysis",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised Learning",
      "to": "Foundation Models",
      "relationship": "subtopic"
    },
    {
      "from": "VC_Dimension_Theory",
      "to": "VC_Dimension_Definition",
      "relationship": "defines"
    },
    {
      "from": "Model_Selection",
      "to": "Generalization_Error",
      "relationship": "depends_on"
    },
    {
      "from": "ReinforcementLearning",
      "to": "DynamicProgramming",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Generalization Error",
      "relationship": "subtopic"
    },
    {
      "from": "Primal_Dual_Problems",
      "to": "d_star_p_star_Equality",
      "relationship": "subtopic"
    },
    {
      "from": "SoftmaxFunction",
      "to": "ProbabilityVector",
      "relationship": "produces"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "State_Transition_Probabilities",
      "relationship": "has_subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Clustering and the k-means algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Support_Vector_Machines",
      "relationship": "subtopic"
    },
    {
      "from": "Language_Model_Probability_Distribution",
      "to": "Pretrained_Language_Models",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Learning a model for an MDP",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalProbabilityModeling",
      "to": "EmbeddingsIntroduction",
      "relationship": "depends_on"
    },
    {
      "from": "Likelihood_Optimization",
      "to": "Single_Example_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "ForwardPass",
      "to": "IntermediateVariables",
      "relationship": "uses"
    },
    {
      "from": "Locally Weighted Linear Regression",
      "to": "Non-Parametric Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Beta_Update_Equation",
      "relationship": "defines"
    },
    {
      "from": "Update Rule for phi_j",
      "to": "M-step Maximization",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_W_Value",
      "to": "Intercept_Term_B",
      "relationship": "depends_on"
    },
    {
      "from": "Value Function",
      "to": "Optimal Value Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Predictive Posterior Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Polynomial Regression Models",
      "to": "Model Selection",
      "relationship": "subtopic"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Convergence_Criteria",
      "relationship": "depends_on"
    },
    {
      "from": "UpdateRule",
      "to": "LMSUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.4 Back-propagation for MLPs",
      "relationship": "contains"
    },
    {
      "from": "LinearRegression",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearning",
      "to": "ReinforcementLearning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Functions Backward Pass",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Formulation",
      "to": "Lagrangian",
      "relationship": "uses"
    },
    {
      "from": "Normalization Techniques",
      "to": "Scale-Invariant Property",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Transformer Model",
      "relationship": "contains"
    },
    {
      "from": "Predictive Posterior Distribution",
      "to": "Fully Bayesian Prediction",
      "relationship": "subtopic"
    },
    {
      "from": "EmbeddingsIntroduction",
      "to": "MachineLearningModels",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Single Neuron Model",
      "relationship": "subtopic"
    },
    {
      "from": "Union_Bound_Application",
      "to": "Uniform_Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "mixture_of_gaussians",
      "to": "unsupervised_learning",
      "relationship": "subtopic"
    },
    {
      "from": "SoftmaxFunction",
      "to": "Logits",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Gradient Calculation",
      "relationship": "contains"
    },
    {
      "from": "Gradient_Descent_Methods",
      "to": "Batch_Gradient_Descent",
      "relationship": "subtopic"
    },
    {
      "from": "ELBOOptimization",
      "to": "Q_iFormulation",
      "relationship": "depends_on"
    },
    {
      "from": "Contrastive_Learning",
      "to": "SIMCLR",
      "relationship": "example_of"
    },
    {
      "from": "Dual Constraints",
      "to": "Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Eigenfaces Method",
      "relationship": "subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.1 Feature maps",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Convolutional_Layers",
      "relationship": "contains"
    },
    {
      "from": "Discretization_in_MDPs",
      "to": "Continuous_State_Space",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO",
      "to": "Definition of ELBO",
      "relationship": "has_subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "Bellman_Equations",
      "relationship": "depends_on"
    },
    {
      "from": "BinaryFeatures",
      "to": "NaiveBayesAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Classical Settings vs. Deep Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bellman_Equation",
      "to": "Optimal_Policy",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "PrincipalComponentsAnalysis",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "contains"
    },
    {
      "from": "Overfitting",
      "to": "Variance",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression Derivation",
      "to": "Perceptron Learning Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "PolynomialModelFitting",
      "to": "GeneralizationFailure",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDistribution",
      "to": "MeanVector",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeopt",
      "to": "Test Error Decomposition",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Overview",
      "to": "Basic Modules Backward Function",
      "relationship": "subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Bayes Rule Application",
      "to": "Conditional Probability p(x|y)",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit Bias in Machine Learning",
      "to": "Machine Learning Papers",
      "relationship": "depends_on"
    },
    {
      "from": "Perceptron Learning Algorithm",
      "to": "Multi-class Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Latent_Variables",
      "relationship": "related_to"
    },
    {
      "from": "CrossEntropyLoss",
      "to": "GradientCalculation",
      "relationship": "depends_on"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "PrimalProblem",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelMethods",
      "relationship": "related_to"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "Policy_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "From non-linear dynamics to LQR",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LogisticRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "contains"
    },
    {
      "from": "Lagrangian Function",
      "to": "Primal Problem",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "Dual_Problem_Formulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "GradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "UnifiedTreatment",
      "to": "ClassificationProblem",
      "relationship": "related_to"
    },
    {
      "from": "LinearModelPrediction",
      "to": "LossFunctionOptimization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Symmetry of Kernel Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Deep Learning Regularization Techniques",
      "relationship": "related_to"
    },
    {
      "from": "ResNetArchitecture",
      "to": "SimplifiedResNet",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Differential_Dynamic_Programming",
      "relationship": "depends_on"
    },
    {
      "from": "latent_variables",
      "to": "mixture_of_gaussians",
      "relationship": "depends_on"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "VectorW",
      "to": "DistanceToBoundary",
      "relationship": "depends_on"
    },
    {
      "from": "Feature_Maps",
      "to": "Feature_Space",
      "relationship": "related_to"
    },
    {
      "from": "BernoulliRandomVariableZ",
      "to": "ReliableEstimateOfEpsilon",
      "relationship": "depends_on"
    },
    {
      "from": "Bellman_Equation",
      "to": "Value_Function",
      "relationship": "subtopic"
    },
    {
      "from": "REINFORCE Algorithm",
      "to": "Randomized Policy Learning",
      "relationship": "subtopic_of"
    },
    {
      "from": "Representation Function",
      "to": "Positive Pair",
      "relationship": "related_to"
    },
    {
      "from": "Finetuning",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Margin Classifier",
      "to": "Maximizing Geometric Margin",
      "relationship": "contains"
    },
    {
      "from": "Training Set",
      "to": "Posterior Distribution on Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "contains"
    },
    {
      "from": "GLMsDesignChoices",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "KernelMethods",
      "to": "SupportVectorMachines",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "3 Generalized linear models",
      "relationship": "contains"
    },
    {
      "from": "High_Dimensional_Statistics",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Partial_Derivatives",
      "to": "Scalar_Functions",
      "relationship": "related_to"
    },
    {
      "from": "MDP_Model_Learning",
      "to": "Expected_Immediate_Reward",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICA_On_Gaussian_Data",
      "to": "Non_Gaussian_Distributions",
      "relationship": "related_to"
    },
    {
      "from": "SimultaneousGuaranteesForAllHypotheses",
      "to": "UpperBoundOnGeneralizationError",
      "relationship": "depends_on"
    },
    {
      "from": "StateTransitionModeling",
      "to": "AlternativeLearningAlgorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Log_Probability_Computation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Descent Optimizer",
      "to": "Implicit Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Linear Regression",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Coordinate_Ascend_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "LogPartitionFunction",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "AsymptoticEfficiency",
      "relationship": "has_subtopic"
    },
    {
      "from": "Multinomial Random Variable",
      "to": "Maximum Likelihood Estimates",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "GenerativeAlgorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConditionalProbabilityDistribution",
      "to": "LikelihoodFunction",
      "relationship": "depends_on"
    },
    {
      "from": "convergence_of_k_means",
      "to": "k-means_algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian_Data_Issue",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "KernelMethods",
      "to": "KernelTrick",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Likelihood_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretrained large language models",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.4 Another algorithm for maximizing \\(\\ell(\\theta)\\)",
      "relationship": "subtopic"
    },
    {
      "from": "Representation Function",
      "to": "Negative Pair",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GeneralizedLinearModelsGLM",
      "relationship": "contains"
    },
    {
      "from": "Generalization Error Analysis",
      "to": "Machine Learning Papers",
      "relationship": "depends_on"
    },
    {
      "from": "TrainingExamples",
      "to": "RegressionProblems",
      "relationship": "related_to"
    },
    {
      "from": "Bias Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "CostFunction",
      "to": "GradientCalculation",
      "relationship": "subtopic"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "Optimal_Bellman_Equation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Fitted_Value_Iteration",
      "to": "Machine_Learning_Models",
      "relationship": "uses"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Kernels_in_SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "Dimensionality Reduction Techniques",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "contains"
    },
    {
      "from": "Algorithm 6",
      "to": "Value Iteration",
      "relationship": "special_case_of"
    },
    {
      "from": "2.3 Multi-class classification",
      "to": "Multinomial Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vectors",
      "to": "Decision_Boundary",
      "relationship": "defines"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "Actions",
      "relationship": "includes_component"
    },
    {
      "from": "1.3 Probabilistic interpretation",
      "to": "Least-squares cost function J",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Vanilla_REINFORCE_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Feature_Maps_and_Kernels",
      "relationship": "contains"
    },
    {
      "from": "Dual Problem",
      "to": "Objective Value Dual",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Linear_Quadratic_Gaussian_(LQG)",
      "relationship": "subtopic"
    },
    {
      "from": "TransformerModelOverview",
      "to": "MachineLearningModels",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModel(GLM)",
      "to": "AssumptionsForGLMs",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Linear_Transformations_And_Densities",
      "relationship": "has_subtopic"
    },
    {
      "from": "Functional_Margin",
      "to": "Confidence_Issue",
      "relationship": "depends_on"
    },
    {
      "from": "Hessian Matrix",
      "to": "Newton's Method",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Policy Gradient Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "ConvergenceBehavior",
      "relationship": "behavior_during"
    },
    {
      "from": "Evidence_Lower_Bound_(ELBO)",
      "to": "Tightening_the_Bound",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVMs)",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Basics",
      "to": "Hypothesis_Class",
      "relationship": "related_to"
    },
    {
      "from": "Neural_Network_Input",
      "to": "Hidden_Units",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "Lagrange_Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Preprocessing Dataset",
      "relationship": "subtopic"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "Quantities_of_Interest",
      "to": "Sample_Size_Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization and regularization",
      "to": "The double descent phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration_Approximations",
      "to": "Expectation_Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "LQR Extension",
      "relationship": "extension_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "CrossEntropyLoss",
      "relationship": "has_subtopic"
    },
    {
      "from": "Maximum Likelihood Estimates",
      "to": "Laplace Smoothing",
      "relationship": "related_to"
    },
    {
      "from": "BernoulliDistribution",
      "to": "LogPartitionFunctionForBernoulli",
      "relationship": "subtopic"
    },
    {
      "from": "GradientComputation",
      "to": "ReparameterizationTrick",
      "relationship": "solution_to"
    },
    {
      "from": "Backpropagation",
      "to": "Loss Function",
      "relationship": "related_to"
    },
    {
      "from": "I Supervised learning",
      "to": "6 Support vector machines",
      "relationship": "contains"
    },
    {
      "from": "Regularizer R(θ)",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Partial_Derivatives",
      "to": "Computational_Efficiency",
      "relationship": "subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "LearnableParameters",
      "relationship": "follows"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Total Neurons and Parameters",
      "relationship": "calculates"
    },
    {
      "from": "Reinforcement Learning and Control",
      "to": "Reinforcement learning",
      "relationship": "subtopic"
    },
    {
      "from": "ReinforcementLearning",
      "to": "MarkovDecisionProcesses",
      "relationship": "defines_formalism"
    },
    {
      "from": "Lagrangian_Formulation",
      "to": "Dual_Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Inner_Products",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Value function approximation",
      "to": "Continuous state MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Confidence_Issue",
      "to": "Normalization_Condition",
      "relationship": "related_to"
    },
    {
      "from": "Predict Step",
      "to": "Transition Model",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Primal_Dual_Formulation",
      "to": "KKT_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "Back-propagation for MLPs",
      "to": "Forward Pass Operations",
      "relationship": "depends_on"
    },
    {
      "from": "Update Rule for mu_j",
      "to": "M-step Maximization",
      "relationship": "subtopic"
    },
    {
      "from": "local_optima_issue",
      "to": "k-means_algorithm",
      "relationship": "related_to"
    },
    {
      "from": "FeatureSelection",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Finetuning_Pretrained_Models",
      "to": "Prediction_Model_Structure",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Multiple_Examples",
      "relationship": "subtopic"
    },
    {
      "from": "ParametersDefinition",
      "to": "ParameterEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "Data Augmentation",
      "to": "Positive Pair",
      "relationship": "subtopic"
    },
    {
      "from": "distortion_function_J",
      "to": "k-means_algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "GeneralizedLinearModel(GLM)",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis_Class_Expansion",
      "to": "Bias_Decrease",
      "relationship": "depends_on"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Covariance Matrix",
      "relationship": "has_part"
    },
    {
      "from": "Posterior Distribution on Parameters",
      "to": "Bayes' Theorem Application",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "LMSUpdateRule",
      "relationship": "related_to"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "JointDistributionModeling",
      "relationship": "subtopic"
    },
    {
      "from": "Data Visualization",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "DiscountFactor",
      "relationship": "includes_parameter"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Mean Vector",
      "relationship": "has_part"
    },
    {
      "from": "BernoulliDistribution",
      "to": "SufficientStatisticForBernoulli",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Differentiable Circuit",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Function",
      "to": "Negative Pair",
      "relationship": "related_to"
    },
    {
      "from": "Contrastive_Learning_Visual_Representations",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Bayes",
      "to": "Machine Learning Papers",
      "relationship": "subtopic"
    },
    {
      "from": "OptimizationInML",
      "to": "MaximizingQuadraticFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Beta_Update_Equation",
      "to": "Inner_Products",
      "relationship": "depends_on"
    },
    {
      "from": "Lagrangian Function",
      "to": "Dual Problem",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Policy_Gradient_Theorem",
      "relationship": "has_subtopic"
    },
    {
      "from": "GradientAscent",
      "to": "StochasticGradientAscent",
      "relationship": "subtopic"
    },
    {
      "from": "Design Matrix Definition",
      "to": "Least Squares Revisited",
      "relationship": "is_subtopic_of"
    },
    {
      "from": "1.3 Probabilistic interpretation",
      "to": "Regression problem",
      "relationship": "depends_on"
    },
    {
      "from": "Regression Algorithms",
      "to": "Supervised Learning",
      "relationship": "related_to"
    },
    {
      "from": "Language Models",
      "to": "Conditional Probability",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningIssues",
      "to": "LinearModelLimitations",
      "relationship": "depends_on"
    },
    {
      "from": "ELBOOptimization",
      "to": "GradientAscent",
      "relationship": "subtopic"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "MultinomialEventModel",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Pretraining_and_Adaptation",
      "to": "Pretraining_Phase",
      "relationship": "has_subtopic"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "RewardFunction",
      "relationship": "defines_function"
    },
    {
      "from": "Kernel Function Properties",
      "to": "Necessary Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Basics",
      "to": "Empirical_Risk_Minimization",
      "relationship": "depends_on"
    },
    {
      "from": "DesignMatrixX",
      "to": "ProbabilityOfData",
      "relationship": "subtopic"
    },
    {
      "from": "SpamDetection",
      "to": "NewWordChallenge",
      "relationship": "depends_on"
    },
    {
      "from": "Data Augmentation",
      "to": "Negative Pair",
      "relationship": "subtopic"
    },
    {
      "from": "HypothesisFunction",
      "to": "LogisticFunction",
      "relationship": "contains"
    },
    {
      "from": "Lagrange_Duality",
      "to": "Constrained_Optimization_Problems",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "EM for mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "VariationalInference",
      "to": "GradientComputation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Expectation_Maximization_Guarantees",
      "to": "Jensens_Inequality",
      "relationship": "related_to"
    },
    {
      "from": "Model Set M",
      "to": "Cross Validation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Discretization_in_MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "Backward Function Overview",
      "to": "W Variable Backward Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "House_Price_Prediction",
      "to": "Feature_Discovery",
      "relationship": "subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Data Comparison",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal Policy in LQR",
      "to": "Discrete Ricatti Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Neural Networks",
      "to": "Stacking Neurons",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Likelihood_Maximization",
      "relationship": "contains"
    },
    {
      "from": "MultiClassClassification",
      "to": "SoftmaxFunction",
      "relationship": "related_to"
    },
    {
      "from": "Normalization Techniques",
      "to": "Unit Variance Scaling",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "SMO_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "LaplaceSmoothing",
      "relationship": "related_to"
    },
    {
      "from": "KernelTrick",
      "to": "KernelsAsSimilarityMetrics",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "Hessian_Matrix",
      "relationship": "related_to"
    },
    {
      "from": "BackwardPass",
      "to": "DerivativesCalculation",
      "relationship": "depends_on"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.1 The multivariate normal distribution",
      "relationship": "contains"
    },
    {
      "from": "ResNetArchitecture",
      "to": "LayerNormalization",
      "relationship": "contains"
    },
    {
      "from": "Learning_Model_for_MDP",
      "to": "Inverted_Pendulum_Problem",
      "relationship": "example_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "IndependenceAssumption",
      "relationship": "related_to"
    },
    {
      "from": "FeatureMapping",
      "to": "GradientDescent",
      "relationship": "depends_on"
    },
    {
      "from": "KernelTrickIntroduction",
      "to": "PhiFunctionExplanation",
      "relationship": "depends_on"
    },
    {
      "from": "UnifiedTreatment",
      "to": "RegressionProblem",
      "relationship": "depends_on"
    },
    {
      "from": "Labeled Dataset",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "AffineTransformation",
      "relationship": "related_to"
    },
    {
      "from": "Optimal Regularization",
      "to": "Explanation and Mitigation Strategy",
      "relationship": "subtopic"
    },
    {
      "from": "Zero-shot Learning",
      "to": "In-context Learning",
      "relationship": "related_to"
    },
    {
      "from": "Non-separable Case",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Components",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Model Selection",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ProbabilityDistributions",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Definition",
      "to": "Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Multiple_Examples",
      "to": "ELBO_Multiple_Examples",
      "relationship": "depends_on"
    },
    {
      "from": "7 Deep learning",
      "to": "7.2 Neural networks",
      "relationship": "subtopic"
    },
    {
      "from": "Jensen's_Inequality",
      "to": "Concave_Functions",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "CanonicalResponseFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "contains"
    },
    {
      "from": "EM_Algorithm",
      "to": "ELBO",
      "relationship": "related_to"
    },
    {
      "from": "ConstructingGLMs",
      "to": "BernoulliDistribution",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis_Class_Parameterization",
      "to": "Linear_Classifiers",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Conv2D",
      "relationship": "contains"
    },
    {
      "from": "Linear Probe",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "Encoder_Decoder_Roles",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "FeatureRepresentation",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Hypothesis",
      "relationship": "depends_on"
    },
    {
      "from": "Zero-Shot_Adaptation",
      "to": "Machine_Learning_Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Machine_Learning_Papers",
      "relationship": "depends_on"
    },
    {
      "from": "ExpectationMaximization",
      "to": "GradientEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "Dual_Form_Optimization",
      "to": "Support_Vector_Machines",
      "relationship": "leads_to"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "Neural_Networks",
      "to": "Parameters_Beta",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModel(GLM)",
      "to": "ExponentialFamilyDistributions",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Decomposition",
      "to": "Average Model (h_avg)",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised learning",
      "to": "Principal components analysis",
      "relationship": "subtopic"
    },
    {
      "from": "In-Context_Learning",
      "to": "Machine_Learning_Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "BackPropagationAlgorithm",
      "to": "GradientComputation",
      "relationship": "depends_on"
    },
    {
      "from": "Cocktail_Party_Problem",
      "to": "Mixing_Matrix_A",
      "relationship": "depends_on"
    },
    {
      "from": "LinearHypothesis",
      "to": "ParametersWeights",
      "relationship": "subtopic"
    },
    {
      "from": "Implicit Regularization",
      "to": "Model-wise Double Descent",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Geometric_Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Bellman's Equation",
      "relationship": "has_subtopic"
    },
    {
      "from": "ExpectationRewriting",
      "to": "RewardFunction",
      "relationship": "depends_on"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "ProjectionOntoUnitVector",
      "relationship": "depends_on"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Notational Consistency",
      "relationship": "defines"
    },
    {
      "from": "Physics Simulation",
      "to": "Off-the-Shelf Software",
      "relationship": "includes"
    },
    {
      "from": "DensityFunctionFormulation",
      "to": "SigmoidFunctionChoice",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "LinearModelLimitations",
      "to": "BiasDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Momentum",
      "relationship": "related_to"
    },
    {
      "from": "OptimalActionComputation",
      "to": "NonStationaryPolicy",
      "relationship": "implies"
    },
    {
      "from": "MachineLearningArchitectures",
      "to": "ResNetArchitecture",
      "relationship": "contains"
    },
    {
      "from": "Log-Likelihood Function",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "depends_on"
    },
    {
      "from": "Loss_Functions",
      "to": "Mean_Squared_Error",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "GELUFunction",
      "relationship": "contains"
    },
    {
      "from": "Log_Probability_Bound",
      "to": "Probability_Distributions",
      "relationship": "depends_on"
    },
    {
      "from": "Probability Estimation",
      "to": "Event Models for Text Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Model_Parameterization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Foundation_Models",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.3 Discussion: GDA and logistic regression",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Policy_Gradient_Theorem",
      "relationship": "contains"
    },
    {
      "from": "Digit Recognition Example",
      "to": "Kernel Matrix Properties",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization_in_MDPs",
      "to": "Discrete_State_Space",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "MultinomialFeatures",
      "relationship": "subtopic"
    },
    {
      "from": "d_star_p_star_Equality",
      "to": "Feasibility_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.1 The exponential family",
      "relationship": "subtopic"
    },
    {
      "from": "Kernels_in_Machine_Learning",
      "to": "Kernel_Function_K",
      "relationship": "related_to"
    },
    {
      "from": "LogisticLossFunction",
      "to": "NegativeLogLikelihood",
      "relationship": "depends_on"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Ambiguity",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Partial Derivatives Basics",
      "relationship": "contains"
    },
    {
      "from": "Gaussian_Distribution_Qi",
      "to": "Mean_and_Variance_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.1 Matrix derivatives",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Maximum_Likelihood_Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Average Loss",
      "to": "Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalDistributionModeling",
      "to": "BernoulliDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Kernel Methods",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Adaptation_Methods",
      "to": "Finetuning_Pretrained_Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICA Algorithm",
      "to": "Density Transformation",
      "relationship": "depends_on"
    },
    {
      "from": "Baseline_Estimator",
      "to": "Value_Function",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "LN_S_Module",
      "relationship": "subtopic_of"
    },
    {
      "from": "2.3 Multi-class classification",
      "to": "Softmax Function",
      "relationship": "subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Physics Simulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "CostFunction",
      "to": "OrdinaryLeastSquares",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "SoftplusFunction",
      "relationship": "contains"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "ValidKernelConditions",
      "to": "KernelsAsSimilarityMetrics",
      "relationship": "related_to"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "LagrangeMultipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Convolutional_Neural_Networks",
      "relationship": "contains"
    },
    {
      "from": "ELBO Lower Bound",
      "to": "Variational Inference",
      "relationship": "depends_on"
    },
    {
      "from": "Implicit_Bias_Noise_Covariance",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "Loss Functions Backward Pass",
      "to": "Squared Loss (MSE)",
      "relationship": "subtopic"
    },
    {
      "from": "Probability_Distributions",
      "to": "Jensen's_Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Jensens_Inequality",
      "to": "Theorem_Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Overparameterized Models",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelMethods",
      "relationship": "contains"
    },
    {
      "from": "Zero-shot Learning",
      "to": "Finetuning",
      "relationship": "related_to"
    },
    {
      "from": "Conv1D_Simplified",
      "to": "Filter_Vector",
      "relationship": "has_subcomponent"
    },
    {
      "from": "Chain_Rule_Application",
      "to": "Jacobian_Matrix_Implications",
      "relationship": "depends_on"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO",
      "to": "Marginal Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "UpdateRule",
      "to": "SingleTrainingExample",
      "relationship": "subtopic"
    },
    {
      "from": "KernelFunctions",
      "to": "PolynomialKernels",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Gradient Descent (GD)",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ModelParameters",
      "relationship": "subtopic"
    },
    {
      "from": "FullyConnectedNN",
      "to": "IntermediateVariables",
      "relationship": "depends_on"
    },
    {
      "from": "Weight Decay",
      "to": "L2 Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.2 The Gaussian discriminant analysis model",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningOverview",
      "to": "VariationalInference",
      "relationship": "contains"
    },
    {
      "from": "Inner_Products",
      "to": "Efficient_Computation",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningIssues",
      "to": "PolynomialModelFitting",
      "relationship": "related_to"
    },
    {
      "from": "ReLUFunction",
      "to": "LeakyReLU",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Policy_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Scaling Invariance",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Partial_Derivatives",
      "relationship": "depends_on"
    },
    {
      "from": "1 Linear regression",
      "to": "1.2 The normal equations",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backpropagation Overview",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Challenges",
      "to": "K_Fold_Cross_Validation",
      "relationship": "depends_on"
    },
    {
      "from": "Neural_Network_Input",
      "to": "Output_Function",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "ConditionalDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO",
      "to": "Rewritten Formulations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Empirical Risk Minimization",
      "relationship": "depends_on"
    },
    {
      "from": "multinomial_distribution",
      "to": "joint_distribution_modeling",
      "relationship": "related_to"
    },
    {
      "from": "Pretraining_Phase",
      "to": "Optimizers",
      "relationship": "uses"
    },
    {
      "from": "Training Data Classification",
      "to": "Decision Boundary",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Gradient_Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.2 LMS (least mean squares) with features",
      "relationship": "subtopic"
    },
    {
      "from": "Clustering",
      "to": "K-Means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Optimization_Problems",
      "to": "Dual_Form",
      "relationship": "related_to"
    },
    {
      "from": "Chapter_16_LQR_DDP_LQG",
      "to": "Finite_Horizon_MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Covariance Matrix Sigma",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "L2 Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Efficient_Evaluation_ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Generalization_Error_Bound",
      "relationship": "subtopic"
    },
    {
      "from": "Variational_Inference",
      "to": "Variational_Autoencoder",
      "relationship": "extends_to"
    },
    {
      "from": "NeuralNetworks",
      "to": "Vectorization",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Back-propagation for MLPs",
      "relationship": "subtopic"
    },
    {
      "from": "Sign_Changes_Ignored",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Geometric Margin",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "PoissonDistribution",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "ConditionalProbabilityDistribution",
      "to": "DesignMatrixX",
      "relationship": "related_to"
    },
    {
      "from": "Language-Specific Methods",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "related_to"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "BandwidthParameter",
      "relationship": "contains"
    },
    {
      "from": "GeneralizationErrorGuarantees",
      "to": "EmpiricalRiskMinimization",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization_in_MDPs",
      "to": "Value_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesFilter",
      "to": "SpamDetection",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "BatchGradientDescentExample",
      "relationship": "subtopic"
    },
    {
      "from": "Cross-Entropy Loss",
      "to": "Negative Log-Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "ELBO",
      "to": "Conditional Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal Policy",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Optimization_Problems",
      "to": "Quadratic_Programming_QP",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Belief States",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Lagrange_Duality",
      "to": "Dual_Form",
      "relationship": "subtopic"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "StateTransitionProbabilities",
      "relationship": "includes_component"
    },
    {
      "from": "Likelihood_Function",
      "to": "Log_Likelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization_Problems",
      "to": "KKT_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "ResNetArchitecture",
      "relationship": "subtopic"
    },
    {
      "from": "HypothesesSpaceK",
      "to": "TrainingErrorGeneralizationGap",
      "relationship": "related_to"
    },
    {
      "from": "JointLikelihood",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Parametric Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Classification Problem",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ProbabilisticModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "K_Fold_Cross_Validation",
      "to": "Leave_One_Out_Cross_Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "The case of infinite H",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis_Class_Size",
      "to": "Machine_Learning_Theory",
      "relationship": "related_to"
    },
    {
      "from": "Union Bound Lemma",
      "to": "Learning Theory Proofs",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "GradientDescentConvergence",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Primal_Dual_Relationship",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generalization and regularization",
      "to": "Sample complexity bounds (optional readings)",
      "relationship": "subtopic"
    },
    {
      "from": "OptimizationInML",
      "to": "Equation622",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Cross-Entropy Loss Function",
      "relationship": "contains"
    },
    {
      "from": "Convolutional Layers",
      "to": "2-D Convolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICAIndependenceAssumption",
      "to": "DensityFunctionFormulation",
      "relationship": "depends_on"
    },
    {
      "from": "Training_Error",
      "to": "Empirical_Risk_Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Widrow-Hoff_Learning_Rule",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Classification Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Probability_Error_Bounds",
      "to": "Training_Error_Generalization_Error",
      "relationship": "related_to"
    },
    {
      "from": "Decision Boundary",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "RegressionProblems",
      "to": "TrainingDataset",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Gradient_Ascend_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient_Computation_Simplified",
      "to": "Machine_Learning_Backward_Propagation",
      "relationship": "related_to"
    },
    {
      "from": "Test Error vs Training Error",
      "to": "Underfitting",
      "relationship": "subtopic"
    },
    {
      "from": "BiologicalInspiration",
      "to": "TwoLayerNetworks",
      "relationship": "depends_on"
    },
    {
      "from": "KernelFunctions",
      "to": "FeatureMapping",
      "relationship": "has_subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "GaussianDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "MarkovDecisionProcesses",
      "to": "States",
      "relationship": "includes_component"
    },
    {
      "from": "Bias-variance tradeoff",
      "to": "A mathematical decomposition (for regression)",
      "relationship": "depends_on"
    },
    {
      "from": "Matrix Derivatives",
      "to": "Least Squares Revisited",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModelsGLM",
      "to": "BernoulliDistribution",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Policy_Iteration",
      "relationship": "contains"
    },
    {
      "from": "Value Function",
      "to": "Time Dependent Dynamics",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Nonlinear_Feature_Mappings",
      "relationship": "depends_on"
    },
    {
      "from": "Text_Classification",
      "to": "Generative_Modeling",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Lagrangian_Formulation",
      "relationship": "has_subtopic"
    },
    {
      "from": "GLMsDesignChoices",
      "to": "OrdinaryLeastSquares",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Functional Margin",
      "relationship": "related_to"
    },
    {
      "from": "KKT_Conditions",
      "to": "Primal_Dual_Relationship",
      "relationship": "related_to"
    },
    {
      "from": "OptimizationInML",
      "to": "ObjectiveFunctionW",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Sparsity Inducing Regularization",
      "to": "Regularizer R(θ)",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Newton's Method",
      "relationship": "related_to"
    },
    {
      "from": "Primal Problem",
      "to": "Objective Value Primal",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureRepresentation",
      "to": "StringFeatureVectors",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "REINFORCE Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Policy_Iteration_Connections",
      "relationship": "related_to"
    },
    {
      "from": "Vanilla_REINFORCE_Algorithm",
      "to": "Empirical_Estimation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Mixture of Gaussians",
      "to": "EM Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries on partial derivatives",
      "relationship": "depends_on"
    },
    {
      "from": "OptimalValueFunction",
      "to": "BellmanEquation",
      "relationship": "related_to"
    },
    {
      "from": "Empirical Risk Minimization",
      "to": "Hypothesis Selection",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Value iteration and policy iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Normalization Techniques",
      "to": "Group Normalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Pretraining_Methods_Computer_Vision",
      "to": "Contrastive_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep Learning Implementation",
      "to": "Matricization Approach",
      "relationship": "depends_on"
    },
    {
      "from": "ELBOOptimization",
      "to": "EfficientEvaluation",
      "relationship": "subtopic"
    },
    {
      "from": "1.3 Probabilistic interpretation",
      "to": "Error term ε^(i)",
      "relationship": "defines"
    },
    {
      "from": "Convolutional_Layers",
      "to": "Channel_Support_in_Conv1D",
      "relationship": "subtopic"
    },
    {
      "from": "ActivationFunctions",
      "to": "SigmoidFunction",
      "relationship": "contains"
    },
    {
      "from": "Continuous_State_MDPs",
      "to": "Machine_Learning_Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Kernel_Trick",
      "relationship": "related_to"
    },
    {
      "from": "Variational_Autoencoder",
      "to": "Reparametrization_Technique",
      "relationship": "contains"
    },
    {
      "from": "Curse of Dimensionality",
      "to": "Discretization Limitations",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "DerivativeSigmoid",
      "relationship": "related_to"
    },
    {
      "from": "Log_Probability_Gradients",
      "to": "Trajectory_Probability_Change",
      "relationship": "has_subtopic"
    },
    {
      "from": "Frequentist Approach",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "uses_method"
    },
    {
      "from": "General EM algorithms",
      "to": "Other interpretation of ELBO",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Policy in LQR",
      "to": "LQR Algorithm Steps",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Explicit Regularization Techniques",
      "relationship": "has_subtopic"
    },
    {
      "from": "ParameterEstimation",
      "to": "LaplaceSmoothing",
      "relationship": "related_to"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Supervised Learning Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Preprocessing with PCA",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Linearization Around Trajectory Points",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "NewtonMethod",
      "relationship": "related_to"
    },
    {
      "from": "ELBO",
      "to": "KL Divergence",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConditionalProbabilityModeling",
      "to": "MachineLearningModels",
      "relationship": "subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Alpha_I_Update",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Initialization",
      "relationship": "contains"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "2D_Convolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Optimal_Margin_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "1.3 Probabilistic interpretation",
      "to": "θ (parameters)",
      "relationship": "depends_on"
    },
    {
      "from": "ConstructingGLMs",
      "to": "GaussianDistribution",
      "relationship": "related_to"
    },
    {
      "from": "BernoulliEventModel",
      "to": "NaiveBayes",
      "relationship": "related_to"
    },
    {
      "from": "VarianceMaximization",
      "to": "PrincipalEigenvector",
      "relationship": "subtopic"
    },
    {
      "from": "Text Generation",
      "to": "Adaptive Sampling",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Models",
      "to": "Classification_Models",
      "relationship": "subtopic"
    },
    {
      "from": "ModelComplexityMeasures",
      "to": "NormOfModels",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworkBasics",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningTheoreticalBounds",
      "to": "TrainingErrorGeneralizationGap",
      "relationship": "subtopic"
    },
    {
      "from": "HoeffdingInequalityApplication",
      "to": "TrainingErrorMean",
      "relationship": "related_to"
    },
    {
      "from": "FeatureSelection",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "M-step Maximization",
      "to": "Expectation-Maximization Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "General Case",
      "to": "Density Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.3 Multi-class classification",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLagrangian",
      "to": "ThetaP",
      "relationship": "leads_to"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.4 Properties of kernels",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "NormalEquations",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Loss Functions Backward Pass",
      "relationship": "subtopic"
    },
    {
      "from": "Feature_Vector",
      "to": "Vocabulary",
      "relationship": "based_on"
    },
    {
      "from": "Sparsity of Model Parameters",
      "to": "L1 Regularization (LASSO)",
      "relationship": "subtopic"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "Scale-Invariant Property",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "KernelFunctions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Training_Set",
      "to": "Hypothesis_Function",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "EMAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Value Function",
      "to": "Bellman Equations",
      "relationship": "subtopic"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "WeightedFitting",
      "relationship": "subtopic_of"
    },
    {
      "from": "Hoeffding Inequality",
      "to": "Learning Theory Proofs",
      "relationship": "subtopic"
    },
    {
      "from": "BinaryClassification",
      "to": "LogisticFunction",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Extension",
      "to": "Kalman Filter",
      "relationship": "uses"
    },
    {
      "from": "Finite-horizon MDPs",
      "to": "LQR, DDP and LQG",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Implicit Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Parametrization",
      "relationship": "defines"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Finite_Hypothesis_Class",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GaussianDistributions",
      "relationship": "contains"
    },
    {
      "from": "Gradient_Ascend_Method",
      "to": "Stochastic_Gradient_Ascend_Rule",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "LQR, DDP and LQG",
      "relationship": "subtopic"
    },
    {
      "from": "VariationalInference",
      "to": "ELBO",
      "relationship": "subtopic_of"
    },
    {
      "from": "Log_Likelihood_Optimization",
      "to": "Evidence_Lower_Bound",
      "relationship": "depends_on"
    },
    {
      "from": "Geometric Margins",
      "to": "Decision Boundary",
      "relationship": "related_to"
    },
    {
      "from": "Temperature Parameter",
      "to": "Adaptive Sampling",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Support_Vector_Machines_SVM",
      "relationship": "has_subtopic"
    },
    {
      "from": "BinaryClassificationProblem",
      "to": "MLPModel",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "ModelAssumptions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Activation Function",
      "to": "ReLU",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Backpropagation",
      "to": "Backward_Functions_Basic_Modules",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backward_Functions_Basic_Modules",
      "to": "Loss_Function_Backward",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "LogisticLossFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Efficiency Considerations",
      "relationship": "related_to"
    },
    {
      "from": "Conditional Probabilistic Models",
      "to": "Exponential Family Distributions",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Constrained_Optimization_Problems",
      "to": "Lagrange_Multipliers",
      "relationship": "subtopic"
    },
    {
      "from": "Decision Boundary",
      "to": "Functional Margins",
      "relationship": "related_to"
    },
    {
      "from": "FunctionalMargin",
      "to": "GeometricMargin",
      "relationship": "subtopic"
    },
    {
      "from": "Jensens_Inequality",
      "to": "Convex_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "9.1Regularization",
      "to": "OverfittingComplexity",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "CovarianceDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "Variance Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "VariationalAutoEncoder",
      "to": "EMAlgorithm",
      "relationship": "extends"
    },
    {
      "from": "Gradient_Descent_Methods",
      "to": "LMS_Update_Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Backward_Functions_Basic_Modules",
      "to": "Matrix_Multiplication_Module_MM",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model Error Analysis",
      "to": "Average Model (h_avg)",
      "relationship": "depends_on"
    },
    {
      "from": "ConditionalDistributionModeling",
      "to": "HypothesisFunction",
      "relationship": "subtopic"
    },
    {
      "from": "ModelComplexityMeasures",
      "to": "NumberofParameters",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Q",
      "relationship": "related_to"
    },
    {
      "from": "Value_Iteration",
      "to": "Optimization_Technique",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Margins_Intuition",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "SVM_Optimization_Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning Algorithm",
      "to": "Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization",
      "to": "Training Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "LQR, DDP and LQG",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworkBasics",
      "to": "MultiLayerNN",
      "relationship": "subtopic"
    },
    {
      "from": "DiscriminativeAlgorithms",
      "to": "LogisticRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Dimensionality Reduction Techniques",
      "to": "Independent Components Analysis (ICA)",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Law_of_Total_Expectation",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "REINFORCE_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "depends_on"
    },
    {
      "from": "Text_Classification",
      "to": "Spam_Filtering",
      "relationship": "subtopic_of"
    },
    {
      "from": "LayerNormalization",
      "to": "LN-S",
      "relationship": "depends_on"
    },
    {
      "from": "Linearization Around Trajectory Points",
      "to": "Non-Stationary Dynamics",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "EM_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "General EM algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "LikelihoodExpression",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "Training_Loss",
      "to": "Empirical_Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Plotting Data Points",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "subtopic"
    },
    {
      "from": "Classification_Models",
      "to": "Probabilistic_Assumptions_Classification",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.1 Ordinary least squares",
      "relationship": "contains"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Clustering",
      "relationship": "depends_on"
    },
    {
      "from": "Deterministic Simulator",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Bayes Rule Application",
      "to": "Class Priors",
      "relationship": "depends_on"
    },
    {
      "from": "Linear_Functions",
      "to": "Feature_Map",
      "relationship": "related_to"
    },
    {
      "from": "Adaptation Algorithm",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureMapping",
      "to": "KernelsAsSimilarityMetrics",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ICAIndependenceAssumption",
      "relationship": "related_to"
    },
    {
      "from": "Validation_Set",
      "to": "k_Fold_Cross_Validation",
      "relationship": "subtopic_of"
    },
    {
      "from": "LogisticRegression",
      "to": "NegativeLikelihoodLoss",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "RegressionProblems",
      "to": "TestExample",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Feature_Engineering",
      "relationship": "depends_on"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Supervised Learning",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Algorithm_7_Vanilla_Policy_Gradient",
      "relationship": "related_to"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Polynomial Models",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Underfitting",
      "relationship": "depends_on"
    },
    {
      "from": "WeightedFitting",
      "to": "WeightsCalculation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Convolutional_Layers",
      "to": "Parameter_Sharing",
      "relationship": "subtopic"
    },
    {
      "from": "E_Step",
      "to": "Soft_Guesses",
      "relationship": "depends_on"
    }
  ]
}