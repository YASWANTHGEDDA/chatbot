{
  "nodes": [
    {
      "id": "I Supervised learning",
      "type": "major",
      "parent": null,
      "description": "Overview of supervised machine learning techniques and algorithms."
    },
    {
      "id": "1 Linear regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to linear regression models and their applications."
    },
    {
      "id": "1.1 LMS algorithm",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Least Mean Squares (LMS) algorithm for updating weights in linear regression."
    },
    {
      "id": "1.2 The normal equations",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Derivation and use of the normal equation method to solve linear regression problems."
    },
    {
      "id": "1.2.1 Matrix derivatives",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Calculation of matrix derivatives used in deriving the normal equations."
    },
    {
      "id": "1.2.2 Least squares revisited",
      "type": "subnode",
      "parent": "1.2 The normal equations",
      "description": "Revisiting least squares method from a different perspective."
    },
    {
      "id": "1.3 Probabilistic interpretation",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Probabilistic view of linear regression and its assumptions."
    },
    {
      "id": "1.4 Locally weighted linear regression (optional reading)",
      "type": "subnode",
      "parent": "1 Linear regression",
      "description": "Technique for non-parametric, locally adaptive linear regression."
    },
    {
      "id": "2 Classification and logistic regression",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to classification problems and the use of logistic regression."
    },
    {
      "id": "2.1 Logistic regression",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Binary and multi-class logistic regression models."
    },
    {
      "id": "2.2 Digression: the perceptron learning algorithm",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Overview of the Perceptron Learning Algorithm for binary classification."
    },
    {
      "id": "2.3 Multi-class classification",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Methods for handling multi-class classification problems using logistic regression."
    },
    {
      "id": "2.4 Another algorithm for maximizing λ(θ)",
      "type": "subnode",
      "parent": "2 Classification and logistic regression",
      "description": "Alternative methods to maximize the likelihood function in logistic regression."
    },
    {
      "id": "3 Generalized linear models",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generalized linear models (GLMs) and their applications."
    },
    {
      "id": "3.1 The exponential family",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Definition and properties of the exponential family in GLMs."
    },
    {
      "id": "3.2 Constructing GLMs",
      "type": "subnode",
      "parent": "3 Generalized linear models",
      "description": "Steps to construct generalized linear models from data."
    },
    {
      "id": "3.2.1 Ordinary least squares",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Ordinary Least Squares (OLS) as a special case of GLM."
    },
    {
      "id": "3.2.2 Logistic regression",
      "type": "subnode",
      "parent": "3.2 Constructing GLMs",
      "description": "Logistic Regression as another example of GLM construction."
    },
    {
      "id": "4 Generative learning algorithms",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to generative models for classification tasks."
    },
    {
      "id": "4.1 Gaussian discriminant analysis",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Gaussian Discriminant Analysis (GDA) model and its applications."
    },
    {
      "id": "4.1.1 The multivariate normal distribution",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Properties of the multivariate normal distribution used in GDA."
    },
    {
      "id": "4.1.2 The Gaussian discriminant analysis model",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Formulation and use of the GDA model for classification."
    },
    {
      "id": "4.1.3 Discussion: GDA and logistic regression",
      "type": "subnode",
      "parent": "4.1 Gaussian discriminant analysis",
      "description": "Comparison between GDA and logistic regression models."
    },
    {
      "id": "4.2 Naive bayes (Option Reading)",
      "type": "subnode",
      "parent": "4 Generative learning algorithms",
      "description": "Introduction to the naive Bayes classifier for text classification tasks."
    },
    {
      "id": "4.2.1 Laplace smoothing",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Technique to handle zero probability events in naive Bayes classifiers."
    },
    {
      "id": "4.2.2 Event models for text classification",
      "type": "subnode",
      "parent": "4.2 Naive bayes (Option Reading)",
      "description": "Application of event models in the context of text classification using naive Bayes."
    },
    {
      "id": "5 Kernel methods",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to kernel methods and their use in machine learning algorithms."
    },
    {
      "id": "5.1 Feature maps",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Mapping of input data into higher-dimensional feature spaces using kernels."
    },
    {
      "id": "5.2 LMS (least mean squares) with features",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Application of the least mean squares algorithm in kernel-based learning."
    },
    {
      "id": "5.3 LMS with the kernel trick",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Use of the kernel trick to extend LMS for non-linear problems."
    },
    {
      "id": "5.4 Properties of kernels",
      "type": "subnode",
      "parent": "5 Kernel methods",
      "description": "Properties and characteristics of different types of kernels used in machine learning."
    },
    {
      "id": "6 Support vector machines",
      "type": "subnode",
      "parent": "I Supervised learning",
      "description": "Introduction to support vector machines (SVMs) for classification tasks."
    },
    {
      "id": "6.1 Margins: intuition",
      "type": "subnode",
      "parent": "6 Support vector machines",
      "description": "Conceptual understanding of margins in SVMs."
    },
    {
      "id": "II Deep learning",
      "type": "major",
      "parent": null,
      "description": "Overview of deep learning techniques and neural networks."
    },
    {
      "id": "7 Deep learning",
      "type": "subnode",
      "parent": "II Deep learning",
      "description": "Introduction to the field of deep learning with non-linear models."
    },
    {
      "id": "7.1 Supervised learning with non-linear models",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Use of non-linear models in supervised learning tasks."
    },
    {
      "id": "7.2 Neural networks",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Introduction to neural network architectures and their applications."
    },
    {
      "id": "7.3 Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Discussion of various modules used in modern neural networks."
    },
    {
      "id": "7.4 Backpropagation",
      "type": "subnode",
      "parent": "7 Deep learning",
      "description": "Backpropagation algorithm for training deep neural networks."
    },
    {
      "id": "7.4.1 Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Basic concepts of partial derivatives necessary for backpropagation."
    },
    {
      "id": "7.4.2 General strategy of backpropagation",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Overview of the general strategy used in the backpropagation algorithm."
    },
    {
      "id": "7.4.3 Backward functions for basic modules",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Derivation and use of backward functions for simple neural network components."
    },
    {
      "id": "7.4.4 Back-propagation for MLPs",
      "type": "subnode",
      "parent": "7.4 Backpropagation",
      "description": "Application of backpropagation to multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Modern_Neural_Networks",
      "type": "major",
      "parent": null,
      "description": "Overview of modules and techniques in modern neural networks."
    },
    {
      "id": "Modules_in_MNN",
      "type": "subnode",
      "parent": "Modern_Neural_Networks",
      "description": "Discussion on various components used in contemporary neural network architectures."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Modern_Neural_Networks",
      "description": "Explains the backpropagation algorithm for training neural networks."
    },
    {
      "id": "Preliminaries_on_PD",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to partial derivatives necessary for understanding backpropagation."
    },
    {
      "id": "General_Strategy_BP",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the general strategy and steps involved in backpropagation."
    },
    {
      "id": "Backward_Functions_BM",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Details on backward functions for basic modules used in neural networks."
    },
    {
      "id": "BP_for_MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Specific application of backpropagation to multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Vectorization_Training_Examples",
      "type": "subnode",
      "parent": "Modern_Neural_Networks",
      "description": "Techniques for vectorizing training examples in neural network training."
    },
    {
      "id": "Generalization",
      "type": "major",
      "parent": "Statistical_Mechanics_of_Learning",
      "description": "Process by which a model learns from training data and applies knowledge to unseen data."
    },
    {
      "id": "Bias_Variance_Tradeoff",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Balancing model complexity to minimize total prediction error by managing bias and variance."
    },
    {
      "id": "Mathematical_Decomposition_BV",
      "type": "subnode",
      "parent": "Bias_Variance_Tradeoff",
      "description": "A mathematical breakdown for regression tasks related to bias and variance."
    },
    {
      "id": "Double_Descent_Phenomenon",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Phenomenon discussed in Section 8.2 regarding the relationship between model complexity and generalization performance."
    },
    {
      "id": "Sample_Complexity_Bounds",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Bounds on sample complexity for learning algorithms, with a focus on finite and infinite hypothesis sets."
    },
    {
      "id": "Regularization_Model_Selection",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting through regularization and model selection methods."
    },
    {
      "id": "Regularization_Techniques",
      "type": "subnode",
      "parent": "Regularization_Model_Selection",
      "description": "Introduction to various regularization techniques used in machine learning."
    },
    {
      "id": "Implicit_Regularization_Effect",
      "type": "subnode",
      "parent": "Regularization_Model_Selection",
      "description": "Discussion on implicit regularization effects, which can occur without explicit constraints."
    },
    {
      "id": "Model_Selection_Cross_Validation",
      "type": "subnode",
      "parent": "Regularization_Model_Selection",
      "description": "Use of cross-validation for selecting the best model among candidates."
    },
    {
      "id": "Bayesian_Statistics_Reg",
      "type": "subnode",
      "parent": "Regularization_Model_Selection",
      "description": "Application of Bayesian statistics in regularization and model selection."
    },
    {
      "id": "Unsupervised_Learning",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning from data without labeled responses."
    },
    {
      "id": "Clustering_KMeans",
      "type": "subnode",
      "parent": "Unsupervised_Learning",
      "description": "Introduction to clustering algorithms and the k-means algorithm."
    },
    {
      "id": "EM_Algorithms",
      "type": "subnode",
      "parent": "Unsupervised_Learning",
      "description": "Explanation of Expectation-Maximization (EM) algorithms for unsupervised learning tasks."
    },
    {
      "id": "EM_for_Mixture_Gaussians",
      "type": "subnode",
      "parent": "EM_Algorithms",
      "description": "Application of EM to mixture models with Gaussian distributions."
    },
    {
      "id": "Jensens_Inequality",
      "type": "subnode",
      "parent": "EM_Algorithms",
      "description": "Discussion on Jensen's inequality and its relevance in the context of EM algorithms."
    },
    {
      "id": "General_EM_Algorithms",
      "type": "subnode",
      "parent": "EM_Algorithms",
      "description": "Overview of general principles and applications of EM algorithms."
    },
    {
      "id": "ELBO_Interpretation",
      "type": "subnode",
      "parent": "General_EM_Algorithms",
      "description": "Alternative interpretations of the Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "Mixture_Gaussians_Revisited",
      "type": "subnode",
      "parent": "EM_Algorithms",
      "description": "Revisit and deeper understanding of mixture models with Gaussian components."
    },
    {
      "id": "Variational_Inference_VAE",
      "type": "subnode",
      "parent": "EM_Algorithms",
      "description": "Introduction to variational inference and the concept of Variational Auto-Encoders (VAEs)."
    },
    {
      "id": "PCA",
      "type": "subnode",
      "parent": "Unsupervised_Learning",
      "description": "Principal Component Analysis for data simplification and noise reduction."
    },
    {
      "id": "ICA",
      "type": "subnode",
      "parent": "Unsupervised_Learning",
      "description": "Independent Component Analysis for finding a new basis in data representation."
    },
    {
      "id": "ICA_Ambiguities",
      "type": "subnode",
      "parent": "ICA",
      "description": "Discussion on the limitations and ambiguities in recovering the unmixing matrix without prior knowledge."
    },
    {
      "id": "Densities_and_Transformations",
      "type": "subnode",
      "parent": "ICA",
      "description": "Exploration of densities and linear transformations within ICA."
    },
    {
      "id": "ICA_Algorithm",
      "type": "subnode",
      "parent": "ICA",
      "description": "Detailed explanation of the Independent Components Analysis algorithm."
    },
    {
      "id": "Self_Supervised_Learning_Foundation_Models",
      "type": "subnode",
      "parent": "Unsupervised_Learning",
      "description": "Introduction to self-supervised learning and foundation models in machine learning."
    },
    {
      "id": "Pretraining_Adaptation",
      "type": "subnode",
      "parent": "Self_Supervised_Learning_Foundation_Models",
      "description": "Overview of pre-training methods and their role in model adaptation."
    },
    {
      "id": "Computer_Vision_Pretraining",
      "type": "subnode",
      "parent": "Self_Supervised_Learning_Foundation_Models",
      "description": "Specific techniques for pre-training models in the context of computer vision."
    },
    {
      "id": "Large_Language_Models",
      "type": "subnode",
      "parent": "Self_Supervised_Learning_Foundation_Models",
      "description": "Discussion on large language model training and their capabilities."
    },
    {
      "id": "Transformers_Uncovered",
      "type": "subnode",
      "parent": "Large_Language_Models",
      "description": "Detailed look into the architecture of Transformer models."
    },
    {
      "id": "Zero_Shot_InContext_Learning",
      "type": "subnode",
      "parent": "Large_Language_Models",
      "description": "Exploration of zero-shot learning and in-context learning with large language models."
    },
    {
      "id": "Reinforcement_Learning_Control",
      "type": "major",
      "parent": null,
      "description": "Techniques for training agents to make decisions based on rewards in an environment."
    },
    {
      "id": "Reinforcement_Learning",
      "type": "subnode",
      "parent": "Reinforcement_Learning_Control",
      "description": "Type of machine learning where an agent learns to make decisions based on rewards and punishments."
    },
    {
      "id": "Markov_Decision_Processes",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Formal framework for modeling decision-making situations in reinforcement learning."
    },
    {
      "id": "Value_Iteration_Policy_Iteration",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Discussion on value iteration and policy iteration algorithms for solving MDPs."
    },
    {
      "id": "Learning_Model_MDP",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Techniques for learning models of the environment in reinforcement learning tasks."
    },
    {
      "id": "Continuous_State_MDPs",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Markov Decision Processes with infinite state spaces due to continuous variables."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous_State_MDPs",
      "description": "Process of converting a continuous state space into a discrete one for algorithmic computation."
    },
    {
      "id": "Value_Function_Approximation",
      "type": "subnode",
      "parent": "Continuous_State_MDPs",
      "description": "Techniques for approximating value functions in continuous state spaces."
    },
    {
      "id": "Policy_Value_Iteration_Connections",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Relationship between policy and value iteration approaches to solving reinforcement learning problems."
    },
    {
      "id": "LQR_DDP_LQG",
      "type": "major",
      "parent": null,
      "description": "Techniques for optimal control in linear systems with quadratic costs."
    },
    {
      "id": "Finite_Horizon_MDPs",
      "type": "subnode",
      "parent": "LQR_DDP_LQG",
      "description": "Overview of finite-horizon MDPs extending optimal Bellman equation to general settings."
    },
    {
      "id": "Linear_Q",
      "type": "subnode",
      "parent": "LQR_DDP_LQG",
      "description": "Discussion on linear quadratic control problems in the context of reinforcement learning."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "General introduction to machine learning concepts and problems."
    },
    {
      "id": "Learning a Model for an MDP",
      "type": "major",
      "parent": "Machine Learning Overview",
      "description": "Overview of methods used in Markov Decision Processes (MDP) for model learning."
    },
    {
      "id": "Continuous State MDPs",
      "type": "subnode",
      "parent": "Learning a Model for an MDP",
      "description": "Discussion on handling continuous state spaces in MDPs."
    },
    {
      "id": "Value Function Approximation",
      "type": "subnode",
      "parent": "Continuous State MDPs",
      "description": "Methods for approximating value functions in large or continuous state spaces."
    },
    {
      "id": "Connections between Policy and Value Iteration",
      "type": "subnode",
      "parent": "Learning a Model for an MDP",
      "description": "Exploration of connections and differences between policy and value iteration methods (optional)."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": "Machine Learning Overview",
      "description": "Introduction to Linear Quadratic Regulation (LQR), Differential Dynamic Programming (DDP) and Linear Quadratic Gaussian (LQG) in control theory."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Discussion on finite horizon problems within the context of Markov Decision Processes."
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Special case of finite-horizon setting where exact solutions are tractable and widely used in robotics."
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Conversion of nonlinear systems into linear form suitable for LQR application."
    },
    {
      "id": "Linearization of Dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Techniques for approximating nonlinear dynamics with linear models."
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Optimization technique used in trajectory optimization and control problems."
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Combination of LQR with stochastic dynamics for optimal control under uncertainty."
    },
    {
      "id": "Policy Gradient Methods",
      "type": "major",
      "parent": "Machine Learning Overview",
      "description": "Techniques for learning policies directly through gradient ascent methods in reinforcement learning."
    },
    {
      "id": "REINFORCE Algorithm",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "A policy gradient method that uses likelihood ratio to update the parameters of a stochastic policy."
    },
    {
      "id": "Supervised Learning Problem",
      "type": "major",
      "parent": null,
      "description": "Formal description of supervised learning with hypothesis function h(x)"
    },
    {
      "id": "Regression",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Learning problem where target variable is continuous"
    },
    {
      "id": "Classification",
      "type": "subnode",
      "parent": "Supervised Learning Problem",
      "description": "Learning problem with discrete target variables"
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": "Parametric Algorithms",
      "description": "Statistical method for modeling the relationship between variables by fitting a linear equation to observed data."
    },
    {
      "id": "Housing Example Dataset",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Dataset including living area and number of bedrooms with corresponding house prices"
    },
    {
      "id": "Feature Selection",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Process of choosing relevant features for the model"
    },
    {
      "id": "MachineLearningBasics",
      "type": "major",
      "parent": null,
      "description": "Introduction to fundamental concepts in machine learning."
    },
    {
      "id": "FunctionRepresentation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "How functions are represented and approximated in machine learning models."
    },
    {
      "id": "LinearHypothesis",
      "type": "subnode",
      "parent": "FunctionRepresentation",
      "description": "Approximating the target function with a linear model."
    },
    {
      "id": "ParametersWeights",
      "type": "subnode",
      "parent": "LinearHypothesis",
      "description": "The parameters or weights in a linear hypothesis function."
    },
    {
      "id": "CostFunction",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Measures the error between predicted values and actual values."
    },
    {
      "id": "OrdinaryLeastSquares",
      "type": "subnode",
      "parent": "CostFunction",
      "description": "A method for minimizing the cost function using least squares regression."
    },
    {
      "id": "LMSAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "An iterative algorithm to find the optimal parameters by minimizing the cost function."
    },
    {
      "id": "GradientDescentAlgorithm",
      "type": "major",
      "parent": null,
      "description": "Optimization algorithm that iteratively minimizes a cost function."
    },
    {
      "id": "LearningRate",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Hyperparameter controlling the step size during gradient descent."
    },
    {
      "id": "CostFunctionJ",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Function to be minimized, often representing error or loss."
    },
    {
      "id": "UpdateRule",
      "type": "subnode",
      "parent": "GradientDescentAlgorithm",
      "description": "Rule for updating model parameters based on gradients and learning rate."
    },
    {
      "id": "LMSUpdateRule",
      "type": "subnode",
      "parent": "UpdateRule",
      "description": "Least Mean Squares update rule, also known as Widrow-Hoff learning rule."
    },
    {
      "id": "PartialDerivative",
      "type": "subnode",
      "parent": "CostFunctionJ",
      "description": "Derivative of the cost function with respect to parameters theta."
    },
    {
      "id": "LMS_Update_Rule",
      "type": "major",
      "parent": null,
      "description": "Least Mean Squares update rule for adjusting parameters in machine learning."
    },
    {
      "id": "Widrow_Hoff_Learning_Rule",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Alternative name for the LMS update rule."
    },
    {
      "id": "Error_Term",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "The difference between actual and predicted values, used to adjust parameters."
    },
    {
      "id": "Single_Training_Example",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Derivation of LMS rule for a single training example."
    },
    {
      "id": "Batch_Gradient_Descent",
      "type": "major",
      "parent": null,
      "description": "Method that considers all examples in the training set on every step."
    },
    {
      "id": "Gradient_Descent",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Optimization algorithm used for minimizing cost functions like J(theta)."
    },
    {
      "id": "Single_Example_Update_Rule",
      "type": "subnode",
      "parent": "LMS_Update_Rule",
      "description": "Update rule derived from a single training example."
    },
    {
      "id": "Multiple_Examples_Update_Rule",
      "type": "subnode",
      "parent": "Batch_Gradient_Descent",
      "description": "Generalized update rule considering all examples in the dataset."
    },
    {
      "id": "LinearRegressionOptimization",
      "type": "major",
      "parent": null,
      "description": "Discussion on the optimization problem for linear regression."
    },
    {
      "id": "GradientDescentConvergence",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Explanation of why gradient descent converges to a global minimum in this context."
    },
    {
      "id": "ConvexQuadraticFunction",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Description of the convex quadratic function associated with linear regression optimization."
    },
    {
      "id": "GradientDescentExample",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "An example showing gradient descent applied to a quadratic function."
    },
    {
      "id": "BatchGradientDescent",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Explanation of batch gradient descent and its application in linear regression."
    },
    {
      "id": "StochasticGradientDescent",
      "type": "subnode",
      "parent": "LinearRegressionOptimization",
      "description": "Explanation of how stochastic gradient descent can be used to minimize the loss function based on calculated gradients."
    },
    {
      "id": "TrainingExampleUpdate",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Updates parameters based on gradient of single training example."
    },
    {
      "id": "ConvergenceBehavior",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Parameters oscillate around minimum but can converge with decreasing learning rate."
    },
    {
      "id": "NormalEquations",
      "type": "major",
      "parent": "LinearRegression",
      "description": "Direct method for finding the optimal parameters without iteration, based on solving linear equations."
    },
    {
      "id": "MatrixDerivatives",
      "type": "subnode",
      "parent": "NormalEquations",
      "description": "Notation for calculus with matrices and n-by-d matrices."
    },
    {
      "id": "Matrix Derivatives",
      "type": "major",
      "parent": null,
      "description": "Derivation of function with respect to matrix A"
    },
    {
      "id": "Gradient Calculation",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Process of calculating parameter gradients using backpropagation equations."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares using matrix derivatives"
    },
    {
      "id": "Design Matrix",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Matrix containing training examples' input values"
    },
    {
      "id": "Target Vector",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Vector containing target values from the training set"
    },
    {
      "id": "LinearRegression",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Techniques for predicting a dependent variable based on one or more independent variables."
    },
    {
      "id": "HypothesisFunction",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "A function that maps input data to predicted labels based on learned parameters."
    },
    {
      "id": "GradientDescent",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Optimization algorithm to minimize cost function by iteratively moving towards a minimum."
    },
    {
      "id": "InvertibilityAssumption",
      "type": "subnode",
      "parent": "LinearRegression",
      "description": "Conditions under which the matrix $X^{T}X$ is invertible."
    },
    {
      "id": "ProbabilisticInterpretation",
      "type": "major",
      "parent": null,
      "description": "Explanation of linear regression from a probabilistic standpoint."
    },
    {
      "id": "LeastSquaresCostFunction",
      "type": "subnode",
      "parent": "ProbabilisticInterpretation",
      "description": "Derivation and justification for using the least-squares cost function in regression problems."
    },
    {
      "id": "ErrorTermAssumption",
      "type": "subnode",
      "parent": "LeastSquaresCostFunction",
      "description": "Assumptions about error term $\textbf{\textepsilon}^{(i)}$ being Gaussian distributed with mean zero and variance $\textsigma^2$."
    },
    {
      "id": "DensityOfErrorTerm",
      "type": "subnode",
      "parent": "ErrorTermAssumption",
      "description": "Expression for the probability density function of error term $\textbf{\textepsilon}^{(i)}$."
    },
    {
      "id": "MachineLearningConcepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning including optimization methods and models."
    },
    {
      "id": "ConditionalProbabilityDistribution",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Describes the distribution of y given x and parameters θ."
    },
    {
      "id": "DesignMatrixX",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Contains all input vectors xi for a dataset."
    },
    {
      "id": "LikelihoodFunction",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and properties of the likelihood function in statistical models."
    },
    {
      "id": "IndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Each yi is independent given xi and θ."
    },
    {
      "id": "MaximumLikelihoodEstimation",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Method to choose parameters θ that maximize likelihood of observed data."
    },
    {
      "id": "LogLikelihoodFunction",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Natural logarithm of the likelihood function for easier computation."
    },
    {
      "id": "MachineLearningOverview",
      "type": "major",
      "parent": null,
      "description": "General introduction to machine learning concepts and terminology."
    },
    {
      "id": "LeastSquaresRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Method for fitting a linear model to data by minimizing squared error."
    },
    {
      "id": "ProbabilisticAssumptions",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Underlying probabilistic assumptions that justify least squares as MLE."
    },
    {
      "id": "LocallyWeightedLinearRegression",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Algorithm that assigns weights to training examples based on proximity to the query point."
    },
    {
      "id": "FeatureSelection",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Importance of choosing the right features for a model's performance."
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "FeatureSelection",
      "description": "Model fails to capture the underlying structure of the data, performing poorly even on training data."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "FeatureSelection",
      "description": "Happens when a model learns too much from training data, leading to poor performance on unseen data."
    },
    {
      "id": "PolynomialRegression",
      "type": "subnode",
      "parent": "FeatureSelection",
      "description": "Technique to fit a polynomial function to data by adding higher-order terms as features."
    },
    {
      "id": "WeightsCalculation",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Method for calculating weights using a formula similar to Gaussian distribution density."
    },
    {
      "id": "BandwidthParameter",
      "type": "subnode",
      "parent": "LocallyWeightedLinearRegression",
      "description": "Parameter controlling the influence of nearby training examples on the model fit."
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "General concepts in machine learning including parametric and non-parametric algorithms."
    },
    {
      "id": "Non-Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Algorithms where the number of parameters can grow with data size."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Non-Parametric Algorithms",
      "description": "A non-parametric method that requires keeping all training data for predictions."
    },
    {
      "id": "Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Algorithms with a fixed number of parameters independent of the dataset size."
    },
    {
      "id": "Classification Problem",
      "type": "major",
      "parent": null,
      "description": "Predicting discrete class labels for given input features."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "A classification task where the output is one of two possible classes."
    },
    {
      "id": "Logistic Regression",
      "type": "major",
      "parent": null,
      "description": "Statistical model for binary classification problems using a logistic function."
    },
    {
      "id": "Machine_Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from data to make predictions or decisions."
    },
    {
      "id": "Classification_Problem",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Task in machine learning where the goal is to predict a discrete class label for an input instance."
    },
    {
      "id": "Logistic_Regression",
      "type": "major",
      "parent": null,
      "description": "Statistical model used for binary classification problems by using a logistic function to model the probability of a certain class or event."
    },
    {
      "id": "ClassificationModels",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Models used for predicting categorical outcomes."
    },
    {
      "id": "LogisticRegressionModel",
      "type": "subnode",
      "parent": "ClassificationModels",
      "description": "Binary classification model using logistic regression."
    },
    {
      "id": "LogLikelihood",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Measure of how likely a given set of observations is under a model with specific parameters."
    },
    {
      "id": "GradientAscent",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Method for optimizing the ELBO using gradient ascent on parameters φ, ψ, and θ."
    },
    {
      "id": "LogisticRegression",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "GLM used for binary classification problems where the outcome is modeled as a probability using the logistic function."
    },
    {
      "id": "GradientAscentRule",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Update rule for parameters in logistic regression."
    },
    {
      "id": "LogisticLossFunction",
      "type": "subnode",
      "parent": "GradientAscentRule",
      "description": "Definition and properties of the logistic loss function."
    },
    {
      "id": "NegativeLogLikelihood",
      "type": "subnode",
      "parent": "LogisticLossFunction",
      "description": "Measure of how well a probabilistic model fits the observed data, used in training models."
    },
    {
      "id": "Logit",
      "type": "subnode",
      "parent": "GradientAscentRule",
      "description": "Linear combination of input features and weights before applying the logistic function."
    },
    {
      "id": "Logistic Regression Derivation",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Derivation of logistic regression's gradient descent update rule."
    },
    {
      "id": "Perceptron Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Historical learning algorithm that outputs binary values using a threshold function."
    },
    {
      "id": "Multi-class Classification",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Classification problems where the response variable can take on multiple discrete values."
    },
    {
      "id": "Multinomial distribution",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Distribution for multi-class outcomes with k probabilities summing to 1."
    },
    {
      "id": "Softmax function",
      "type": "subnode",
      "parent": "2.3 Multi-class classification",
      "description": "Function transforming scores into a probability distribution over multiple classes."
    },
    {
      "id": "SoftmaxFunction",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "A function that maps a vector of real numbers to a probability distribution over k possible outcomes."
    },
    {
      "id": "Logits",
      "type": "subnode",
      "parent": "SoftmaxFunction",
      "description": "Inputs to the softmax function, often used in machine learning models."
    },
    {
      "id": "ProbabilityVector",
      "type": "subnode",
      "parent": "SoftmaxFunction",
      "description": "Output of the softmax function representing probabilities of outcomes."
    },
    {
      "id": "ProbabilisticModel",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "A model that uses probability to predict outcomes based on input data."
    },
    {
      "id": "CrossEntropyLoss",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "A loss function that measures the performance of a classification model whose output is a probability value between 0 and 1."
    },
    {
      "id": "Machine_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and assumptions."
    },
    {
      "id": "Cross_Entropy_Loss",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and properties of cross-entropy loss function in classification tasks."
    },
    {
      "id": "Softmax_Function",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Explanation of the softmax function used in calculating probabilities from scores."
    },
    {
      "id": "Gradient_Calculation",
      "type": "subnode",
      "parent": "Cross_Entropy_Loss",
      "description": "Derivation and explanation of gradients for cross-entropy loss with respect to input vectors."
    },
    {
      "id": "GradientCalculation",
      "type": "subnode",
      "parent": "CrossEntropyLoss",
      "description": "Derivation and calculation of gradients for the cross-entropy loss with respect to model parameters."
    },
    {
      "id": "NewtonMethod",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Explanation of Newton's method for finding zeros of a function and its application to logistic regression optimization."
    },
    {
      "id": "Newton's Method",
      "type": "major",
      "parent": null,
      "description": "An iterative optimization algorithm for finding roots of a real-valued function."
    },
    {
      "id": "Finding Root",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "The process of finding the value of θ where f(θ) = 0 using Newton's method."
    },
    {
      "id": "Maximizing Function",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Using Newton's method to find maxima by setting first derivative equal to zero."
    },
    {
      "id": "Multidimensional Generalization",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Extending Newton's method to vector-valued θ using the Hessian matrix."
    },
    {
      "id": "OptimizationMethods",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques for finding the minimum or maximum of a function."
    },
    {
      "id": "HessianMatrix",
      "type": "subnode",
      "parent": "NewtonMethod",
      "description": "A square matrix of second-order partial derivatives used in Newton's method."
    },
    {
      "id": "FisherScoring",
      "type": "subnode",
      "parent": "NewtonMethod",
      "description": "Application of Newton's method to logistic regression for parameter estimation."
    },
    {
      "id": "GeneralizedLinearModels",
      "type": "major",
      "parent": "MachineLearningModels",
      "description": "Introduction to GLMs which can model various types of data distributions, including Poisson."
    },
    {
      "id": "ExponentialFamilyDistributions",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Class of statistical models that includes many common distributions like Bernoulli, Gaussian, Poisson, etc."
    },
    {
      "id": "NaturalParameter",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The canonical parameter η in the exponential family formula."
    },
    {
      "id": "SufficientStatistic",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "A function T(y) that summarizes data relevant to inference for a given model."
    },
    {
      "id": "LogPartitionFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "The function a(η) which acts as the log of the normalization constant."
    },
    {
      "id": "BernoulliDistribution",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Probability distribution of a random variable which takes value 1 with probability p and value 0 with probability q=1-p."
    },
    {
      "id": "NaturalParameterForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "η = log(φ/(1-φ)), which is the sigmoid function."
    },
    {
      "id": "SufficientStatisticForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "T(y) = y, representing the outcome of a Bernoulli trial."
    },
    {
      "id": "LogPartitionFunctionForBernoulli",
      "type": "subnode",
      "parent": "BernoulliDistribution",
      "description": "a(η) = log(1 + e^η), ensuring normalization."
    },
    {
      "id": "SigmoidFunction",
      "type": "subnode",
      "parent": "NaturalParameterForBernoulli",
      "description": "Maps real numbers to (0, 1) range; less commonly used due to vanishing gradient issue."
    },
    {
      "id": "MachineLearningModels",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning models including logistic regression and generative algorithms."
    },
    {
      "id": "GaussianDistribution",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Continuous probability distribution that is often used to model real-valued data such as heights or weights."
    },
    {
      "id": "PoissonDistributionModeling",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Using Poisson distribution for modeling website visitor counts based on features like promotions and weather."
    },
    {
      "id": "PredictionGoal",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "The goal is to predict the expected value of T(y) given x, often with T(y)=y."
    },
    {
      "id": "NaturalParameterRelation",
      "type": "subnode",
      "parent": "GeneralizedLinearModels",
      "description": "Assumption that natural parameter eta and inputs x are linearly related: eta = theta^T * x."
    },
    {
      "id": "GLMDesignChoices",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Three design choices for Generalized Linear Models (GLMs)."
    },
    {
      "id": "ConditionalDistributionModeling",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Focuses on modeling the conditional distribution p(y|x;θ)."
    },
    {
      "id": "BernoulliDistributions",
      "type": "subnode",
      "parent": "ConditionalDistributionModeling",
      "description": "Discusses using Bernoulli distributions for binary-valued y."
    },
    {
      "id": "LogisticFunction",
      "type": "subnode",
      "parent": "ExponentialFamilyDistributions",
      "description": "Derives and explains the logistic function as a result of Bernoulli distribution assumptions."
    },
    {
      "id": "CanonicalResponseFunction",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Introduces canonical response functions in GLMs."
    },
    {
      "id": "CanonicalLinkFunction",
      "type": "subnode",
      "parent": "CanonicalResponseFunction",
      "description": "Explains the inverse of the canonical response function as a link function."
    },
    {
      "id": "MachineLearningAlgorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms including gradient descent and stochastic gradient descent."
    },
    {
      "id": "ConditionalDistribution",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Modeling conditional probabilities of y given x and parameters theta."
    },
    {
      "id": "DiscriminativeApproach",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Directly modeling the decision boundary between classes"
    },
    {
      "id": "PerceptronAlgorithm",
      "type": "subnode",
      "parent": "DiscriminativeApproach",
      "description": "Learning mappings from input space to labels directly"
    },
    {
      "id": "GenerativeApproach",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Modeling p(x|y) and using Bayes' rule for classification"
    },
    {
      "id": "ClassPriors",
      "type": "subnode",
      "parent": "GenerativeApproach",
      "description": "The prior probability of each class before observing data"
    },
    {
      "id": "Bayesian Classification",
      "type": "major",
      "parent": null,
      "description": "Classification using Bayes' theorem to calculate posterior probabilities."
    },
    {
      "id": "Class Priors",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Prior probability of each class before observing data."
    },
    {
      "id": "Likelihood",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Probability of features given a specific class."
    },
    {
      "id": "Posterior Distribution",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Probability distribution after incorporating evidence."
    },
    {
      "id": "Gaussian Discriminant Analysis (GDA)",
      "type": "major",
      "parent": null,
      "description": "Generative learning model assuming multivariate normal distribution for likelihood."
    },
    {
      "id": "Machine_Learning_Basics",
      "type": "major",
      "parent": null,
      "description": "Introduction to fundamental concepts in machine learning."
    },
    {
      "id": "Random_Variables",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "Variables whose possible values are outcomes of a random phenomenon."
    },
    {
      "id": "Gaussian_Distribution",
      "type": "subnode",
      "parent": "Machine_Learning_Basics",
      "description": "A continuous probability distribution that is often used in machine learning for modeling data."
    },
    {
      "id": "Mean_of_Gaussian",
      "type": "subnode",
      "parent": "Gaussian_Distribution",
      "description": "The expected value of a Gaussian random variable."
    },
    {
      "id": "Covariance_Matrix",
      "type": "subnode",
      "parent": "Gaussian_Distribution",
      "description": "A matrix that gives the covariance between each pair of elements of a given random vector."
    },
    {
      "id": "Standard_Normal_Distribution",
      "type": "subnode",
      "parent": "Gaussian_Distribution",
      "description": "A Gaussian distribution with zero mean and identity covariance matrix."
    },
    {
      "id": "Density_Properties",
      "type": "subnode",
      "parent": "Gaussian_Distribution",
      "description": "Properties of the density function of a Gaussian distribution, including spread-out and compression based on covariance."
    },
    {
      "id": "MultivariateDistributions",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Exploration of multivariate normal distributions in ML."
    },
    {
      "id": "CovarianceMatrixImpact",
      "type": "subnode",
      "parent": "MultivariateDistributions",
      "description": "Effect of covariance matrix on density contours."
    },
    {
      "id": "MeanVectorMovement",
      "type": "subnode",
      "parent": "MultivariateDistributions",
      "description": "Influence of mean vector changes on distribution position."
    },
    {
      "id": "GaussianDiscriminantAnalysis",
      "type": "major",
      "parent": "MachineLearningModels",
      "description": "Model for classification problems with continuous features using multivariate normal distributions."
    },
    {
      "id": "NormalDistributionsInGDA",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "Use of normal distributions for class-conditional probabilities in GDA."
    },
    {
      "id": "DecisionBoundary",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "The boundary that separates different classes in a classification problem."
    },
    {
      "id": "ParameterEstimation",
      "type": "subnode",
      "parent": "LogLikelihood",
      "description": "Process of finding optimal parameters for models using maximum likelihood estimation."
    },
    {
      "id": "DecisionBoundaries",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "The criteria that separate different classes in classification models."
    },
    {
      "id": "ModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "The underlying assumptions made by machine learning models about the data distribution."
    },
    {
      "id": "AsymptoticEfficiency",
      "type": "subnode",
      "parent": "GaussianDiscriminantAnalysis",
      "description": "Property of GDA that ensures optimal performance in large datasets when assumptions are correct."
    },
    {
      "id": "GDA",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Generative Discriminative Algorithm with strong modeling assumptions."
    },
    {
      "id": "Robustness",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Logistic regression's robustness against incorrect modeling assumptions."
    },
    {
      "id": "PoissonData",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Application of models on Poisson distributed data."
    },
    {
      "id": "NaiveBayes",
      "type": "major",
      "parent": "BernoulliEventModel",
      "description": "A generative learning algorithm that works well for many classification problems including text classification with modifications."
    },
    {
      "id": "Text_Classification",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Techniques for categorizing text into predefined classes such as spam detection."
    },
    {
      "id": "Spam_Filtering",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Application of machine learning to distinguish spam from non-spam emails."
    },
    {
      "id": "Training_Set",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Set of examples used to train a machine learning model."
    },
    {
      "id": "Feature_Vector",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Vector representation of an email based on its content."
    },
    {
      "id": "Vocabulary",
      "type": "subnode",
      "parent": "Feature_Vector",
      "description": "Set of words used to represent emails in a feature vector."
    },
    {
      "id": "Stop_Words",
      "type": "subnode",
      "parent": "Vocabulary",
      "description": "Commonly excluded high-frequency words like 'the', 'of', and 'and'."
    },
    {
      "id": "Machine_Learning_Topic",
      "type": "major",
      "parent": null,
      "description": "General topic of machine learning concepts and algorithms."
    },
    {
      "id": "Feature_Vector_Selection",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Process of selecting relevant features from raw data, often excluding stop words."
    },
    {
      "id": "Naive_Bayes_Assumption",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Assumption that attributes are conditionally independent given the class label in Naive Bayes classifiers."
    },
    {
      "id": "Generative_Models",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Models that generate data instances from probability distributions over features and classes."
    },
    {
      "id": "Conditional_Independence",
      "type": "subnode",
      "parent": "Naive_Bayes_Assumption",
      "description": "Concept where variables are independent given a specific condition, in this case the class label y."
    },
    {
      "id": "NaiveBayesAlgorithm",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "ConditionalProbability",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "The probability of a feature given the class label in Naive Bayes."
    },
    {
      "id": "JointLikelihood",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Probability of observing data under model parameters."
    },
    {
      "id": "Prediction",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Using learned parameters to predict class labels for new examples."
    },
    {
      "id": "BinaryFeatures",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Features are binary-valued in the basic formulation of Naive Bayes."
    },
    {
      "id": "MultinomialGeneralization",
      "type": "subnode",
      "parent": "NaiveBayesAlgorithm",
      "description": "Extension to handle features with multiple discrete values."
    },
    {
      "id": "LaplaceSmoothing",
      "type": "major",
      "parent": "ProbabilityEstimation",
      "description": "A technique to avoid zero probabilities by adding a small constant to the numerator and denominator."
    },
    {
      "id": "Machine_Learning_Conferences",
      "type": "major",
      "parent": null,
      "description": "Top machine learning conferences including NeurIPS."
    },
    {
      "id": "NeurIPS_Paper_Submission",
      "type": "subnode",
      "parent": "Machine_Learning_Conferences",
      "description": "Process of submitting a paper to the NeurIPS conference."
    },
    {
      "id": "Naive_Bayes_Filter",
      "type": "major",
      "parent": null,
      "description": "Algorithm for classifying emails as spam or non-spam using Naive Bayes classifier."
    },
    {
      "id": "New_Words_in_Emails",
      "type": "subnode",
      "parent": "Naive_Bayes_Filter",
      "description": "Handling of new words not seen in the training set by a Naive Bayes filter."
    },
    {
      "id": "Probability_Estimation_Issue",
      "type": "subnode",
      "parent": "New_Words_in_Emails",
      "description": "Problem with estimating probability as zero for unseen events based on finite data."
    },
    {
      "id": "ProbabilityEstimation",
      "type": "major",
      "parent": null,
      "description": "Discusses the issue of estimating probabilities from finite data sets."
    },
    {
      "id": "MultinomialRandomVariable",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "A random variable taking values in a discrete set with probabilities summing to 1."
    },
    {
      "id": "MaximumLikelihoodEstimates",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "Estimating parameters based on observed data, which can result in zero probability estimates."
    },
    {
      "id": "NaiveBayesClassifier",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between features."
    },
    {
      "id": "EventModelsForTextClassification",
      "type": "subnode",
      "parent": "ProbabilityEstimation",
      "description": "Models used to classify text data by estimating the probability of events related to the text."
    },
    {
      "id": "BernoulliEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "A model used for text classification assuming binary inclusion or exclusion of words."
    },
    {
      "id": "MultinomialEventModel",
      "type": "subnode",
      "parent": "EventModelsForTextClassification",
      "description": "An alternative to Bernoulli, considering word counts and identities in emails."
    },
    {
      "id": "SpamNonSpamDetermination",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "First step in generating an email: determining if it's spam or not-spam."
    },
    {
      "id": "WordGenerationProcess",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "Generating each word independently from a multinomial distribution."
    },
    {
      "id": "ProbabilityFormula",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "Overall probability of an email given by the product of individual probabilities."
    },
    {
      "id": "ParametersDefinition",
      "type": "subnode",
      "parent": "MultinomialEventModel",
      "description": "Definitions for parameters φ_y, φ_k|y=1, and φ_k|y=0."
    },
    {
      "id": "Naive_Bayes_Classifier",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "Parameter_Estimation",
      "type": "subnode",
      "parent": "Naive_Bayes_Classifier",
      "description": "Process of estimating parameters for the Naive Bayes model using maximum likelihood estimation."
    },
    {
      "id": "Laplace_Smoothing",
      "type": "subnode",
      "parent": "Parameter_Estimation",
      "description": "Technique to avoid zero probabilities in parameter estimates by adding a small constant to counts."
    },
    {
      "id": "Kernel_Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques for extending linear models to handle non-linear relationships between variables."
    },
    {
      "id": "Feature_Maps",
      "type": "subnode",
      "parent": "Kernel_Methods",
      "description": "Transformation of input data into a higher-dimensional space to fit more complex models."
    },
    {
      "id": "Feature_Mapping",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Mapping data into a higher dimensional space to compute similarity metrics."
    },
    {
      "id": "Linear_Functions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Representation of linear functions over transformed features."
    },
    {
      "id": "Cubic_Function_Example",
      "type": "subnode",
      "parent": "Feature_Mapping",
      "description": "Illustration using a cubic function as an example."
    },
    {
      "id": "LMS_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Least Mean Squares algorithm for fitting models with features."
    },
    {
      "id": "Gradient_Descent_Update_Rule",
      "type": "subnode",
      "parent": "LMS_Algorithm",
      "description": "Update rule for gradient descent in the context of feature maps."
    },
    {
      "id": "GradientDescentUpdateRule",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "The update rule for the gradient descent algorithm in machine learning."
    },
    {
      "id": "FeatureMapPhiX",
      "type": "subnode",
      "parent": "MachineLearningAlgorithms",
      "description": "Mapping input features to higher-dimensional space using the function φ(x)."
    },
    {
      "id": "HighDimensionalFeatures",
      "type": "subnode",
      "parent": "FeatureMapPhiX",
      "description": "The computational challenge when dealing with high-dimensional feature maps."
    },
    {
      "id": "KernelTrickIntroduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to the kernel trick to address computational issues in machine learning algorithms."
    },
    {
      "id": "KernelTrick",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Technique used to apply the dual form with high-dimensional feature spaces."
    },
    {
      "id": "PhiFunction",
      "type": "subnode",
      "parent": "KernelTrick",
      "description": "Mapping function that transforms input into a higher dimensional space."
    },
    {
      "id": "ThetaVector",
      "type": "subnode",
      "parent": "KernelTrick",
      "description": "Weight vector in the transformed feature space, often of high dimensionality."
    },
    {
      "id": "Initialization",
      "type": "subnode",
      "parent": "KernelTrick",
      "description": "Random initialization of parameters before starting the algorithm."
    },
    {
      "id": "IterativeUpdateRule",
      "type": "subnode",
      "parent": "KernelTrick",
      "description": "Process for updating coefficients in each iteration based on input data and current model predictions."
    },
    {
      "id": "BetaCoefficients",
      "type": "subnode",
      "parent": "ThetaVector",
      "description": "Set of coefficients used to implicitly represent the high-dimensional θ vector."
    },
    {
      "id": "Batch Gradient Descent",
      "type": "major",
      "parent": null,
      "description": "Algorithm for updating parameters in machine learning models."
    },
    {
      "id": "Beta Update Equation",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Equation defining how β is updated iteratively."
    },
    {
      "id": "Theta Representation",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Representation of θ as a sum of feature maps weighted by β."
    },
    {
      "id": "Pre-Computation Optimization",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Optimization technique for pre-computing inner products to speed up iterations."
    },
    {
      "id": "Efficient Inner Product Calculation",
      "type": "subnode",
      "parent": "Pre-Computation Optimization",
      "description": "Method to compute inner product efficiently without explicit feature map computation."
    },
    {
      "id": "Feature_Maps_and_Kernels",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Exploration of feature maps and their corresponding kernels."
    },
    {
      "id": "Kernel_Functions",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Definition and properties of kernel functions in machine learning."
    },
    {
      "id": "Inner_Product_Computation",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Techniques for computing inner products between feature maps."
    },
    {
      "id": "Algorithm_for_Kernel_Calculation",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Detailed algorithm to calculate kernel values efficiently."
    },
    {
      "id": "Efficient_Representation_Update",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Method for updating the representation vector efficiently using kernels."
    },
    {
      "id": "Prediction_From_Kernel",
      "type": "subnode",
      "parent": "Feature_Maps_and_Kernels",
      "description": "Using kernel knowledge to compute predictions from feature maps."
    },
    {
      "id": "KernelsInML",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Introduction to kernel functions used in machine learning for efficient computation of inner products in high-dimensional spaces."
    },
    {
      "id": "FeatureMapPhi",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Definition and role of feature map φ in inducing kernel function K(x,z)."
    },
    {
      "id": "KernelFunctionProperties",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Discussion on intrinsic properties and characteristics of the kernel function."
    },
    {
      "id": "Algorithm5.11",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Description of algorithm (5.11) that operates based on kernel functions without needing explicit feature map φ."
    },
    {
      "id": "ValidKernelFunctions",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Criteria for determining if a given function K(x,z) can be considered a valid kernel function."
    },
    {
      "id": "ExampleKernels",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Concrete examples of kernels and their properties, including the example with (x^Tz)^2."
    },
    {
      "id": "KernelFunctionExample1",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "First example of a kernel function defined as \\\\(K(x,z) = (x^{T}z)^{2}\\\\)."
    },
    {
      "id": "FeatureMappingPhi",
      "type": "subnode",
      "parent": "KernelFunctionExample1",
      "description": "Feature mapping \\(\\phi(x)\\) corresponding to the first kernel function."
    },
    {
      "id": "ComputationalEfficiency",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Discussion on computational efficiency of calculating kernels versus high-dimensional feature mappings."
    },
    {
      "id": "KernelFunctionExample2",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Second example of a kernel function defined as \\\\(K(x,z) = (x^{T}z+c)^{2}\\\\)."
    },
    {
      "id": "FeatureMappingPhiWithC",
      "type": "subnode",
      "parent": "KernelFunctionExample2",
      "description": "Feature mapping \\(\\phi(x)\\) corresponding to the second kernel function including parameter \\(c\\)."
    },
    {
      "id": "ParameterCControl",
      "type": "subnode",
      "parent": "KernelFunctionExample2",
      "description": "Explanation of how parameter \\(c\\) controls the weighting between first and second order terms."
    },
    {
      "id": "GeneralKernels",
      "type": "subnode",
      "parent": "KernelsInML",
      "description": "Overview of general kernels defined as \\\\(K(x,z)=(x^{T}z+c)^{k}\\)."
    },
    {
      "id": "Machine_Learning_Kernels",
      "type": "major",
      "parent": null,
      "description": "Overview of kernels in machine learning and their properties."
    },
    {
      "id": "Kernel_Similarity_Metrics",
      "type": "subnode",
      "parent": "Machine_Learning_Kernels",
      "description": "Using kernels as measures of similarity between input vectors."
    },
    {
      "id": "Gaussian_Kernel",
      "type": "subnode",
      "parent": "Machine_Learning_Kernels",
      "description": "A specific kernel function that corresponds to an infinite dimensional feature space."
    },
    {
      "id": "Valid_Kernel_Conditions",
      "type": "subnode",
      "parent": "Machine_Learning_Kernels",
      "description": "Conditions a function must meet to be considered a valid kernel in machine learning."
    },
    {
      "id": "NecessaryConditions",
      "type": "subnode",
      "parent": "KernelFunctionProperties",
      "description": "Symmetry and positive semi-definiteness of the kernel matrix."
    },
    {
      "id": "SufficientConditions",
      "type": "subnode",
      "parent": "KernelFunctionProperties",
      "description": "Condition for K to be a valid kernel is both necessary and sufficient."
    },
    {
      "id": "FeatureMapping",
      "type": "subnode",
      "parent": "NecessaryConditions",
      "description": "Correspondence between kernel function and feature mapping."
    },
    {
      "id": "SymmetryProperty",
      "type": "subnode",
      "parent": "NecessaryConditions",
      "description": "Kernel matrix must be symmetric."
    },
    {
      "id": "PositiveSemiDefinite",
      "type": "subnode",
      "parent": "NecessaryConditions",
      "description": "Kernel matrix is positive semi-definite."
    },
    {
      "id": "ArbitraryVectorZ",
      "type": "subnode",
      "parent": "PositiveSemiDefinite",
      "description": "For any vector z, the expression z^T Kz >= 0 holds."
    },
    {
      "id": "Kernels in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of kernel functions and their properties."
    },
    {
      "id": "Valid Kernels",
      "type": "subnode",
      "parent": "Kernels in Machine Learning",
      "description": "Conditions for a function to be considered a valid Mercer kernel."
    },
    {
      "id": "Mercer's Theorem",
      "type": "subnode",
      "parent": "Valid Kernels",
      "description": "Theorem stating necessary and sufficient conditions for a kernel matrix to be positive semi-definite."
    },
    {
      "id": "Testing Kernel Validity",
      "type": "subnode",
      "parent": "Valid Kernels",
      "description": "Alternative methods to test if a function is a valid kernel, including feature mapping."
    },
    {
      "id": "Kernel Examples",
      "type": "subnode",
      "parent": "Kernels in Machine Learning",
      "description": "Examples of kernels used for specific problems such as digit recognition and protein classification."
    },
    {
      "id": "Machine_Learning_Features",
      "type": "major",
      "parent": null,
      "description": "Features used in machine learning models for predicting housing prices."
    },
    {
      "id": "String_Features",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Feature extraction from strings using substring counts."
    },
    {
      "id": "Support_Vector_Machines",
      "type": "subnode",
      "parent": "Kernel_Methods",
      "description": "Supervised learning algorithm for classification and regression analysis."
    },
    {
      "id": "Machine_Learning_Algorithms",
      "type": "major",
      "parent": null,
      "description": "Algorithms used in machine learning for solving problems like MDPs."
    },
    {
      "id": "Kernel_Trick",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Technique used to apply linear classifiers to non-linear data by transforming the feature space."
    },
    {
      "id": "Margins_Intuition",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Introduction to the concept of margins in SVMs."
    },
    {
      "id": "Optimal_Margin_Classifier",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Classifier that maximizes the margin between classes."
    },
    {
      "id": "Lagrange_Duality",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Mathematical framework for solving optimization problems in SVMs."
    },
    {
      "id": "Kernels_in_SVMs",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Technique allowing SVM to work efficiently in high-dimensional spaces."
    },
    {
      "id": "SMO_Algorithm",
      "type": "subnode",
      "parent": "Support_Vector_Machines",
      "description": "Efficient algorithm for solving the dual problem in SVMs, introduced by John Platt."
    },
    {
      "id": "Decision Boundaries",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Boundaries used to classify data points into different categories."
    },
    {
      "id": "Functional Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Concept formalizing confident classifications based on distance from decision boundary."
    },
    {
      "id": "Geometric Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Measure of confidence in classification based on geometric distance to the separating hyperplane."
    },
    {
      "id": "Training Set",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Collection of data used for training a machine learning model."
    },
    {
      "id": "Support Vector Machines (SVMs)",
      "type": "major",
      "parent": null,
      "description": "Algorithm for classification that maximizes the margin between classes."
    },
    {
      "id": "Support_Vector_Machines_SVMs",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Binary classification algorithm focusing on geometric margins."
    },
    {
      "id": "Notation_for_SVMs",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "Introduction to SVM-specific notation and parameters."
    },
    {
      "id": "Functional_and_Geometric_Margins",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "Explanation of margins in the context of SVMs, including definitions and implications."
    },
    {
      "id": "FunctionalMargin",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "A margin that depends on scaling of parameters and is related to geometric margin when norm of W equals 1."
    },
    {
      "id": "ConfidenceMeasure",
      "type": "subnode",
      "parent": "FunctionalMargin",
      "description": "Discussion on the adequacy of functional margin as a confidence measure."
    },
    {
      "id": "NormalizationCondition",
      "type": "subnode",
      "parent": "FunctionalMargin",
      "description": "Proposed normalization condition to address issues with functional margin scaling."
    },
    {
      "id": "FunctionMarginTrainingSet",
      "type": "subnode",
      "parent": "FunctionalMargin",
      "description": "Definition of function margin in the context of a training set."
    },
    {
      "id": "GeometricMargins",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to geometric margins and their relationship with decision boundaries."
    },
    {
      "id": "VectorW",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "A vector orthogonal to the decision boundary and related to class separation."
    },
    {
      "id": "DistanceToBoundary",
      "type": "subnode",
      "parent": "DecisionBoundary",
      "description": "The perpendicular distance from a point to the decision boundary."
    },
    {
      "id": "UnitVectorW",
      "type": "subnode",
      "parent": "VectorW",
      "description": "A unit vector in the direction of W, used for calculating distances."
    },
    {
      "id": "GeometricMargin",
      "type": "major",
      "parent": "MachineLearningConcepts",
      "description": "The perpendicular distance from a point to the decision boundary, scaled by the class label."
    },
    {
      "id": "ScalingInvariance",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "The property of a model to remain unchanged under scaling transformations of its parameters."
    },
    {
      "id": "OptimalMarginClassifier",
      "type": "major",
      "parent": null,
      "description": "A classifier that maximizes geometric margin for a given dataset."
    },
    {
      "id": "LinearSeparabilityAssumption",
      "type": "subnode",
      "parent": "OptimalMarginClassifier",
      "description": "Assumption that positive and negative examples can be separated by a hyperplane."
    },
    {
      "id": "MaximizeGeometricMarginProblem",
      "type": "subnode",
      "parent": "OptimalMarginClassifier",
      "description": "Formulation of the optimization problem to find classifier with maximum geometric margin."
    },
    {
      "id": "Support Vector Machines (SVM)",
      "type": "major",
      "parent": null,
      "description": "Machine learning model for classification and regression analysis."
    },
    {
      "id": "Optimization Problem in SVM",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Key mathematical formulation to solve for optimal hyperplane."
    },
    {
      "id": "Geometric Margin",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Distance between the decision boundary and closest data points."
    },
    {
      "id": "Functional Margin",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Margin considering weight vector normalization."
    },
    {
      "id": "Non-Convex Constraint Issue",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Challenge with initial constraint on weight vector norm."
    },
    {
      "id": "Transformation to Convex Formulation",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Process of converting non-convex problem into a convex one."
    },
    {
      "id": "Scaling Constraint",
      "type": "subnode",
      "parent": "Transformation to Convex Formulation",
      "description": "Constraint ensuring functional margin is 1, simplifying optimization."
    },
    {
      "id": "Final Optimization Problem",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Convex formulation for finding optimal hyperplane parameters."
    },
    {
      "id": "Optimization Problem",
      "type": "major",
      "parent": "Non-Separable Case",
      "description": "Formulation to minimize misclassification and margin shrinkage due to outliers."
    },
    {
      "id": "Convex Quadratic Objective",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Objective function to minimize in the optimization problem."
    },
    {
      "id": "Linear Constraints",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Constraints that must be satisfied, linear in nature."
    },
    {
      "id": "Optimal Margin Classifier",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Result of solving the optimization problem."
    },
    {
      "id": "Quadratic Programming (QP)",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Method for solving convex quadratic objective problems with linear constraints."
    },
    {
      "id": "Lagrange Duality",
      "type": "major",
      "parent": null,
      "description": "Concept used to derive the dual form of optimization problems."
    },
    {
      "id": "Constrained Optimization Problems",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Problems with an objective function and constraints."
    },
    {
      "id": "Lagrangian",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Function combining the objective and constraint functions using Lagrange multipliers."
    },
    {
      "id": "Lagrange Multipliers",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Coefficients used in the Lagrangian to enforce constraints."
    },
    {
      "id": "ConstrainedOptimization",
      "type": "major",
      "parent": null,
      "description": "Generalization of Lagrange multipliers to include inequality constraints"
    },
    {
      "id": "LagrangeMultipliers",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Coefficients used in the Lagrangian function for optimization problems"
    },
    {
      "id": "PrimalProblem",
      "type": "subnode",
      "parent": "ConstrainedOptimization",
      "description": "Minimizing a function subject to inequality and equality constraints"
    },
    {
      "id": "GeneralizedLagrangian",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Combination of the objective function with Lagrange multipliers for constraints"
    },
    {
      "id": "ThetaP",
      "type": "subnode",
      "parent": "PrimalProblem",
      "description": "Function that returns f(w) if w satisfies primal constraints, otherwise infinity"
    },
    {
      "id": "Primal_Problem",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Definition and objective of the primal problem in optimization."
    },
    {
      "id": "Dual_Problem",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "A related optimization problem that can sometimes replace the primal problem."
    },
    {
      "id": "Primal_Objective_Function",
      "type": "subnode",
      "parent": "Primal_Problem",
      "description": "Objective function for the primal problem, θℚ(w)."
    },
    {
      "id": "Dual_Objective_Function",
      "type": "subnode",
      "parent": "Dual_Problem",
      "description": "Objective function for the dual problem, θℝ(α,β)."
    },
    {
      "id": "Optimal_Value_Primal",
      "type": "subnode",
      "parent": "Primal_Problem",
      "description": "Definition of p* as the optimal value for the primal problem."
    },
    {
      "id": "Optimal_Value_Dual",
      "type": "subnode",
      "parent": "Dual_Problem",
      "description": "Definition of d* as the optimal value for the dual problem."
    },
    {
      "id": "Relationship_Primal_Dual",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Explanation of the relationship between primal and dual problems, including inequalities."
    },
    {
      "id": "Machine_Learning_Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimization problems in machine learning and their solutions."
    },
    {
      "id": "d_star_p_star_Equality",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Equality condition under which dual and primal problems have the same solution."
    },
    {
      "id": "Convexity_Conditions",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Conditions on functions for convex optimization."
    },
    {
      "id": "Feasibility_Constraints",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Constraints ensuring the feasibility of solutions in optimization problems."
    },
    {
      "id": "KKT_Conditions",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Conditions that must be satisfied at the optimal solution of a constrained optimization problem."
    },
    {
      "id": "Optimal_Margin_Classifiers",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Focuses on classifiers that maximize the margin between classes."
    },
    {
      "id": "Dual_Formulation",
      "type": "subnode",
      "parent": "Optimal_Margin_Classifiers",
      "description": "Transformation of the primal problem into a dual form for easier computation."
    },
    {
      "id": "Support_Vectors",
      "type": "subnode",
      "parent": "KKT_Conditions",
      "description": "Training examples that lie on the margin and influence the classifier's decision boundary."
    },
    {
      "id": "Dual_Complementarity",
      "type": "subnode",
      "parent": "KKT_Conditions",
      "description": "Condition indicating when constraints are active in optimization problems."
    },
    {
      "id": "SupportVectors",
      "type": "major",
      "parent": null,
      "description": "Points that lie on the decision boundary and affect the optimal solution."
    },
    {
      "id": "DualFormulation",
      "type": "major",
      "parent": null,
      "description": "Derivation of the dual form of the optimization problem in machine learning."
    },
    {
      "id": "LagrangianFunction",
      "type": "subnode",
      "parent": "DualFormulation",
      "description": "Mathematical function used to find the optimal solution involving Lagrange multipliers."
    },
    {
      "id": "InnerProduct",
      "type": "subnode",
      "parent": "DualFormulation",
      "description": "Key idea in expressing algorithm terms of inner products between input points."
    },
    {
      "id": "Optimization_Problems",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Formulation and solving of optimization problems in ML."
    },
    {
      "id": "Lagrangian_Formulation",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Use of Lagrangian to formulate constrained optimization problems."
    },
    {
      "id": "Dual_Optimization",
      "type": "subnode",
      "parent": "Optimization_Problems",
      "description": "Transformation and solving of dual optimization problems."
    },
    {
      "id": "OptimalValueForInterceptTerm",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Derivation and formula for the optimal intercept term b* in a model."
    },
    {
      "id": "PredictionUsingSupportVectors",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Process of making predictions using support vectors and inner products."
    },
    {
      "id": "SVMAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to the Support Vector Machine algorithm for classification in high-dimensional spaces."
    },
    {
      "id": "RegularizationAndNonSeparableCase",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on handling non-separable data with regularization techniques."
    },
    {
      "id": "Support Vector Machines",
      "type": "major",
      "parent": null,
      "description": "Algorithm for learning in high-dimensional spaces."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Technique used in machine learning to manage model complexity by adding a regularizer term to the loss function."
    },
    {
      "id": "Non-Separable Case",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Handling datasets that are not linearly separable."
    },
    {
      "id": "L1 Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Penalizes the sum of absolute values of coefficients to reduce model complexity."
    },
    {
      "id": "Lagrangian Function",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Dual formulation used for solving the optimization problem with constraints."
    },
    {
      "id": "Support_Vector_Machines_SVM",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Algorithm for classification and regression analysis based on statistical learning theory."
    },
    {
      "id": "Dual_Problem_Formulation",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVM",
      "description": "Formulation of the SVM problem in its dual form to simplify optimization."
    },
    {
      "id": "Lagrange_Multipliers",
      "type": "subnode",
      "parent": "Dual_Problem_Formulation",
      "description": "Multipliers used in constrained optimization problems, here for SVMs."
    },
    {
      "id": "Sequential_Minimal_Optimization_SMO",
      "type": "subnode",
      "parent": "Support_Vector_Machines_SVMs",
      "description": "Efficient algorithm for solving the dual problem of SVMs."
    },
    {
      "id": "Coordinate_Ascend_Algorithm",
      "type": "major",
      "parent": null,
      "description": "Optimization technique for unconstrained problems."
    },
    {
      "id": "Unconstrained_Optimization_Problem",
      "type": "subnode",
      "parent": "Coordinate_Ascend_Algorithm",
      "description": "Maximizing a function of parameters without constraints."
    },
    {
      "id": "Coordinate_Ascent",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Algorithm for optimizing functions by moving along coordinate directions."
    },
    {
      "id": "Quadratic_Function_Optimization",
      "type": "subnode",
      "parent": "Coordinate_Ascent",
      "description": "Optimizing a quadratic function using coordinate ascent."
    },
    {
      "id": "SVM_Derivation_SMO",
      "type": "major",
      "parent": null,
      "description": "Deriving the Sequential Minimal Optimization algorithm for SVMs."
    },
    {
      "id": "Dual_Optimization_Problem",
      "type": "subnode",
      "parent": "SVM_Derivation_SMO",
      "description": "Formulation of the dual optimization problem in SVMs."
    },
    {
      "id": "SMO_Algorithm_Overview",
      "type": "subnode",
      "parent": "SVM_Derivation_SMO",
      "description": "Overview and motivation for the SMO algorithm used in SVM optimization."
    },
    {
      "id": "Convergence_Criteria",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Conditions for determining when the EM algorithm has reached its optimal solution."
    },
    {
      "id": "Efficient_Update_Strategy",
      "type": "subnode",
      "parent": "SMO_Algorithm",
      "description": "Strategy to efficiently update alpha values in SMO."
    },
    {
      "id": "Constraints_on_Alphas",
      "type": "subnode",
      "parent": "Efficient_Update_Strategy",
      "description": "Mathematical constraints on the alpha values during optimization."
    },
    {
      "id": "Optimization_in_Machine_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for optimizing functions in machine learning models."
    },
    {
      "id": "Alpha_Parameters",
      "type": "subnode",
      "parent": "Optimization_in_Machine_Learning",
      "description": "Parameters α used to optimize the objective function W(α)."
    },
    {
      "id": "Objective_Function_W",
      "type": "subnode",
      "parent": "Optimization_in_Machine_Learning",
      "description": "Function W(α) to be optimized in machine learning models."
    },
    {
      "id": "Quadratic_Formulation",
      "type": "subnode",
      "parent": "Objective_Function_W",
      "description": "Formulating the objective function as a quadratic equation for optimization."
    },
    {
      "id": "SMO Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Sequential Minimal Optimization algorithm for solving SVM problems."
    },
    {
      "id": "Alpha Update",
      "type": "subnode",
      "parent": "SMO Algorithm",
      "description": "Process of updating alpha values in SMO."
    },
    {
      "id": "Box Constraint",
      "type": "subnode",
      "parent": "Alpha Update",
      "description": "Constraint that limits the range of alpha values."
    },
    {
      "id": "Deep Learning Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to deep learning concepts and neural networks."
    },
    {
      "id": "Supervised Learning with Non-Linear Models",
      "type": "subnode",
      "parent": "Deep Learning Introduction",
      "description": "Exploration of non-linear models in supervised learning context."
    },
    {
      "id": "NonLinearModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Abstract non-linear model used in machine learning for regression problems."
    },
    {
      "id": "TrainingExamples",
      "type": "subnode",
      "parent": "NonLinearModel",
      "description": "Set of training examples used to define the cost function."
    },
    {
      "id": "LeastSquareCostFunction",
      "type": "subnode",
      "parent": "NonLinearModel",
      "description": "Definition and formula for least square cost function in regression problems."
    },
    {
      "id": "MeanSquaredLoss",
      "type": "subnode",
      "parent": "NonLinearModel",
      "description": "Definition of mean squared loss for the entire dataset."
    },
    {
      "id": "BinaryClassification",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Introduction to binary classification models and their cost functions."
    },
    {
      "id": "ProbabilityPrediction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Output probability for class 1 using the logistic function on logits."
    },
    {
      "id": "NegativeLikelihoodLoss",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Loss function derived from the negative log-likelihood for binary classification."
    },
    {
      "id": "TotalLossFunction",
      "type": "subnode",
      "parent": "LogisticRegression",
      "description": "Average of individual loss functions over all training examples."
    },
    {
      "id": "MultiClassClassification",
      "type": "major",
      "parent": null,
      "description": "Extension of binary classification to multiple classes using softmax function."
    },
    {
      "id": "LogitsInMulticlass",
      "type": "subnode",
      "parent": "MultiClassClassification",
      "description": "Vector of predictions for each class before applying softmax function."
    },
    {
      "id": "NegativeLogLikelihoodLossMulticlass",
      "type": "subnode",
      "parent": "MultiClassClassification",
      "description": "Loss function derived from the negative log-likelihood in multi-class setting."
    },
    {
      "id": "Loss Function",
      "type": "major",
      "parent": "Self-Supervised Pretraining",
      "description": "Function to minimize distance between positive pairs and maximize distance between negative pairs."
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Specific form of loss function for probabilistic models."
    },
    {
      "id": "Cross-Entropy Loss",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Loss function used for measuring the performance of a classification model."
    },
    {
      "id": "Average Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Total loss averaged over all training examples."
    },
    {
      "id": "Conditional Probabilistic Model",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Model where output distribution is conditional on input features."
    },
    {
      "id": "Exponential Family",
      "type": "subnode",
      "parent": "Conditional Probabilistic Model",
      "description": "Family of distributions with exponential form for parameterization."
    },
    {
      "id": "Optimizers",
      "type": "major",
      "parent": "Pretraining",
      "description": "Algorithms used to minimize loss functions during training, e.g., SGD or ADAM."
    },
    {
      "id": "Gradient Descent (GD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Algorithm that iteratively updates parameters in the direction of steepest descent."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Variant of GD using a single or subset of examples for each update."
    },
    {
      "id": "Learning Rate",
      "type": "subnode",
      "parent": "Gradient Descent (GD)",
      "description": "Hyperparameter controlling the size of parameter updates in gradient descent."
    },
    {
      "id": "MiniBatchSGD",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Variant of SGD that uses a batch of samples to update parameters."
    },
    {
      "id": "Hyperparameters",
      "type": "subnode",
      "parent": "StochasticGradientDescent",
      "description": "Parameters like learning rate and number of iterations that control the optimization process."
    },
    {
      "id": "NeuralNetworks",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Introduction to neural networks in the context of machine learning problems."
    },
    {
      "id": "RegressionProblem",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Explanation and handling of regression problems using neural networks."
    },
    {
      "id": "ClassificationProblem",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Explanation and handling of classification problems using neural networks."
    },
    {
      "id": "SingleNeuronNN",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Introduction to a neural network with a single neuron for solving specific problems."
    },
    {
      "id": "HousingPricePrediction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Example of using a single neuron NN for predicting housing prices based on house size."
    },
    {
      "id": "ReLUActivationFunction",
      "type": "subnode",
      "parent": "SingleNeuronNN",
      "description": "Introduction to the ReLU activation function used in neural networks."
    },
    {
      "id": "Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Networks that simulate the human brain's structure and function."
    },
    {
      "id": "ReLU Function",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Rectified Linear Unit activation function used in neural networks."
    },
    {
      "id": "Activation Functions",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Functions used to introduce non-linearity in the network, such as ReLU."
    },
    {
      "id": "Single Neuron Model",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Model with a single neuron using an activation function."
    },
    {
      "id": "Bias Term",
      "type": "subnode",
      "parent": "Single Neuron Model",
      "description": "Error due to overly simplistic model assumptions."
    },
    {
      "id": "Weight Vector",
      "type": "subnode",
      "parent": "Single Neuron Model",
      "description": "Vector of weights assigned to input features."
    },
    {
      "id": "Stacking Neurons",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Process of combining multiple neurons to form complex networks."
    },
    {
      "id": "Housing Prediction Example",
      "type": "subnode",
      "parent": "Stacking Neurons",
      "description": "Illustration using house features like size, bedrooms, zip code, and neighborhood wealth."
    },
    {
      "id": "Family_Size",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Derived feature based on house size and number of bedrooms."
    },
    {
      "id": "Walkability",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Feature indicating how walkable a neighborhood is, derived from zip code."
    },
    {
      "id": "School_Quality",
      "type": "subnode",
      "parent": "Machine_Learning_Features",
      "description": "Quality of local elementary school predicted by combining zip code and neighborhood wealth."
    },
    {
      "id": "Neural_Network_Input",
      "type": "major",
      "parent": null,
      "description": "Set of input features for a neural network model."
    },
    {
      "id": "Hidden_Units",
      "type": "subnode",
      "parent": "Neural_Network_Input",
      "description": "Intermediate variables (a1, a2, a3) representing derived features in the neural network."
    },
    {
      "id": "ReLU_Function",
      "type": "subnode",
      "parent": "Hidden_Units",
      "description": "Activation function used for hidden units to introduce non-linearity."
    },
    {
      "id": "Output_Parameterization",
      "type": "major",
      "parent": null,
      "description": "Final output of the neural network as a linear combination of intermediate variables."
    },
    {
      "id": "NeuralNetworksParameters",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of parameters in a neural network model."
    },
    {
      "id": "BiologicalInspiration",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explanation of the inspiration from biological neural networks for artificial ones."
    },
    {
      "id": "TwoLayerNN",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Description of a two-layer fully-connected neural network with one hidden layer."
    },
    {
      "id": "FullyConnectedNeuralNetwork",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and characteristics of fully connected neural networks."
    },
    {
      "id": "IntermediateVariablesDependence",
      "type": "subnode",
      "parent": "FullyConnectedNeuralNetwork",
      "description": "Explanation that intermediate variables depend on all inputs in a fully-connected network."
    },
    {
      "id": "TwoLayerFCNN",
      "type": "subnode",
      "parent": "FullyConnectedNeuralNetwork",
      "description": "Definition of a two-layer fully connected neural network with m hidden units and d-dimensional input."
    },
    {
      "id": "VectorizationMotivation",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Introduction to vectorization in the context of implementing more complex neural networks for efficiency."
    },
    {
      "id": "VectorizationInNN",
      "type": "major",
      "parent": null,
      "description": "Process of converting loop-based code into matrix operations for efficiency."
    },
    {
      "id": "EfficiencyConcerns",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Discussion on the inefficiency of using loops in neural network implementations."
    },
    {
      "id": "ParallelismInGPUs",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Importance of GPU parallelism for deep learning performance."
    },
    {
      "id": "MatrixAlgebra",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Use of matrix algebra to speed up neural network computations."
    },
    {
      "id": "BLASOptimization",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Utilizing BLAS for optimized numerical linear algebra operations."
    },
    {
      "id": "TwoLayerNetworkExample",
      "type": "subnode",
      "parent": "VectorizationInNN",
      "description": "Illustration of vectorized two-layer fully-connected neural network."
    },
    {
      "id": "WeightMatrices",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Description of weight matrices in a neural network model."
    },
    {
      "id": "BiasVectors",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Explanation of bias vectors used in the neural network layers."
    },
    {
      "id": "ActivationFunctions",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Introduction to activation functions such as ReLU and their role in neural networks."
    },
    {
      "id": "MultiLayerNN",
      "type": "subnode",
      "parent": "NeuralNetworks",
      "description": "Overview of multi-layer fully-connected neural networks and their structure."
    },
    {
      "id": "Multi-layer Fully-Connected Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Stacking layers to create deeper neural networks."
    },
    {
      "id": "Weight Matrices and Biases",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Dimensions of weight matrices and biases for compatibility."
    },
    {
      "id": "Total Neurons and Parameters",
      "type": "subnode",
      "parent": "Multi-layer Fully-Connected Neural Networks",
      "description": "Calculating total neurons and parameters in a multi-layer network."
    },
    {
      "id": "Other Activation Functions",
      "type": "major",
      "parent": null,
      "description": "Alternative non-linear functions to ReLU such as sigmoid and tanh."
    },
    {
      "id": "TanhFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Similar to sigmoid but maps to (-1, 1); also suffers from vanishing gradient problem."
    },
    {
      "id": "ReLUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Rectified Linear Unit; popular due to its simplicity and effectiveness."
    },
    {
      "id": "LeakyReLU",
      "type": "subnode",
      "parent": "ReLUFunction",
      "description": "Variant of ReLU with a small slope for negative inputs."
    },
    {
      "id": "GELUFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Gaussian Error Linear Unit; used in NLP models like BERT and GPT."
    },
    {
      "id": "SoftplusFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Smoothed version of ReLU with a proper second-order derivative."
    },
    {
      "id": "IdentityFunction",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Linear function; not used as an activation due to lack of non-linearity."
    },
    {
      "id": "Feature_Engineering",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of selecting and transforming raw data into features for use in machine learning models."
    },
    {
      "id": "Deep_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Subfield focusing on neural networks with multiple layers to learn complex representations from data."
    },
    {
      "id": "Neural_Networks",
      "type": "subnode",
      "parent": "Deep_Learning",
      "description": "Models composed of layers of interconnected nodes to simulate the human brain's neural structure."
    },
    {
      "id": "Parameters_Beta",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Collection of parameters in a fully-connected network excluding those in the last layer."
    },
    {
      "id": "Function_Phi_Beta",
      "type": "subnode",
      "parent": "Parameters_Beta",
      "description": "Feature map function derived from input and parameters β."
    },
    {
      "id": "Model_H_Theta_Bar",
      "type": "subnode",
      "parent": "Neural_Networks",
      "description": "Linear model over the features generated by φ_β(x)."
    },
    {
      "id": "Learned_Features",
      "type": "subnode",
      "parent": "Deep_Learning",
      "description": "Features learned automatically in deep learning models, often from the penultimate layer."
    },
    {
      "id": "Deep_Learning_Features",
      "type": "major",
      "parent": null,
      "description": "Discussion on feature discovery in deep learning models."
    },
    {
      "id": "House_Price_Prediction",
      "type": "subnode",
      "parent": "Deep_Learning_Features",
      "description": "Example of using neural networks for predicting house prices without specifying intermediate features."
    },
    {
      "id": "Feature_Representation",
      "type": "subnode",
      "parent": "Deep_Learning_Features",
      "description": "Exploration of feature maps and their utility across different datasets."
    },
    {
      "id": "Black_Box_Issue",
      "type": "subnode",
      "parent": "Deep_Learning_Features",
      "description": "Complexity and interpretability issues in neural networks, often referred to as a black box."
    },
    {
      "id": "Modern_Neural_Networks_Modules",
      "type": "major",
      "parent": null,
      "description": "Introduction to various building blocks used in modern neural network architectures."
    },
    {
      "id": "MLP_Basics",
      "type": "subnode",
      "parent": "Modern_Neural_Networks_Modules",
      "description": "Definition and basic understanding of multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Matrix_Multiplication_Module",
      "type": "subnode",
      "parent": "Modern_Neural_Networks_Modules",
      "description": "Description of matrix multiplication as a fundamental building block in neural networks."
    },
    {
      "id": "Nonlinear_Activation_Blocks",
      "type": "subnode",
      "parent": "Modern_Neural_Networks_Modules",
      "description": "Explanation of nonlinear activation functions as essential components in MLP architectures."
    },
    {
      "id": "MLP_Architecture",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Multi-layer perceptron architecture in neural networks."
    },
    {
      "id": "Nonlinear_Activation_Module",
      "type": "subnode",
      "parent": "MLP_Architecture",
      "description": "Component applying nonlinear functions to inputs."
    },
    {
      "id": "Layer_Composition",
      "type": "subnode",
      "parent": "MLP_Architecture",
      "description": "Combination of matrix multiplication and activation layers."
    },
    {
      "id": "Residual_Connections",
      "type": "major",
      "parent": null,
      "description": "Technique in deep learning to alleviate vanishing gradient problem."
    },
    {
      "id": "Simplified_ResNet",
      "type": "subnode",
      "parent": "Residual_Connections",
      "description": "A composition of residual blocks and matrix multiplication layers."
    },
    {
      "id": "MachineLearningArchitecture",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning architectures including ResNet and Transformer."
    },
    {
      "id": "ResNetArchitecture",
      "type": "subnode",
      "parent": "MachineLearningArchitecture",
      "description": "Deep residual network architecture using convolution layers with batch normalization."
    },
    {
      "id": "LayerNormalization",
      "type": "subnode",
      "parent": "MachineLearningArchitecture",
      "description": "Technique used to normalize the layer inputs in deep neural networks."
    },
    {
      "id": "ConvolutionalLayers",
      "type": "subnode",
      "parent": "ResNetArchitecture",
      "description": "Layer type in neural networks used for image and sequence data processing."
    },
    {
      "id": "BatchNormalization",
      "type": "subnode",
      "parent": "ResNetArchitecture",
      "description": "Normalization technique applied after convolutions and activations in ResNet."
    },
    {
      "id": "LayerNormSubModule",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Sub-module of layer normalization denoted as LN-S."
    },
    {
      "id": "LNParameters",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Learnable parameters beta and gamma for desired mean and standard deviation in layer normalization."
    },
    {
      "id": "MachineLearning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "LN-S",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Standardized version of Layer Normalization without affine transformation."
    },
    {
      "id": "AffineTransformation",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Transformation that scales and shifts the output of LN-S."
    },
    {
      "id": "LearnableParameters",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Scalars β and γ used to adjust mean and standard deviation."
    },
    {
      "id": "ScalingInvariantProperty",
      "type": "subnode",
      "parent": "LayerNormalization",
      "description": "Property ensuring model output remains unchanged under parameter scaling."
    },
    {
      "id": "NormalizationLayers",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Techniques to normalize data in neural networks."
    },
    {
      "id": "LayerNorm",
      "type": "subnode",
      "parent": "NormalizationLayers",
      "description": "Normalization technique for individual neurons across different positions in the same layer."
    },
    {
      "id": "ScaleInvariantProperty",
      "type": "subnode",
      "parent": "NormalizationLayers",
      "description": "Property of normalization layers making them scale-invariant with respect to weights not at the last layer."
    },
    {
      "id": "OtherNormalizationTechniques",
      "type": "subnode",
      "parent": "NormalizationLayers",
      "description": "Alternative normalization methods like batch and group normalization."
    },
    {
      "id": "1DConvolution",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Simplified version of convolution layer focusing on one-dimensional data."
    },
    {
      "id": "2DConvolution",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Type of convolution used for two-dimensional image data processing."
    },
    {
      "id": "Convolutional_Neural_Networks",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Type of deep learning algorithm used primarily for image recognition."
    },
    {
      "id": "1D_Convolution",
      "type": "subnode",
      "parent": "Convolutional_Neural_Networks",
      "description": "A simplified version of convolution layer suitable for sequence data like text."
    },
    {
      "id": "Filter_Vector",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Vector used in 1-D convolution to extract features from input sequences."
    },
    {
      "id": "Bias_Scalar",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Scalar added to each output of the convolution operation."
    },
    {
      "id": "Matrix_Multiplication",
      "type": "subnode",
      "parent": "1D_Convolution",
      "description": "Operation used in Conv1D-S to transform input vectors into output vectors with shared parameters."
    },
    {
      "id": "ParameterSharing",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Explanation of parameter sharing in convolution operations."
    },
    {
      "id": "EfficiencyComparison",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Comparison of efficiency between convolution and generic matrix multiplication."
    },
    {
      "id": "ChannelConcepts",
      "type": "subnode",
      "parent": "ConvolutionalLayers",
      "description": "Explanation of input and output channels in Conv1D operations."
    },
    {
      "id": "Conv1D-S",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "One-dimensional convolutional module with specific parameters."
    },
    {
      "id": "TotalParametersConv1D",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Calculation of total number of parameters in Conv1D architecture."
    },
    {
      "id": "LinearMappingComparison",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Comparison between linear mapping and Conv1D parameter count."
    },
    {
      "id": "Conv2D-S",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Two-dimensional convolutional module with specific parameters."
    },
    {
      "id": "TotalParametersConv2D",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Calculation of total number of parameters in Conv2D architecture."
    },
    {
      "id": "Differentiable Circuit",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Composition of arithmetic operations and elementary functions in a network."
    },
    {
      "id": "Gradient Computation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of calculating gradients with respect to inputs and parameters."
    },
    {
      "id": "Loss Function Computation",
      "type": "subnode",
      "parent": "Backpropagation and Neural Networks",
      "description": "Details on how loss functions are computed in neural networks."
    },
    {
      "id": "Auto-Differentiation",
      "type": "subnode",
      "parent": "Backpropagation and Neural Networks",
      "description": "Implementation details of auto-differentiation in deep learning packages."
    },
    {
      "id": "Chain Rule",
      "type": "major",
      "parent": "Backpropagation",
      "description": "Explanation of the mathematical rule used for computing derivatives in nested functions."
    },
    {
      "id": "Backpropagation Strategy",
      "type": "subnode",
      "parent": "Chain Rule",
      "description": "General strategy and principles behind backpropagation."
    },
    {
      "id": "Basic Modules Backward Function",
      "type": "subnode",
      "parent": "Chain Rule",
      "description": "Details on computing backward functions for basic neural network modules."
    },
    {
      "id": "Backprop Algorithm MLPs",
      "type": "subnode",
      "parent": "Chain Rule",
      "description": "Concrete backpropagation algorithm for multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Preliminaries Partial Derivatives",
      "type": "major",
      "parent": null,
      "description": "Introduction to partial derivatives and their computation in machine learning contexts."
    },
    {
      "id": "Scalar Variable Dependence",
      "type": "subnode",
      "parent": "Preliminaries Partial Derivatives",
      "description": "Explanation of scalar variables depending on other variables with dimensions preserved."
    },
    {
      "id": "Partial_Derivatives",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Explanation and notation for partial derivatives in multi-variate functions."
    },
    {
      "id": "Chain_Rule",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Review of the chain rule with focus on auto-differentiation."
    },
    {
      "id": "Scalar_Functions",
      "type": "subnode",
      "parent": "Partial_Derivatives",
      "description": "Discussion on derivatives of scalar functions relative to vectors, matrices, or tensors."
    },
    {
      "id": "Mathematical_Notations",
      "type": "subnode",
      "parent": "Partial_Derivatives",
      "description": "Challenges in notation for multi-variate partial derivatives and their computation."
    },
    {
      "id": "Vectorized_Notation",
      "type": "subnode",
      "parent": "Chain_Rule",
      "description": "Use of vectorized notation to simplify the chain rule application."
    },
    {
      "id": "Chain_Rule_Applications",
      "type": "major",
      "parent": null,
      "description": "Exploration of the chain rule in machine learning contexts."
    },
    {
      "id": "Backward_Function_Linear_Map",
      "type": "subnode",
      "parent": "Chain_Rule_Applications",
      "description": "Description of how backward functions are linear maps from ∂J/∂u to ∂J/∂z."
    },
    {
      "id": "Jacobian_Matrix_Transpose",
      "type": "subnode",
      "parent": "Backward_Function_Linear_Map",
      "description": "Explanation that the matrix in equation (7.54) is the transpose of the Jacobian matrix of function g."
    },
    {
      "id": "Complexity_of_Jacobian_Matrices",
      "type": "subnode",
      "parent": "Jacobian_Matrix_Transpose",
      "description": "Discussion on complexities when z is a tensor or matrix, requiring flattening or additional notations."
    },
    {
      "id": "Equation_7.53_Usefulness",
      "type": "subnode",
      "parent": "Complexity_of_Jacobian_Matrices",
      "description": "Explanation of the convenience and effectiveness of equation (7.53) in all cases."
    },
    {
      "id": "Derivation_Section_7.4.3",
      "type": "subnode",
      "parent": "Equation_7.53_Usefulness",
      "description": "Use of equation (7.53) for derivations in Section 7.4.3."
    },
    {
      "id": "Abstract_Problem_Description",
      "type": "subnode",
      "parent": "Chain_Rule_Applications",
      "description": "Description of an abstract problem where J depends on z via u, but f is not given or complex."
    },
    {
      "id": "Loss Function Composition",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Abstract representation of loss functions as compositions of modules."
    },
    {
      "id": "Modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Building blocks such as MM, σ, Conv2D, LN used in neural networks."
    },
    {
      "id": "Binary_Classification",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Classification problem where labels are binary (0 or 1)."
    },
    {
      "id": "MultiLayer_Perceptron_(MLP)",
      "type": "subnode",
      "parent": "Binary_Classification",
      "description": "Neural network model for binary classification tasks."
    },
    {
      "id": "Loss_Function",
      "type": "subnode",
      "parent": "Binary_Classification",
      "description": "Function used to measure the performance of a model and optimize its parameters."
    },
    {
      "id": "Modules_in_MLP",
      "type": "subnode",
      "parent": "MultiLayer_Perceptron_(MLP)",
      "description": "Components such as MM and nonlinear activations in MLP."
    },
    {
      "id": "Parameters_and_Operations",
      "type": "subnode",
      "parent": "Modules_in_MLP",
      "description": "Discussion on parameters and fixed operations within modules."
    },
    {
      "id": "Intermediate_Variables",
      "type": "subnode",
      "parent": "Loss_Function",
      "description": "Variables used in the computation of loss function values."
    },
    {
      "id": "Forward_Pass",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of computing intermediate variables for backpropagation."
    },
    {
      "id": "Backward_Pass",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Calculation of gradients with respect to parameters and intermediate variables."
    },
    {
      "id": "Machine Learning Fundamentals",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning concepts and algorithms."
    },
    {
      "id": "Backpropagation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Algorithm for computing gradients in neural networks."
    },
    {
      "id": "Chain Rule Application",
      "type": "subnode",
      "parent": "Backpropagation Algorithm",
      "description": "Use of the chain rule to compute derivatives efficiently."
    },
    {
      "id": "Efficiency and Modularity",
      "type": "subnode",
      "parent": "Backpropagation Algorithm",
      "description": "Discussion on the efficiency gained from modular design in backpropagation."
    },
    {
      "id": "NeuralNetworksComposition",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Viewing neural networks as compositions of atomic operations."
    },
    {
      "id": "BackpropagationDiscussion",
      "type": "subnode",
      "parent": "NeuralNetworksComposition",
      "description": "Discusses the backpropagation process in detail."
    },
    {
      "id": "MatrixMultiplicationModule",
      "type": "subnode",
      "parent": "BackpropagationDiscussion",
      "description": "Details on computing backward functions for matrix multiplication modules."
    },
    {
      "id": "LossFunctions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Explains the computation of loss functions in neural networks."
    },
    {
      "id": "Backward Function Overview",
      "type": "major",
      "parent": null,
      "description": "Overview of backward functions in machine learning."
    },
    {
      "id": "Matrix Multiplication Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Details on the backward function for matrix multiplication operations."
    },
    {
      "id": "Vectorized Notation",
      "type": "subnode",
      "parent": "Matrix Multiplication Backward Function",
      "description": "Explanation of vectorized notation used in the backward function calculation."
    },
    {
      "id": "Efficiency Considerations",
      "type": "subnode",
      "parent": "Matrix Multiplication Backward Function",
      "description": "Discussion on computational efficiency for computing the backward function."
    },
    {
      "id": "Activation Functions Backward Function",
      "type": "subnode",
      "parent": "Backward Function Overview",
      "description": "Details on the backward function for activation functions in neural networks."
    },
    {
      "id": "BackwardFunctionEfficiency",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on the efficiency of backward functions."
    },
    {
      "id": "VectorizedBackwardFunction",
      "type": "subnode",
      "parent": "BackwardFunctionEfficiency",
      "description": "Derivation of the vectorized form of backward function."
    },
    {
      "id": "ScalarToScalarActivation",
      "type": "subnode",
      "parent": "ActivationFunctions",
      "description": "Treatment of vector-to-vector activation as scalar-to-scalar activations."
    },
    {
      "id": "LossFunctionBackwardPass",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of backward pass for different loss functions."
    },
    {
      "id": "Machine Learning Loss Functions",
      "type": "major",
      "parent": null,
      "description": "Overview of loss functions used in machine learning models."
    },
    {
      "id": "Logistic Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Loss Functions",
      "description": "Function used for binary classification problems."
    },
    {
      "id": "Cross-Entropy Loss Function",
      "type": "subnode",
      "parent": "Machine Learning Loss Functions",
      "description": "Alternative loss function to logistic loss, often used in multi-class classification."
    },
    {
      "id": "Back-propagation Algorithm",
      "type": "major",
      "parent": null,
      "description": "Method for computing gradients of the loss function with respect to parameters in neural networks."
    },
    {
      "id": "MLP Backpropagation",
      "type": "subnode",
      "parent": "Back-propagation Algorithm",
      "description": "Specific application of backpropagation to multi-layer perceptrons (MLPs)."
    },
    {
      "id": "Forward Pass",
      "type": "subnode",
      "parent": "MLP Backpropagation",
      "description": "Sequence of operations that compute the output from input in an MLP."
    },
    {
      "id": "Backward Pass",
      "type": "subnode",
      "parent": "MLP Backpropagation",
      "description": "Process of computing gradients of loss function w.r.t. parameters and activations in reverse order."
    },
    {
      "id": "Gradient Descent",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Optimization technique for minimizing cost functions in machine learning models."
    },
    {
      "id": "Vectorization over Training Examples",
      "type": "major",
      "parent": null,
      "description": "Technique to parallelize computations across multiple training examples."
    },
    {
      "id": "TrainingSetExamples",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Discussion on training examples and their representation."
    },
    {
      "id": "MatrixNotation",
      "type": "subnode",
      "parent": "MachineLearningBasics",
      "description": "Use of matrix notation in representing operations for multiple training examples."
    },
    {
      "id": "LayerActivations",
      "type": "subnode",
      "parent": "TrainingSetExamples",
      "description": "First-layer activations for each example using matrix operations."
    },
    {
      "id": "Vectorization",
      "type": "subnode",
      "parent": "MatrixNotation",
      "description": "Technique to vectorize operations for efficiency."
    },
    {
      "id": "Broadcasting",
      "type": "subnode",
      "parent": "MatrixNotation",
      "description": "Process of broadcasting constants across dimensions in matrix operations."
    },
    {
      "id": "GeneralizationToLayers",
      "type": "subnode",
      "parent": "Vectorization",
      "description": "Extending vectorized operations to multiple layers with considerations."
    },
    {
      "id": "Matricization Approach",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Approach to generalize matrix operations in deep learning layers."
    },
    {
      "id": "Implementation Subtleties",
      "type": "subnode",
      "parent": "Matricization Approach",
      "description": "Details on how data points are represented and converted between different conventions."
    },
    {
      "id": "Data Representation",
      "type": "subnode",
      "parent": "Implementation Subtleties",
      "description": "Discussion of row-major vs column-major data representation in deep learning packages."
    },
    {
      "id": "Training Loss Function",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Description of loss functions used to train machine learning models."
    },
    {
      "id": "Loss_Functions",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Functions used to evaluate the performance of a model during training."
    },
    {
      "id": "Training_Loss",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Measure of error on training data, often minimized during learning."
    },
    {
      "id": "Test_Error",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Evaluation metric for model performance on unseen test examples."
    },
    {
      "id": "Mean_Squared_Error",
      "type": "subnode",
      "parent": "Training_Loss",
      "description": "Common loss function measuring average squared difference between predictions and actual values."
    },
    {
      "id": "Empirical_Distribution",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Distribution based on training data, used to approximate population distribution."
    },
    {
      "id": "Population_Distribution",
      "type": "subnode",
      "parent": "Loss_Functions",
      "description": "Theoretical distribution representing the entire population of possible examples."
    },
    {
      "id": "Training_Data_Set",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Set of data used to train a machine learning model."
    },
    {
      "id": "Test_Data_Set",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Unseen set of data used for evaluating the performance of trained models."
    },
    {
      "id": "Training_Test_Distributions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the distribution of training and test datasets."
    },
    {
      "id": "Domain_Shift",
      "type": "subnode",
      "parent": "Training_Test_Distributions",
      "description": "Scenario where training and test distributions differ."
    },
    {
      "id": "Test_Error_Training_Error",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Relationship between test error and training error, including overfitting and underfitting concepts."
    },
    {
      "id": "Generalization_Gap",
      "type": "subnode",
      "parent": "Test_Error_Training_Error",
      "description": "Difference between the test error and training error."
    },
    {
      "id": "Overfitting_Underfitting",
      "type": "subnode",
      "parent": "Test_Error_Training_Error",
      "description": "Conditions leading to overfitting or underfitting of models."
    },
    {
      "id": "Bias_Variance_Tradoff",
      "type": "major",
      "parent": null,
      "description": "Analysis of how model parameters affect test error through bias and variance terms."
    },
    {
      "id": "Bias-Variance Tradeoff",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Trade-off between model complexity and error due to overfitting or underfitting."
    },
    {
      "id": "Double Descent Phenomenon",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Phenomenon where test error decreases then increases and decreases again with model complexity or sample size."
    },
    {
      "id": "Training and Test Datasets",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Illustration of the difference between training and test datasets using a quadratic function example."
    },
    {
      "id": "Linear Regression Models",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Comparison between simple linear models and more complex polynomial models in fitting data."
    },
    {
      "id": "Example Linear Model Fit",
      "type": "subnode",
      "parent": "Linear Regression Models",
      "description": "Illustration of a fitted linear model that underfits the training dataset due to high bias."
    },
    {
      "id": "MachineLearningChallenges",
      "type": "major",
      "parent": null,
      "description": "Overview of challenges in machine learning model fitting."
    },
    {
      "id": "LinearModelLimitations",
      "type": "subnode",
      "parent": "MachineLearningChallenges",
      "description": "Issues with linear models and their inability to capture data structure."
    },
    {
      "id": "BiasDefinition",
      "type": "subnode",
      "parent": "LinearModelLimitations",
      "description": "Difference between true function and expected model output, squared to form bias term."
    },
    {
      "id": "UnderfittingExample",
      "type": "subnode",
      "parent": "LinearModelLimitations",
      "description": "Illustration of underfitting with linear models."
    },
    {
      "id": "PolynomialModelFitting",
      "type": "subnode",
      "parent": "MachineLearningChallenges",
      "description": "Issues and successes in fitting polynomial models to data."
    },
    {
      "id": "HighDegreePolynomialLimitations",
      "type": "subnode",
      "parent": "PolynomialModelFitting",
      "description": "Problems with high-degree polynomials failing to generalize well."
    },
    {
      "id": "GeneralizationFailure",
      "type": "subnode",
      "parent": "HighDegreePolynomialLimitations",
      "description": "Explanation of why a 5th degree polynomial fails on test data despite fitting training data well."
    },
    {
      "id": "BiasVsVarianceTradeoff",
      "type": "subnode",
      "parent": "MachineLearningChallenges",
      "description": "Introduction to the concept of balancing bias and variance in model selection."
    },
    {
      "id": "Polynomial Degree",
      "type": "subnode",
      "parent": "Overfitting",
      "description": "Degree of polynomial affects fitting and overfitting risk."
    },
    {
      "id": "Variance",
      "type": "subnode",
      "parent": "Overfitting",
      "description": "Model sensitivity to training data variations leading to high test error."
    },
    {
      "id": "Model Complexity",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Degree of flexibility or capacity of a model to fit data."
    },
    {
      "id": "Test Error Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "The test error can be broken down into bias and variance components."
    },
    {
      "id": "Training Dataset",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Set of data used to train a machine learning model."
    },
    {
      "id": "Test Example",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Single instance used for evaluating the performance of trained models."
    },
    {
      "id": "MSE Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Breaking down Mean Squared Error into bias and variance components."
    },
    {
      "id": "Claim 8.1.1",
      "type": "subnode",
      "parent": "MSE Decomposition",
      "description": "Mathematical claim about the expectation of squared sums of independent random variables."
    },
    {
      "id": "Average Model",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Theoretical model obtained by averaging over all possible training sets."
    },
    {
      "id": "BiasVarianceTradeoff",
      "type": "subnode",
      "parent": "MachineLearning",
      "description": "Conceptual framework for understanding model performance."
    },
    {
      "id": "MSEDecomposition",
      "type": "subnode",
      "parent": "BiasVarianceTradeoff",
      "description": "Breaking down mean squared error into bias and variance components."
    },
    {
      "id": "AverageModel",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Theoretical model representing the average prediction over an infinite number of datasets."
    },
    {
      "id": "VarianceDefinition",
      "type": "subnode",
      "parent": "MSEDecomposition",
      "description": "Variability of the model predictions around their average value, representing variance term."
    },
    {
      "id": "Variance Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Model sensitivity to training data randomness."
    },
    {
      "id": "Model-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Peak in test error occurs when model complexity is high relative to data size."
    },
    {
      "id": "Sample-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Peak in test error occurs at n approximately equal to d, sample-specific."
    },
    {
      "id": "Overparameterized Models",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Models with more parameters than necessary, showing reduced test errors after initial increase."
    },
    {
      "id": "Historical Context",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Discovery and popularization of the double descent phenomenon by various researchers."
    },
    {
      "id": "Explanation and Mitigation Strategy",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Understanding and addressing strategies for the double descent effect."
    },
    {
      "id": "Optimal Regularization",
      "type": "subnode",
      "parent": "Sample-wise Double Descent",
      "description": "Improving test error by tuning regularization parameters effectively."
    },
    {
      "id": "Implicit Regularization",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Effect of optimizers like gradient descent in overparameterized models."
    },
    {
      "id": "Overparameterized Models Generalization",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Explanation for why overparameterized models generalize well despite fitting data perfectly."
    },
    {
      "id": "Gradient_Descent_Optimizer",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Optimizer used to minimize loss functions in linear models."
    },
    {
      "id": "Minimum_Norm_Solution",
      "type": "subnode",
      "parent": "Gradient_Descent_Optimizer",
      "description": "Solution found by gradient descent with zero initialization, characterized by minimum norm."
    },
    {
      "id": "Overparameterized_Regime",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Scenario where the number of parameters exceeds the number of training samples."
    },
    {
      "id": "Model_Complexity_Measurements",
      "type": "subnode",
      "parent": "Double_Descent_Phenomenon",
      "description": "Different ways to measure model complexity such as number of parameters or norm of the model."
    },
    {
      "id": "Norm_of_Models",
      "type": "subnode",
      "parent": "Model_Complexity_Measurements",
      "description": "Alternative complexity measure focusing on the norm rather than the number of parameters."
    },
    {
      "id": "Linear Regression Model",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Model used for predicting real-valued outputs based on input features."
    },
    {
      "id": "Sample Complexity Bounds",
      "type": "major",
      "parent": null,
      "description": "Analysis of the number of samples required to learn a model with certain accuracy."
    },
    {
      "id": "Model Selection",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Process of choosing the best model from a set of candidate models based on performance metrics."
    },
    {
      "id": "Generalization Error",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Error rate of a hypothesis when applied to unseen data, distinct from training error."
    },
    {
      "id": "Learning Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical framework for understanding the principles behind machine learning algorithms."
    },
    {
      "id": "Machine_Learning_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including concepts like generalization and sample complexity."
    },
    {
      "id": "Union_Bound",
      "type": "subnode",
      "parent": "Probability_Theory",
      "description": "A lemma stating the upper bound on probability for union of events."
    },
    {
      "id": "Hoeffding_Inequality",
      "type": "subnode",
      "parent": "Probability_Theory",
      "description": "Describes bounds on deviations from expected values in Bernoulli distributions."
    },
    {
      "id": "Chernoff_Bound",
      "type": "subnode",
      "parent": "Hoeffding_Inequality",
      "description": "Alternative name for Hoeffding inequality in learning theory context."
    },
    {
      "id": "Probability_Theory",
      "type": "major",
      "parent": null,
      "description": "Mathematical framework for analyzing random events and their probabilities."
    },
    {
      "id": "TrainingSet",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A dataset used for training a model with labeled examples."
    },
    {
      "id": "TrainingError",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "The error rate of a hypothesis on the training dataset."
    },
    {
      "id": "GeneralizationError",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "The expected error rate of a hypothesis when applied to new, unseen data drawn from the same distribution."
    },
    {
      "id": "PACAssumption",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "An assumption in learning theory that training and testing are performed on the same distribution with independently drawn examples."
    },
    {
      "id": "EmpiricalRiskMinimization",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A method for fitting model parameters by minimizing the empirical risk over a given dataset."
    },
    {
      "id": "Empirical_Risk_Minimization",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of minimizing training error to find optimal parameters for a hypothesis."
    },
    {
      "id": "Hypothesis_Class",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Set of all classifiers considered by a learning algorithm, abstracting from specific parameterization."
    },
    {
      "id": "Finite_Hypothesis_Class",
      "type": "subnode",
      "parent": "Empirical_Risk_Minimization",
      "description": "Case where the hypothesis class consists of a finite number of hypotheses."
    },
    {
      "id": "Generalization_Error_Guarantees",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Strategies for ensuring that the model performs well on unseen data."
    },
    {
      "id": "Reliability_of_Training_Error_Estimate",
      "type": "subnode",
      "parent": "Generalization_Error_Guarantees",
      "description": "Showing that training error is a reliable estimate of generalization error."
    },
    {
      "id": "Upper_Bound_on_Generalization_Error",
      "type": "subnode",
      "parent": "Generalization_Error_Guarantees",
      "description": "Establishing an upper bound on the model's generalization error."
    },
    {
      "id": "Bernoulli_Random_Variable_Z",
      "type": "subnode",
      "parent": "Reliability_of_Training_Error_Estimate",
      "description": "Random variable indicating whether a hypothesis misclassifies a randomly drawn example."
    },
    {
      "id": "Training_Error_Formula",
      "type": "subnode",
      "parent": "Reliability_of_Training_Error_Estimate",
      "description": "Formula expressing training error as the mean of Bernoulli random variables."
    },
    {
      "id": "Hoeffding_Inequality_Application",
      "type": "subnode",
      "parent": "Reliability_of_Training_Error_Estimate",
      "description": "Using Hoeffding's inequality to bound the probability that training and generalization errors differ significantly."
    },
    {
      "id": "Simultaneous_Guarantees_for_All_hypotheses",
      "type": "subnode",
      "parent": "Upper_Bound_on_Generalization_Error",
      "description": "Ensuring reliability of error estimates for all hypotheses in the hypothesis space."
    },
    {
      "id": "Uniform_Convergence",
      "type": "major",
      "parent": "Machine_Learning_Theory",
      "description": "Property ensuring that the difference between empirical and true errors is small across all hypotheses in a class."
    },
    {
      "id": "Probability_Bound",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Bound on probability of deviation between empirical and true errors."
    },
    {
      "id": "Union_Bound_Application",
      "type": "subnode",
      "parent": "Probability_Bound",
      "description": "Application of union bound to combine individual event probabilities."
    },
    {
      "id": "Quantities_of_Interest",
      "type": "subnode",
      "parent": "Uniform_Convergence",
      "description": "Three key quantities: n, γ, and probability of error."
    },
    {
      "id": "Determining_n",
      "type": "subnode",
      "parent": "Quantities_of_Interest",
      "description": "Finding minimum sample size n for given γ and δ to ensure error bounds."
    },
    {
      "id": "Generalization_Error",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Difference between the error on training data and actual performance on unseen data."
    },
    {
      "id": "Training_Error",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Error measured on the dataset used to train a model."
    },
    {
      "id": "Sample_Complexity",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "Number of training examples needed for an algorithm to achieve certain performance with high probability."
    },
    {
      "id": "Hypothesis_Class_Size",
      "type": "subnode",
      "parent": "Machine_Learning_Theory",
      "description": "The number of possible hypotheses in a given hypothesis class affects the generalization error bounds."
    },
    {
      "id": "Hypothesis Class Switching",
      "type": "major",
      "parent": null,
      "description": "Exploration of switching to a larger hypothesis class and its implications on bias and variance."
    },
    {
      "id": "Bias Decrease",
      "type": "subnode",
      "parent": "Hypothesis Class Switching",
      "description": "Switching to a larger hypothesis class decreases the bias term."
    },
    {
      "id": "Variance Increase",
      "type": "subnode",
      "parent": "Hypothesis Class Switching",
      "description": "Switching to a larger hypothesis class increases the variance term."
    },
    {
      "id": "Sample Complexity Bound",
      "type": "major",
      "parent": null,
      "description": "Derivation of sample complexity bound for finite hypothesis classes."
    },
    {
      "id": "Corollary Proof",
      "type": "subnode",
      "parent": "Sample Complexity Bound",
      "description": "Proof involving the relationship between n, gamma, delta, and k."
    },
    {
      "id": "Infinite Hypothesis Classes",
      "type": "major",
      "parent": null,
      "description": "Discussion on extending results to infinite hypothesis classes."
    },
    {
      "id": "Floating Point Representation",
      "type": "subnode",
      "parent": "Infinite Hypothesis Classes",
      "description": "Explanation of how real number parameters are represented in finite precision using floating point numbers."
    },
    {
      "id": "Floating_Point_Precision",
      "type": "subnode",
      "parent": "Hypothesis_Class_Size",
      "description": "Impact of floating point precision on hypothesis class size."
    },
    {
      "id": "Non_ERM_Algorithms",
      "type": "subnode",
      "parent": "Empirical_Risk_Minimization",
      "description": "Learning algorithms not based on empirical risk minimization and their theoretical guarantees."
    },
    {
      "id": "HypothesisClassParameterization",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on how hypothesis class can be parameterized differently but represent the same set of classifiers."
    },
    {
      "id": "LinearClassifierDefinition",
      "type": "subnode",
      "parent": "HypothesisClassParameterization",
      "description": "Definition and alternative representation of linear classifiers in d dimensions."
    },
    {
      "id": "ShatteringConcept",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of the concept of shattering a set with a hypothesis class."
    },
    {
      "id": "VCDimensionDefinition",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and explanation of Vapnik-Chervonenkis dimension for a hypothesis class."
    },
    {
      "id": "ExampleOfShattering",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Illustration with an example set of points to demonstrate shattering by linear classifiers in two dimensions."
    },
    {
      "id": "VC Dimension",
      "type": "major",
      "parent": null,
      "description": "A measure of the capacity or complexity of a hypothesis class in machine learning."
    },
    {
      "id": "Shattering",
      "type": "subnode",
      "parent": "VC Dimension",
      "description": "The ability of a hypothesis class to perfectly classify any possible labeling of a set of points."
    },
    {
      "id": "Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "A fundamental theorem in learning theory that provides bounds on the difference between empirical and true errors."
    },
    {
      "id": "Empirical Error",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "The error rate of a hypothesis based on training data."
    },
    {
      "id": "True Error",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "The actual error rate of a hypothesis when applied to the entire population."
    },
    {
      "id": "Uniform Convergence",
      "type": "major",
      "parent": null,
      "description": "A property indicating that empirical errors converge uniformly to true errors as sample size increases."
    },
    {
      "id": "Corollary on Sample Complexity",
      "type": "subnode",
      "parent": "Vapnik's Theorem",
      "description": "Relates the number of training examples needed for learning well with VC dimension and error bounds."
    },
    {
      "id": "Chapter 9 Regularization and Model Selection",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Focuses on techniques to control model complexity and prevent overfitting."
    },
    {
      "id": "Training Loss/Cost Function",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Function used to measure the performance of a machine learning model on training data."
    },
    {
      "id": "Regularizer Term",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Additional term added to the loss function to penalize overly complex models and prevent overfitting."
    },
    {
      "id": "Regularized Loss Function",
      "type": "subnode",
      "parent": "Training Loss/Cost Function",
      "description": "Combination of original loss function and regularizer, used to control model complexity."
    },
    {
      "id": "Regularization Parameter (λ)",
      "type": "subnode",
      "parent": "Regularizer Term",
      "description": "Hyperparameter that controls the trade-off between fitting the training data and keeping the model simple."
    },
    {
      "id": "Regularized Loss",
      "type": "major",
      "parent": null,
      "description": "Combination of loss function and regularizer to balance model fit and complexity."
    },
    {
      "id": "Loss Function J(θ)",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Measures how well the model fits the training data."
    },
    {
      "id": "Regularizer R(θ)",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Penalizes model complexity to prevent overfitting."
    },
    {
      "id": "Regularization Parameter λ",
      "type": "subnode",
      "parent": "Regularized Loss",
      "description": "Controls the trade-off between fitting data and reducing model complexity."
    },
    {
      "id": "L2 Regularization",
      "type": "subnode",
      "parent": "Regularizer R(θ)",
      "description": "Encourages small L2 norm of parameters, often called weight decay in deep learning."
    },
    {
      "id": "Weight Decay",
      "type": "subnode",
      "parent": "L2 Regularization",
      "description": "Adjustment to gradient descent that shrinks model weights during training."
    },
    {
      "id": "Inductive Bias",
      "type": "subnode",
      "parent": "Regularizer R(θ)",
      "description": "Imposes structural constraints on the model parameters based on prior knowledge."
    },
    {
      "id": "Regularization in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by adding a penalty for complexity."
    },
    {
      "id": "Sparsity Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Encourages model parameters to be sparse or zero."
    },
    {
      "id": "L0 Norm",
      "type": "subnode",
      "parent": "Sparsity Regularization",
      "description": "Counts the number of non-zero elements in θ, denoted by ∣θ∣_0."
    },
    {
      "id": "L1 Norm (LASSO)",
      "type": "subnode",
      "parent": "Sparsity Regularization",
      "description": "Continuous surrogate for L0 norm, promotes sparsity in parameters."
    },
    {
      "id": "Gradient Descent Incompatibility",
      "type": "subnode",
      "parent": "L0 Norm",
      "description": "Cannot optimize L0 norm directly with gradient descent due to discontinuity."
    },
    {
      "id": "L2 Norm Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizes the sum of squared parameter values, common for linear models and kernel methods."
    },
    {
      "id": "Kernel Methods Compatibility",
      "type": "subnode",
      "parent": "L2 Norm Regularization",
      "description": "More compatible with L2 norm due to feature inner product requirements."
    },
    {
      "id": "Deep Learning Regularization Techniques",
      "type": "major",
      "parent": null,
      "description": "Various methods used in deep learning models for regularization."
    },
    {
      "id": "Dropout",
      "type": "subnode",
      "parent": "Deep Learning Regularization Techniques",
      "description": "Randomly drops units from the network during training to prevent co-adaptation of neurons."
    },
    {
      "id": "Regularization in Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of regularization techniques and concepts in deep learning."
    },
    {
      "id": "Explicit Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm, and Lipschitzness regularization."
    },
    {
      "id": "Implicit Regularization Effect",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Impact of optimizers on model generalization through implicit bias or algorithmic regularization."
    },
    {
      "id": "Loss Landscape in Deep Learning",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Discussion on the existence of multiple global minima and their impact on model performance."
    },
    {
      "id": "Optimizer Impact on Generalization",
      "type": "subnode",
      "parent": "Implicit Regularization Effect",
      "description": "How different optimizers converge to different global minima affecting generalization performance."
    },
    {
      "id": "Generalization Performance",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Compares generalization performance of different optimizer settings."
    },
    {
      "id": "Learning Rate Schedules",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Analyzes the effect of learning rate schedules on model training and generalization."
    },
    {
      "id": "Initialization Parameters",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Discusses impact of initialization parameters such as learning rates and batch sizes."
    },
    {
      "id": "Flat Minima Hypothesis",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Proposes that flatter minima generalize better due to smaller curvature in loss function."
    },
    {
      "id": "Cross Validation",
      "type": "major",
      "parent": null,
      "description": "Describes model selection via cross validation for choosing among different models."
    },
    {
      "id": "Model Selection via Cross Validation",
      "type": "major",
      "parent": null,
      "description": "Process of choosing the best model from a set of models using cross validation."
    },
    {
      "id": "Polynomial Regression Model",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Regression model with polynomial features to fit data."
    },
    {
      "id": "Bias and Variance Tradeoff",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Balancing underfitting (high bias) and overfitting (high variance)."
    },
    {
      "id": "Cross Validation Technique",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Method to evaluate model performance by splitting data into subsets."
    },
    {
      "id": "Bandwidth Parameter for LWR",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Parameter in locally weighted regression affecting the influence of nearby points."
    },
    {
      "id": "Regularization Parameter C",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Penalty term in SVM to control model complexity and prevent overfitting."
    },
    {
      "id": "MachineLearningOptimization",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms as optimization problems."
    },
    {
      "id": "CrossValidation",
      "type": "major",
      "parent": null,
      "description": "Technique to estimate the generalization error of models."
    },
    {
      "id": "HoldOutCrossValidation",
      "type": "subnode",
      "parent": "CrossValidation",
      "description": "Method involving splitting data into training and validation sets for model selection."
    },
    {
      "id": "TrainingSetSplitting",
      "type": "subnode",
      "parent": "HoldOutCrossValidation",
      "description": "Process of dividing the dataset into train, validate, and test subsets."
    },
    {
      "id": "Validation Set Size",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Determines the fraction of total examples used as validation set, typically 1/4 to 1/3 or 30%."
    },
    {
      "id": "Hold Out Cross Validation",
      "type": "subnode",
      "parent": "Validation Set Size",
      "description": "Uses a fixed portion of data for validation and may retrain on full dataset."
    },
    {
      "id": "k-fold Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Divides the dataset into k subsets, uses each subset as validation set once."
    },
    {
      "id": "Machine_Learning_Challenges",
      "type": "major",
      "parent": null,
      "description": "Challenges in machine learning problems with scarce data."
    },
    {
      "id": "k_fold_cross_validation",
      "type": "subnode",
      "parent": "Machine_Learning_Challenges",
      "description": "A method to evaluate models by splitting data into k subsets and training on k-1 while testing on the remaining subset."
    },
    {
      "id": "leave_one_out_cv",
      "type": "subnode",
      "parent": "k_fold_cross_validation",
      "description": "Special case of cross validation where one example is left out for testing each time."
    },
    {
      "id": "model_evaluation",
      "type": "major",
      "parent": null,
      "description": "Process of assessing the performance of machine learning models using various techniques."
    },
    {
      "id": "Leave-One-Out CV",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Method of cross validation where one training example is held out at a time."
    },
    {
      "id": "Bayesian Statistics",
      "type": "major",
      "parent": null,
      "description": "Statistical approach treating parameters as random variables with prior distributions."
    },
    {
      "id": "MLE",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Estimation method assuming parameters are constant but unknown values."
    },
    {
      "id": "Prior Distribution",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Distribution expressing initial beliefs about parameter values before seeing data."
    },
    {
      "id": "Bayesian Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Incorporates Bayesian statistics into machine learning models."
    },
    {
      "id": "Posterior Distribution on Parameters",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Probability distribution over parameters given the data."
    },
    {
      "id": "Bayesian Logistic Regression",
      "type": "subnode",
      "parent": "Training Set",
      "description": "Logistic regression with Bayesian inference applied."
    },
    {
      "id": "Prediction on New Data",
      "type": "subnode",
      "parent": "Posterior Distribution on Parameters",
      "description": "Making predictions using the posterior distribution over parameters."
    },
    {
      "id": "Fully Bayesian Prediction",
      "type": "subnode",
      "parent": "Bayesian Machine Learning",
      "description": "Predictions based on averaging over the posterior distribution of θ."
    },
    {
      "id": "Bayesian Inference",
      "type": "major",
      "parent": null,
      "description": "Involves approximating posterior distribution for θ."
    },
    {
      "id": "Posterior Approximation",
      "type": "subnode",
      "parent": "Bayesian Inference",
      "description": "Approximates the posterior with a point estimate or MAP."
    },
    {
      "id": "MAP Estimate",
      "type": "subnode",
      "parent": "Posterior Approximation",
      "description": "Point estimate for θ using maximum a posteriori probability."
    },
    {
      "id": "MLE vs. MAP",
      "type": "subnode",
      "parent": "MAP Estimate",
      "description": "Comparison with MLE, including prior term in MAP formula."
    },
    {
      "id": "Prior Choice",
      "type": "subnode",
      "parent": "Posterior Approximation",
      "description": "Common choice of Gaussian prior for θ to avoid overfitting."
    },
    {
      "id": "Unsupervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning without labeled data, includes clustering algorithms."
    },
    {
      "id": "Clustering",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Grouping of unlabeled data into clusters."
    },
    {
      "id": "K-Means Algorithm",
      "type": "subnode",
      "parent": "Clustering",
      "description": "Iterative algorithm for clustering by minimizing distance to centroids."
    },
    {
      "id": "k-means_algorithm",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that partitions data into k clusters"
    },
    {
      "id": "distortion_function",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Measures sum of squared distances from each point to its cluster centroid"
    },
    {
      "id": "centroid_initialization",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Randomly selecting k training examples as initial centroids"
    },
    {
      "id": "inner_loop_steps",
      "type": "subnode",
      "parent": "k-means_algorithm",
      "description": "Assigning points to closest centroid and updating centroids based on mean of assigned points"
    },
    {
      "id": "k-means Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "Distortion Function J",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Function measuring the quality of clustering in k-means."
    },
    {
      "id": "Convergence Properties",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Properties related to convergence and local optima in k-means."
    },
    {
      "id": "EM Algorithms",
      "type": "major",
      "parent": "Variational Auto-Encoder (VAE)",
      "description": "Expectation-Maximization algorithms used as a basis for VAEs."
    },
    {
      "id": "Mixture of Gaussians",
      "type": "subnode",
      "parent": "EM Algorithms",
      "description": "Modeling data using a mixture of Gaussian distributions."
    },
    {
      "id": "UnsupervisedLearning",
      "type": "major",
      "parent": null,
      "description": "Setting where data points lack labels."
    },
    {
      "id": "JointDistribution",
      "type": "subnode",
      "parent": "UnsupervisedLearning",
      "description": "Modeling data with joint distribution p(x,z)."
    },
    {
      "id": "MixtureOfGaussians",
      "type": "subnode",
      "parent": "JointDistribution",
      "description": "Data generation process using multiple Gaussian distributions."
    },
    {
      "id": "LatentVariables",
      "type": "subnode",
      "parent": "MixtureOfGaussians",
      "description": "Hidden variables z that determine data distribution."
    },
    {
      "id": "ModelParameters",
      "type": "subnode",
      "parent": "JointDistribution",
      "description": "Parameters phi, mu, and Sigma defining the model."
    },
    {
      "id": "GaussianMixtureModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "A probabilistic model that assumes all data points are generated from a mixture of several Gaussian distributions with unknown parameters."
    },
    {
      "id": "EMAlgorithm",
      "type": "subnode",
      "parent": "GaussianMixtureModel",
      "description": "An iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables."
    },
    {
      "id": "EM_Algorithm",
      "type": "major",
      "parent": "Machine_Learning_Topic",
      "description": "Iterative algorithm for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models involving latent variables."
    },
    {
      "id": "E_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Estimation step where posterior distribution of latent variables is computed given observed data and current parameters."
    },
    {
      "id": "M_Step",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Maximization step where parameters are updated to maximize the expected log-likelihood found in E-step."
    },
    {
      "id": "Gaussian_Mixture_Models",
      "type": "major",
      "parent": null,
      "description": "Model for clustering data into multiple Gaussian distributions with different means and covariances."
    },
    {
      "id": "Soft_Assignments",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Probabilistic assignments of data points to clusters instead of hard assignments."
    },
    {
      "id": "K_Means_Clustering",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that assigns each data point to a single cluster based on proximity to cluster centroids."
    },
    {
      "id": "Local_Optima_Issue",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Risk of converging to suboptimal solutions due to the iterative nature of EM."
    },
    {
      "id": "Convergence_Guarantees",
      "type": "major",
      "parent": null,
      "description": "Theoretical guarantees regarding the convergence properties of the EM algorithm."
    },
    {
      "id": "Expectation_Maximization_Guarantees",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the guarantees and convergence properties of EM algorithm."
    },
    {
      "id": "Convex_Functions",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Definition and properties of convex functions used in Jensen's inequality."
    },
    {
      "id": "Theorem_Statement",
      "type": "subnode",
      "parent": "Jensens_Inequality",
      "description": "Formal statement of Jensen's inequality theorem."
    },
    {
      "id": "Jensen's Inequality",
      "type": "major",
      "parent": null,
      "description": "Inequality relating the expected value of a convex function to the function of the expected value."
    },
    {
      "id": "Convex Function",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "A mathematical concept where a line segment between any two points on its graph lies above or on the graph."
    },
    {
      "id": "Concave Function",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Opposite of convex, with a line segment below the function's graph."
    },
    {
      "id": "Expected Value (E[X])",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Average value of a random variable over its probability distribution."
    },
    {
      "id": "General EM Algorithm",
      "type": "major",
      "parent": null,
      "description": "A method for finding maximum likelihood estimates in probabilistic models with latent variables."
    },
    {
      "id": "Latent Variable Model",
      "type": "subnode",
      "parent": "General EM Algorithm",
      "description": "Model involving observable and unobservable (latent) random variables."
    },
    {
      "id": "Log-Likelihood",
      "type": "subnode",
      "parent": "General EM Algorithm",
      "description": "Function used in maximum likelihood estimation, defined as the logarithm of the likelihood function."
    },
    {
      "id": "Non_Convex_Optimization",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Difficulty of optimizing likelihood function directly due to non-convexity."
    },
    {
      "id": "Latent_Variables",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Random variables that are not observed but influence the model's parameters."
    },
    {
      "id": "Single_Example_Optimization",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Simplification of EM algorithm for a single example before extending it to multiple examples."
    },
    {
      "id": "Likelihood_Function",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Function representing the probability of observed data given model parameters, often used in maximum likelihood estimation."
    },
    {
      "id": "ProbabilityDistribution",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on probability distribution over latent variables z."
    },
    {
      "id": "JensensInequality",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Explanation of Jensen's inequality and its application in deriving a lower-bound."
    },
    {
      "id": "LogLikelihoodBound",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Derivation showing how to obtain a lower bound on the log likelihood using Q distribution."
    },
    {
      "id": "EvidenceLowerBoundELBO",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Definition and importance of ELBO in variational inference."
    },
    {
      "id": "PosteriorDistribution",
      "type": "subnode",
      "parent": "EvidenceLowerBoundELBO",
      "description": "Using posterior distribution to make ELBO tight."
    },
    {
      "id": "Log_Likelihood_Optimization",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Process of optimizing the log-likelihood function to find best model parameters."
    },
    {
      "id": "Single_Example_Case",
      "type": "subnode",
      "parent": "Log_Likelihood_Optimization",
      "description": "Optimizing log-likelihood for a single training example."
    },
    {
      "id": "Multiple_Examples_Case",
      "type": "subnode",
      "parent": "Log_Likelihood_Optimization",
      "description": "Extending optimization to multiple training examples by summing over all examples."
    },
    {
      "id": "Evidence_Lower_Bound_(ELBO)",
      "type": "major",
      "parent": null,
      "description": "A lower bound on the log-likelihood that guides parameter estimation in models with latent variables."
    },
    {
      "id": "Q_Distributions",
      "type": "subnode",
      "parent": "Evidence_Lower_Bound_(ELBO)",
      "description": "Set of distributions used to approximate posterior over latent variables for each training example."
    },
    {
      "id": "Jensen's_Inequality",
      "type": "major",
      "parent": null,
      "description": "Mathematical inequality used to prove that each iteration of EM increases log-likelihood."
    },
    {
      "id": "ELBO",
      "type": "subnode",
      "parent": "EM_Algorithm",
      "description": "Evidence Lower Bound, objective function used in variational inference to approximate posterior distribution."
    },
    {
      "id": "ELBO Interpretations",
      "type": "major",
      "parent": null,
      "description": "Different interpretations and forms of the Evidence Lower Bound (ELBO)."
    },
    {
      "id": "Formulation of ELBO",
      "type": "subnode",
      "parent": "ELBO Interpretations",
      "description": "Definition and initial formulation of ELBO."
    },
    {
      "id": "Rewritten Form",
      "type": "subnode",
      "parent": "ELBO Interpretations",
      "description": "Alternative form involving expectation and KL divergence."
    },
    {
      "id": "Marginal Distribution Independence",
      "type": "subnode",
      "parent": "ELBO Interpretations",
      "description": "Case where marginal distribution of z is independent of parameter θ."
    },
    {
      "id": "Conditional Formulation",
      "type": "subnode",
      "parent": "ELBO Interpretations",
      "description": "Form involving conditional distribution p(z|x)."
    },
    {
      "id": "Mixture of Gaussians Example",
      "type": "major",
      "parent": null,
      "description": "Application of EM algorithm to fit parameters in a mixture of Gaussian models."
    },
    {
      "id": "E-step Calculation",
      "type": "subnode",
      "parent": "Mixture of Gaussians Example",
      "description": "Calculation of posterior probabilities of latent variables given observed data and current parameter estimates."
    },
    {
      "id": "Expectation-Maximization Algorithm",
      "type": "major",
      "parent": null,
      "description": "Iterative method for finding maximum likelihood estimates of parameters in probabilistic models."
    },
    {
      "id": "M-step Maximization",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Maximizing the expected log-likelihood found in the E step as a function of the parameters."
    },
    {
      "id": "Parameter Updates",
      "type": "subnode",
      "parent": "M-step Maximization",
      "description": "Deriving update rules for model parameters such as φ, μ, and Σ based on E-step results."
    },
    {
      "id": "θ Calculation",
      "type": "subnode",
      "parent": "E-step Calculation",
      "description": "Calculation of posterior probabilities for latent variables using Bayes' theorem."
    },
    {
      "id": "ExpectationMaximizationAlgorithm",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models."
    },
    {
      "id": "MStepUpdateRule",
      "type": "subnode",
      "parent": "ExpectationMaximizationAlgorithm",
      "description": "Derivation and formula for updating parameters during the maximization step."
    },
    {
      "id": "PhiParameterUpdate",
      "type": "subnode",
      "parent": "MStepUpdateRule",
      "description": "Specific update rule for parameter phi in EM algorithm, involving Lagrangian method."
    },
    {
      "id": "LagrangianMethod",
      "type": "subnode",
      "parent": "PhiParameterUpdate",
      "description": "Mathematical technique used to handle constraints during the maximization step of EM."
    },
    {
      "id": "VariationalInference",
      "type": "major",
      "parent": "MachineLearningOverview",
      "description": "Technique for approximating probability distributions in complex models using variational methods."
    },
    {
      "id": "VariationalAutoEncoder",
      "type": "subnode",
      "parent": "VariationalInference",
      "description": "Family of algorithms that extend EM to more complex models parameterized by neural networks."
    },
    {
      "id": "Variational Auto-Encoder (VAE)",
      "type": "major",
      "parent": "Machine Learning Overview",
      "description": "Technique extending EM algorithms to complex models using neural networks."
    },
    {
      "id": "Variational Inference",
      "type": "subnode",
      "parent": "Variational Auto-Encoder (VAE)",
      "description": "Statistical technique approximating posterior distributions in Bayesian inference."
    },
    {
      "id": "Re-parametrization Trick",
      "type": "subnode",
      "parent": "Variational Auto-Encoder (VAE)",
      "description": "Technique enabling backpropagation through stochastic nodes in VAEs."
    },
    {
      "id": "High-Dimensional Latent Variables",
      "type": "subnode",
      "parent": "Variational Auto-Encoder (VAE)",
      "description": "Non-linear models dealing with high-dimensional continuous latent variables."
    },
    {
      "id": "Gaussian Mixture Models",
      "type": "major",
      "parent": null,
      "description": "Models representing data as a mixture of Gaussian distributions."
    },
    {
      "id": "Mean Field Assumption",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Assumption of independence among latent variables for simplifying calculations."
    },
    {
      "id": "Continuous Latent Variables",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Latent variables taking values in continuous spaces, requiring specific techniques for optimization."
    },
    {
      "id": "Succinct_Representation",
      "type": "subnode",
      "parent": "Gaussian_Distribution",
      "description": "Efficient representation of means for all examples using functions q and v."
    },
    {
      "id": "Variational_Autoencoder",
      "type": "subnode",
      "parent": "Latent_Variables",
      "description": "Use of neural networks as encoder (q, v) and decoder in variational autoencoders."
    },
    {
      "id": "ELBO_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Evaluation and optimization of the Evidence Lower Bound for fixed Q_i."
    },
    {
      "id": "ELBOOptimization",
      "type": "subnode",
      "parent": "VariationalInference",
      "description": "Process of optimizing the Evidence Lower Bound in variational inference."
    },
    {
      "id": "GaussianDistributions",
      "type": "subnode",
      "parent": "VariationalInference",
      "description": "Use of Gaussian distributions to model latent variables in variational inference."
    },
    {
      "id": "GradientComputation",
      "type": "subnode",
      "parent": "ELBO",
      "description": "Process of computing gradients for optimization during the ELBO maximization."
    },
    {
      "id": "ReparameterizationTrick",
      "type": "subnode",
      "parent": "GradientComputation",
      "description": "Technique to compute gradients through stochastic variables by re-expressing them in a differentiable form."
    },
    {
      "id": "Expectation_Maximization",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Techniques for estimating parameters in statistical models."
    },
    {
      "id": "Reparameterization_Trick",
      "type": "subnode",
      "parent": "Expectation_Maximization",
      "description": "Method to compute gradients through expectations involving random variables."
    },
    {
      "id": "Gradient_Ascend_Algorithm",
      "type": "subnode",
      "parent": "Expectation_Maximization",
      "description": "Optimization method for maximizing the evidence lower bound (ELBO)."
    },
    {
      "id": "Principal_Components_Analysis",
      "type": "major",
      "parent": null,
      "description": "Dimensionality reduction technique focusing on principal components of data."
    },
    {
      "id": "Data_Subspace_Identification",
      "type": "subnode",
      "parent": "Principal_Components_Analysis",
      "description": "Identifying the subspace where data approximately lies for efficient computation."
    },
    {
      "id": "Data Redundancy Detection",
      "type": "major",
      "parent": null,
      "description": "Identifying and removing redundant attributes in data sets."
    },
    {
      "id": "PCA Algorithm Introduction",
      "type": "subnode",
      "parent": "Data Redundancy Detection",
      "description": "Introduction to Principal Component Analysis for detecting intrinsic patterns."
    },
    {
      "id": "Normalization Process",
      "type": "subnode",
      "parent": "PCA Algorithm Introduction",
      "description": "Preprocessing step before PCA, involves mean subtraction and variance scaling."
    },
    {
      "id": "Mean Subtraction",
      "type": "subnode",
      "parent": "Normalization Process",
      "description": "Subtracting the mean of each feature to center the data around zero."
    },
    {
      "id": "Variance Scaling",
      "type": "subnode",
      "parent": "Normalization Process",
      "description": "Dividing by standard deviation to ensure unit variance for comparability."
    },
    {
      "id": "Feature Correlation Example",
      "type": "subnode",
      "parent": "Data Redundancy Detection",
      "description": "Example illustrating correlation between piloting skill and enjoyment in RC helicopter pilots."
    },
    {
      "id": "Normalization",
      "type": "major",
      "parent": null,
      "description": "Process of adjusting data to a standard scale or distribution."
    },
    {
      "id": "Mean Removal",
      "type": "subnode",
      "parent": "Normalization",
      "description": "Subtracting the mean from each feature to center the data around zero."
    },
    {
      "id": "Data Rescaling",
      "type": "subnode",
      "parent": "Normalization",
      "description": "Adjusting data scale when attributes are already on a comparable scale."
    },
    {
      "id": "Major Axis of Variation",
      "type": "major",
      "parent": null,
      "description": "Direction that captures the maximum variance in the dataset."
    },
    {
      "id": "Projection Direction",
      "type": "subnode",
      "parent": "Major Axis of Variation",
      "description": "Finding a unit vector to project data for maximizing variance retention."
    },
    {
      "id": "PrincipalComponentAnalysis",
      "type": "major",
      "parent": null,
      "description": "Technique for reducing dimensionality while retaining variance."
    },
    {
      "id": "ProjectionOntoDirectionU",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Finding the projection of data points onto a unit vector u."
    },
    {
      "id": "VarianceMaximization",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Objective to maximize variance of projections in one dimension."
    },
    {
      "id": "EmpiricalCovarianceMatrix",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Matrix representing the covariance between data points."
    },
    {
      "id": "PrincipalEigenvector",
      "type": "subnode",
      "parent": "VarianceMaximization",
      "description": "Vector maximizing variance and being an eigenvector of the covariance matrix."
    },
    {
      "id": "kDimensionalSubspace",
      "type": "subnode",
      "parent": "PrincipalComponentAnalysis",
      "description": "Projection into a k-dimensional subspace using top k eigenvectors."
    },
    {
      "id": "OrthogonalBasis",
      "type": "subnode",
      "parent": "kDimensionalSubspace",
      "description": "Set of vectors forming an orthogonal basis for the data in reduced dimensions."
    },
    {
      "id": "Dimensionality Reduction",
      "type": "subnode",
      "parent": "PCA",
      "description": "Reduces the number of random variables under consideration by obtaining a set of principal variables."
    },
    {
      "id": "Eigenvectors and Eigenvalues",
      "type": "subnode",
      "parent": "PCA",
      "description": "Used to find principal components of data."
    },
    {
      "id": "Principal Components",
      "type": "subnode",
      "parent": "PCA",
      "description": "Orthogonal basis vectors that maximize variance in the data."
    },
    {
      "id": "Approximation Error Minimization",
      "type": "subnode",
      "parent": "PCA",
      "description": "Derives PCA by minimizing error from projection onto k-dimensional subspace."
    },
    {
      "id": "Data Visualization",
      "type": "subnode",
      "parent": "PCA",
      "description": "Reduces high dimensional data to 2 or 3 dimensions for visualization."
    },
    {
      "id": "Compression",
      "type": "subnode",
      "parent": "PCA",
      "description": "Represents high-dimensional data in lower dimensions for efficient storage and processing."
    },
    {
      "id": "Machine_Learning_Applications",
      "type": "major",
      "parent": null,
      "description": "Applications of machine learning techniques in data analysis and processing."
    },
    {
      "id": "Dimensionality_Reduction",
      "type": "subnode",
      "parent": "Machine_Learning_Applications",
      "description": "Techniques to reduce the number of random variables under consideration."
    },
    {
      "id": "Clustering_Cars",
      "type": "subnode",
      "parent": "PCA",
      "description": "Using PCA to identify similar car types based on features."
    },
    {
      "id": "Computational_Benefits",
      "type": "subnode",
      "parent": "Dimensionality_Reduction",
      "description": "Reduces computational complexity and helps avoid overfitting."
    },
    {
      "id": "Noise_Reduction",
      "type": "subnode",
      "parent": "PCA",
      "description": "Estimating intrinsic features from noisy data, like 'piloting karma' from noisy measures."
    },
    {
      "id": "Eigenfaces_Method",
      "type": "subnode",
      "parent": "Noise_Reduction",
      "description": "Using PCA to reduce dimensionality of face images for better matching and retrieval."
    },
    {
      "id": "Cocktail_Party_Problem",
      "type": "subnode",
      "parent": "ICA",
      "description": "Example problem illustrating ICA where multiple speakers' voices are mixed and need separation."
    },
    {
      "id": "ICA_Overview",
      "type": "subnode",
      "parent": "Machine_Learning_Topic",
      "description": "Introduction to Independent Component Analysis (ICA) in the context of signal processing."
    },
    {
      "id": "Mixing_Matrix",
      "type": "subnode",
      "parent": "ICA_Overview",
      "description": "Matrix A representing the mixing of independent sources into observed data."
    },
    {
      "id": "Unmixing_Matrix",
      "type": "subnode",
      "parent": "ICA_Overview",
      "description": "Inverse matrix W used to recover original source signals from mixed observations."
    },
    {
      "id": "ICA Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discusses inherent ambiguities in Independent Component Analysis."
    },
    {
      "id": "Permutation Matrix",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Matrix used to permute the order of elements in a vector."
    },
    {
      "id": "Scaling Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Ambiguity due to inability to determine correct scaling factors for sources."
    },
    {
      "id": "Sign Change Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Ambiguity related to indistinguishability of source signals and their negative counterparts."
    },
    {
      "id": "Scaling_Impact",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Explains the effect of scaling on ICA's recovered sources."
    },
    {
      "id": "Non_Gaussian_Sources",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Discusses conditions under which ambiguities do not affect ICA recovery."
    },
    {
      "id": "Gaussian_Data_Issue",
      "type": "subnode",
      "parent": "ICA_Ambiguities",
      "description": "Describes the problem with Gaussian data in ICA."
    },
    {
      "id": "Mixing_Matrix_Rotation",
      "type": "subnode",
      "parent": "Gaussian_Data_Issue",
      "description": "Illustrates how rotation of mixing matrix does not change results for Gaussian sources."
    },
    {
      "id": "ICAOnGaussianData",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Discussion on the limitations of ICA when applied to Gaussian data due to rotational symmetry."
    },
    {
      "id": "RotationalSymmetry",
      "type": "subnode",
      "parent": "ICAOnGaussianData",
      "description": "Explanation of how multivariate standard normal distribution's rotational symmetry affects ICA."
    },
    {
      "id": "NonGaussianDataRecovery",
      "type": "subnode",
      "parent": "ICAOnGaussianData",
      "description": "Discussion on the possibility of recovering independent sources from non-Gaussian data."
    },
    {
      "id": "DensitiesAndLinearTransformations",
      "type": "major",
      "parent": null,
      "description": "Exploration of how linear transformations affect densities in random variables."
    },
    {
      "id": "EffectOfLinearTransformations",
      "type": "subnode",
      "parent": "DensitiesAndLinearTransformations",
      "description": "Explanation on the impact of linear transformations on the density functions of random variables."
    },
    {
      "id": "CorrectFormulaForDensityTransformation",
      "type": "subnode",
      "parent": "EffectOfLinearTransformations",
      "description": "Derivation and explanation of the correct formula for transforming densities under linear transformations."
    },
    {
      "id": "Density Transformation",
      "type": "major",
      "parent": null,
      "description": "Transforming density functions under linear transformations."
    },
    {
      "id": "1D Example",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Example of 1-dimensional transformation with W = A^-1."
    },
    {
      "id": "General Case",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Extension to vector-valued distributions and general matrices."
    },
    {
      "id": "Volume Mapping",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Explanation of volume changes under linear transformations in d-dimensions."
    },
    {
      "id": "ICA Algorithm Introduction",
      "type": "major",
      "parent": null,
      "description": "Introduction to Independent Component Analysis algorithm derivation and interpretation."
    },
    {
      "id": "ICAIndependenceAssumption",
      "type": "subnode",
      "parent": "MachineLearningConcepts",
      "description": "Independent Component Analysis assumes sources are statistically independent."
    },
    {
      "id": "JointDistributionModeling",
      "type": "subnode",
      "parent": "MaximumLikelihoodEstimation",
      "description": "Modeling the joint distribution of source signals as a product of marginals."
    },
    {
      "id": "DensityFunctionForSources",
      "type": "subnode",
      "parent": "ICAIndependenceAssumption",
      "description": "Defining density functions for individual sources in ICA."
    },
    {
      "id": "CumulativeDistributionFunction",
      "type": "subnode",
      "parent": "DensityFunctionForSources",
      "description": "CDF defines the probability that a random variable is less than or equal to a given value."
    },
    {
      "id": "SigmoidFunctionChoice",
      "type": "subnode",
      "parent": "DensityFunctionForSources",
      "description": "Choosing sigmoid function as default for source density in ICA due to its properties."
    },
    {
      "id": "Data_Preprocessing",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Assumption about data preprocessing for zero mean."
    },
    {
      "id": "Probability_Density_Functions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on probability density functions and their properties."
    },
    {
      "id": "Logistic_Function",
      "type": "subnode",
      "parent": "Probability_Density_Functions",
      "description": "Properties of the logistic function in relation to zero mean data."
    },
    {
      "id": "Model_Parameters",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Introduction to model parameters such as matrix W."
    },
    {
      "id": "Log_Likelihood",
      "type": "subnode",
      "parent": "Model_Parameters",
      "description": "Expression for log likelihood in terms of the parameter matrix W."
    },
    {
      "id": "Gradient_Ascend_Rule",
      "type": "subnode",
      "parent": "Model_Parameters",
      "description": "Derivation and explanation of stochastic gradient ascent learning rule."
    },
    {
      "id": "Convergence_and_Sources_Recovery",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Steps after convergence to recover original sources from data."
    },
    {
      "id": "Independence_Assumption",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on the assumption of independence between training examples and its implications."
    },
    {
      "id": "Training Methods",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Different approaches to train models such as stochastic gradient ascent."
    },
    {
      "id": "Self-supervised Learning",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Learning from data without explicit labels, using the structure of the input itself."
    },
    {
      "id": "Foundation Models",
      "type": "major",
      "parent": "Machine Learning Models",
      "description": "Models pre-trained on large datasets and adaptable to various tasks with limited labeled data."
    },
    {
      "id": "Pretraining Phase",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Training a model on massive unlabeled data to learn good representations."
    },
    {
      "id": "Adaptation Phase",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Fine-tuning the pretrained model for specific tasks with limited labeled data."
    },
    {
      "id": "Transfer_Learning",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Technique where a model trained for one task is used as the starting point for another related task."
    },
    {
      "id": "Pretraining",
      "type": "subnode",
      "parent": "Transfer_Learning",
      "description": "Initial training phase on large, unlabeled datasets to learn general representations."
    },
    {
      "id": "Adaptation",
      "type": "subnode",
      "parent": "Transfer_Learning",
      "description": "Fine-tuning the pretrained model for a specific downstream task with labeled data."
    },
    {
      "id": "Unlabeled_Data",
      "type": "subnode",
      "parent": "Pretraining",
      "description": "Dataset used in pretraining phase, typically large and unlabeled."
    },
    {
      "id": "Labeled_Data",
      "type": "subnode",
      "parent": "Adaptation",
      "description": "Data with labels specific to the downstream task used for fine-tuning."
    },
    {
      "id": "Pretraining_Loss",
      "type": "subnode",
      "parent": "Pretraining",
      "description": "Loss function guiding the learning process during pretraining phase."
    },
    {
      "id": "Self_Supervised_Learning",
      "type": "subnode",
      "parent": "Pretraining_Loss",
      "description": "Learning method where supervision is derived from the data itself, not external labels."
    },
    {
      "id": "Downstream Task",
      "type": "major",
      "parent": null,
      "description": "A task for which a model is adapted using labeled data."
    },
    {
      "id": "Labeled Dataset",
      "type": "subnode",
      "parent": "Downstream Task",
      "description": "Dataset containing examples and their labels for the downstream task."
    },
    {
      "id": "Zero-Shot Learning",
      "type": "subnode",
      "parent": "Downstream Task",
      "description": "Scenario where no labeled data is available for adaptation."
    },
    {
      "id": "Few-Shot Learning",
      "type": "subnode",
      "parent": "Downstream Task",
      "description": "Scenario with a small number of labeled examples (1-50)."
    },
    {
      "id": "Adaptation Algorithm",
      "type": "major",
      "parent": null,
      "description": "Algorithm that adapts a pretrained model for downstream tasks."
    },
    {
      "id": "Linear Probe",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Uses a linear head on top of the representation to predict labels without changing the pretrained model."
    },
    {
      "id": "Finetuning",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Further trains both the pretrained and prediction models for adaptation."
    },
    {
      "id": "SGD",
      "type": "subnode",
      "parent": "Linear Probe",
      "description": "Stochastic gradient descent used to train parameters in linear probe approach."
    },
    {
      "id": "Downstream Task Loss",
      "type": "subnode",
      "parent": "Linear Probe",
      "description": "Loss function guiding the training of the linear head for prediction accuracy."
    },
    {
      "id": "Machine_Learning_Adaptation",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning adaptation techniques."
    },
    {
      "id": "Finetuning_Pretrained_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation",
      "description": "Process of finetuning pretrained models for specific tasks."
    },
    {
      "id": "Prediction_Model_Structure",
      "type": "subnode",
      "parent": "Finetuning_Pretrained_Models",
      "description": "Structure of the prediction model using pretrained and new parameters."
    },
    {
      "id": "Optimization_Objective",
      "type": "subnode",
      "parent": "Prediction_Model_Structure",
      "description": "Objective function for optimizing both w and theta."
    },
    {
      "id": "Pretraining_Methods",
      "type": "major",
      "parent": null,
      "description": "Introduction to pretraining methods in machine learning."
    },
    {
      "id": "Supervised_Pretraining",
      "type": "subnode",
      "parent": "Pretraining_Methods",
      "description": "Method using labeled datasets for training neural networks."
    },
    {
      "id": "Contrastive_Learning",
      "type": "subnode",
      "parent": "Pretraining_Methods",
      "description": "Learning paradigm where models are trained to distinguish similar instances from dissimilar ones."
    },
    {
      "id": "Self-Supervised Pretraining",
      "type": "major",
      "parent": null,
      "description": "Method using unlabeled data to train models."
    },
    {
      "id": "Representation Function",
      "type": "subnode",
      "parent": "Self-Supervised Pretraining",
      "description": "Function mapping images to representations."
    },
    {
      "id": "Data Augmentation",
      "type": "subnode",
      "parent": "Self-Supervised Pretraining",
      "description": "Technique for generating similar image pairs."
    },
    {
      "id": "Positive Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Augmented versions of the same original image."
    },
    {
      "id": "Negative Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Randomly selected images and their augmentations."
    },
    {
      "id": "SIMCLR",
      "type": "subnode",
      "parent": "Contrastive_Learning",
      "description": "Algorithm for unsupervised learning using contrastive loss function."
    },
    {
      "id": "Augmentation_Techniques",
      "type": "subnode",
      "parent": "SIMCLR",
      "description": "Techniques for creating multiple versions of training data to improve model robustness."
    },
    {
      "id": "Negative_Pair",
      "type": "subnode",
      "parent": "Contrastive_Learning",
      "description": "Pair of samples that are not semantically related, used in contrastive learning."
    },
    {
      "id": "Pretrained_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Models pre-trained on large datasets for better generalization."
    },
    {
      "id": "Natural_Language_Processing",
      "type": "major",
      "parent": null,
      "description": "Techniques and models for processing human language data."
    },
    {
      "id": "Language_Models",
      "type": "subnode",
      "parent": "Natural_Language_Processing",
      "description": "Models predicting the probability of sequences of words."
    },
    {
      "id": "ConditionalProbabilityModeling",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Techniques for modeling conditional probabilities in sequences."
    },
    {
      "id": "ParameterizedModels",
      "type": "subnode",
      "parent": "ConditionalProbabilityModeling",
      "description": "Introduction to parameterized models and their use in machine learning."
    },
    {
      "id": "EmbeddingsAndRepresentations",
      "type": "subnode",
      "parent": "ParameterizedModels",
      "description": "Explanation of embeddings for discrete variables like words."
    },
    {
      "id": "TransformerModel",
      "type": "subnode",
      "parent": "ParameterizedModels",
      "description": "Introduction to the Transformer model and its input-output interface."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Models that learn from data to make predictions or decisions."
    },
    {
      "id": "Transformer Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A type of deep learning model that processes sequential data."
    },
    {
      "id": "Conditional Probability",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "Probability distribution of an event given the occurrence of another event."
    },
    {
      "id": "Softmax Function",
      "type": "subnode",
      "parent": "Conditional Probability",
      "description": "Function that converts raw scores into probabilities for classification tasks."
    },
    {
      "id": "Training Process",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "Process of optimizing model parameters to minimize loss function."
    },
    {
      "id": "TextGenerationModels",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Models used for generating text based on learned patterns."
    },
    {
      "id": "TemperatureParameter",
      "type": "subnode",
      "parent": "TextGenerationModels",
      "description": "Adjusts the randomness of generated text by modifying softmax probabilities."
    },
    {
      "id": "ZeroShotLearning",
      "type": "major",
      "parent": null,
      "description": "Adapting models to new tasks without task-specific training data."
    },
    {
      "id": "InContextLearning",
      "type": "subnode",
      "parent": "ZeroShotLearning",
      "description": "Model adaptation using context from similar tasks or examples."
    },
    {
      "id": "Machine_Learning_Adaptation_Methods",
      "type": "major",
      "parent": null,
      "description": "Overview of adaptation methods in machine learning."
    },
    {
      "id": "Zero-Shot_Adaptation",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation_Methods",
      "description": "Adapting models without input-output pairs from downstream tasks."
    },
    {
      "id": "In-Context_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Adaptation_Methods",
      "description": "Learning with a few labeled examples to generate prompts for new data."
    },
    {
      "id": "Language_Model_Prediction",
      "type": "subnode",
      "parent": "Zero-Shot_Adaptation",
      "description": "Using language models to predict the next word based on given input."
    },
    {
      "id": "Prompt_Construction",
      "type": "subnode",
      "parent": "In-Context_Learning",
      "description": "Creating prompts by concatenating labeled examples and test data."
    },
    {
      "id": "Reinforcement Learning",
      "type": "major",
      "parent": null,
      "description": "Learning through interaction to maximize cumulative reward."
    },
    {
      "id": "Supervised Learning",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Type of machine learning task where a model is trained on labeled data to predict outcomes for new inputs."
    },
    {
      "id": "Reward Function",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Function that evaluates the performance of actions in an environment."
    },
    {
      "id": "States",
      "type": "subnode",
      "parent": "Markov_Decision_Processes",
      "description": "Set of all possible conditions or configurations an agent can be in."
    },
    {
      "id": "Actions",
      "type": "subnode",
      "parent": "Markov_Decision_Processes",
      "description": "Set of all possible actions an agent can take in a given state."
    },
    {
      "id": "State_Transition_Probabilities",
      "type": "subnode",
      "parent": "Markov_Decision_Processes",
      "description": "Probabilities associated with moving from one state to another based on action taken."
    },
    {
      "id": "Discount_Factor",
      "type": "subnode",
      "parent": "Markov_Decision_Processes",
      "description": "Parameter that determines the present value of future rewards in reinforcement learning."
    },
    {
      "id": "Reward_Function",
      "type": "subnode",
      "parent": "Markov_Decision_Processes",
      "description": "Function mapping states and actions to real numbers representing immediate rewards."
    },
    {
      "id": "Markov Decision Process (MDP)",
      "type": "major",
      "parent": null,
      "description": "Framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "State Transition",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Random transition from one state to another based on action taken."
    },
    {
      "id": "Action Selection",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Choosing an action in a given state according to policy."
    },
    {
      "id": "Total Payoff",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Cumulative reward over time, discounted by factor γ."
    },
    {
      "id": "Discount Factor (γ)",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Parameter that discounts future rewards relative to immediate ones."
    },
    {
      "id": "Policy",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Function mapping states to actions, guiding decision-making process."
    },
    {
      "id": "Value Function",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Function representing the expected return starting from a state and following a policy."
    },
    {
      "id": "Policy Execution",
      "type": "major",
      "parent": null,
      "description": "Describes the process of executing a policy in states and actions."
    },
    {
      "id": "Bellman Equations",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Equations used to update the value function iteratively until convergence."
    },
    {
      "id": "Immediate Reward",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Reward received immediately upon entering a state s."
    },
    {
      "id": "Expected Future Rewards",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Summation of expected discounted rewards after the first action in an MDP."
    },
    {
      "id": "Optimal Value Function",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Function that defines the optimal value at each time step."
    },
    {
      "id": "Bellman's Equation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Equation defining the optimal value function recursively."
    },
    {
      "id": "Optimal Policy",
      "type": "subnode",
      "parent": "Policy",
      "description": "Policy that maximizes the cumulative reward given the dynamics model and value function assumptions."
    },
    {
      "id": "Value Iteration",
      "type": "subnode",
      "parent": "Optimal Policy",
      "description": "Reinforcement learning algorithm that computes the value function of an MDP to find the optimal policy."
    },
    {
      "id": "Policy Iteration",
      "type": "subnode",
      "parent": "Optimal Policy",
      "description": "Algorithm for finding optimal policies in reinforcement learning by iteratively improving policy and evaluating it."
    },
    {
      "id": "Synchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Updating all state values simultaneously before applying new estimates."
    },
    {
      "id": "Asynchronous Updates",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Updating state values one at a time in some order until convergence."
    },
    {
      "id": "Value_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithm using Bellman equations to find optimal policies in reinforcement learning."
    },
    {
      "id": "Policy_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Algorithm for finding optimal policies in reinforcement learning problems."
    },
    {
      "id": "Convergence_to_V_star",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Process of value iteration converging to optimal state values."
    },
    {
      "id": "Optimal_Policy_Finding",
      "type": "subnode",
      "parent": "Value_Iteration",
      "description": "Using converged state values to determine the best policy."
    },
    {
      "id": "Policy_Evaluation",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Calculating value function for current policy."
    },
    {
      "id": "Policy_Improvement",
      "type": "subnode",
      "parent": "Policy_Iteration",
      "description": "Updating policy based on the value function."
    },
    {
      "id": "Bellman_Equations",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Equations used to solve for optimal policies in MDPs."
    },
    {
      "id": "Comparison_Value_Policy_Iteration",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Discussion on the advantages and disadvantages of value vs policy iteration."
    },
    {
      "id": "MDP_Models",
      "type": "subnode",
      "parent": "Machine_Learning_Algorithms",
      "description": "Models describing the environment and actions in reinforcement learning."
    },
    {
      "id": "Learning_Model_for_MDP",
      "type": "subnode",
      "parent": "MDP_Models",
      "description": "Process of estimating transition probabilities and rewards from data in an MDP."
    },
    {
      "id": "mdp_model_learning",
      "type": "major",
      "parent": null,
      "description": "Learning models for Markov Decision Processes (MDPs)"
    },
    {
      "id": "state_transition_probabilities_estimation",
      "type": "subnode",
      "parent": "mdp_model_learning",
      "description": "Estimating state transition probabilities in MDPs"
    },
    {
      "id": "reward_estimation",
      "type": "subnode",
      "parent": "mdp_model_learning",
      "description": "Estimating rewards based on observed outcomes"
    },
    {
      "id": "value_iteration_policy_optimization",
      "type": "subnode",
      "parent": "mdp_model_learning",
      "description": "Optimizing policies using value iteration and policy iteration"
    },
    {
      "id": "algorithm_for_unknown_probabilities",
      "type": "subnode",
      "parent": "mdp_model_learning",
      "description": "Algorithm for learning in MDPs with unknown state transition probabilities"
    },
    {
      "id": "optimization_in_value_iteration",
      "type": "subnode",
      "parent": "value_iteration_policy_optimization",
      "description": "Optimizing value iteration by initializing with previous solution"
    },
    {
      "id": "Algorithmic_Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques and algorithms used in solving machine learning problems."
    },
    {
      "id": "Discretization in MDPs",
      "type": "major",
      "parent": null,
      "description": "Process of converting continuous state space into discrete states for easier computation."
    },
    {
      "id": "Continuous-State MDP",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "Markov Decision Process with continuous state variables."
    },
    {
      "id": "Discrete-State MDP",
      "type": "subnode",
      "parent": "Discretization in MDPs",
      "description": "MDP with discrete states derived from discretizing the continuous space."
    },
    {
      "id": "Piecewise Constant Representation",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Representation where function values are constant within each segment of the domain, used in discretization methods."
    },
    {
      "id": "Curse of Dimensionality",
      "type": "major",
      "parent": null,
      "description": "Exponential increase in data volume and computational complexity with increasing number of dimensions in a dataset."
    },
    {
      "id": "Machine_Learning_Topics",
      "type": "major",
      "parent": null,
      "description": "Main topics in machine learning."
    },
    {
      "id": "State_Space_Representation",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "Methods for representing state spaces in ML problems."
    },
    {
      "id": "Grid_Cells_Method",
      "type": "subnode",
      "parent": "State_Space_Representation",
      "description": "Using grid cells to represent continuous states discretely."
    },
    {
      "id": "Curse_of_Dimensionality",
      "type": "subnode",
      "parent": "State_Space_Representation",
      "description": "Exponential growth of state space with dimensions leading to computational infeasibility."
    },
    {
      "id": "Model_or_Simulator",
      "type": "subnode",
      "parent": "Value_Function_Approximation",
      "description": "Using a model or simulator to approximate value functions."
    },
    {
      "id": "Model Creation Methods",
      "type": "major",
      "parent": null,
      "description": "Various methods to create a model for predicting state transitions in an MDP."
    },
    {
      "id": "Physics Simulation",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Using physical laws or software to simulate system behavior based on current state and action."
    },
    {
      "id": "Open Dynamics Engine",
      "type": "subnode",
      "parent": "Physics Simulation",
      "description": "Free/open-source physics simulator used for simulating mechanical systems like inverted pendulum."
    },
    {
      "id": "Learning from Data",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Inferring state transition probabilities by observing outcomes of actions taken in an MDP over multiple trials."
    },
    {
      "id": "StateTransitionModel",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Linear model to predict next state based on current state and action."
    },
    {
      "id": "DeterministicModel",
      "type": "subnode",
      "parent": "StateTransitionModel",
      "description": "Predicts exact next state given current state and action."
    },
    {
      "id": "StochasticModel",
      "type": "subnode",
      "parent": "StateTransitionModel",
      "description": "Models next state as a random function with noise term."
    },
    {
      "id": "NonLinearModels",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Use non-linear feature mappings or algorithms for predicting state transitions."
    },
    {
      "id": "NonLinearFeatureMappings",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Use of non-linear feature mappings for state and action representation."
    },
    {
      "id": "MDPSimulators",
      "type": "subnode",
      "parent": "MachineLearningModels",
      "description": "Building simulators for Markov Decision Processes."
    },
    {
      "id": "FittedValueIteration",
      "type": "major",
      "parent": null,
      "description": "Algorithm for approximating value functions in continuous state MDPs."
    },
    {
      "id": "ContinuousStateSpace",
      "type": "subnode",
      "parent": "FittedValueIteration",
      "description": "Handling problems with large continuous state spaces and smaller action sets."
    },
    {
      "id": "DiscretizationIssues",
      "type": "subnode",
      "parent": "ContinuousStateSpace",
      "description": "Discussion on challenges of discretizing state versus action spaces."
    },
    {
      "id": "ValueIterationUpdate",
      "type": "subnode",
      "parent": "FittedValueIteration",
      "description": "Process of updating value function in continuous states using integrals instead of summations."
    },
    {
      "id": "Fitted Value Iteration",
      "type": "major",
      "parent": "Supervised Learning",
      "description": "Algorithm that uses supervised learning to approximate the value function."
    },
    {
      "id": "Supervised Learning Algorithm",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Used to get V(s) close to y^(i)."
    },
    {
      "id": "Feature Mapping (phi)",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Maps state to feature vector for regression."
    },
    {
      "id": "State Sampling",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Randomly samples states from the state space S."
    },
    {
      "id": "Action Estimation (q(a))",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Estimates expected reward for each action in a given state."
    },
    {
      "id": "Reward and Future Value Calculation",
      "type": "subnode",
      "parent": "Action Estimation (q(a))",
      "description": "Calculates R(s) + gamma * V(s') for sampled states."
    },
    {
      "id": "Max Action Selection",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Selects action with highest estimated value y^(i)."
    },
    {
      "id": "Parameter Update (theta)",
      "type": "subnode",
      "parent": "Supervised Learning Algorithm",
      "description": "Updates parameters to minimize error between V(s) and y^(i)."
    },
    {
      "id": "Convergence Issues",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Does not always converge but often works well in practice."
    },
    {
      "id": "Deterministic Simulations",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Simplifies the algorithm by setting k=1 for deterministic outcomes."
    },
    {
      "id": "Policy Definition",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Defines action selection based on approximated value function."
    },
    {
      "id": "Value_Functions",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Function that estimates the long-term reward starting from a given state and action."
    },
    {
      "id": "Value_Evaluation",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Procedure to evaluate the value function given a policy."
    },
    {
      "id": "Expectation_Approximation",
      "type": "subnode",
      "parent": "Value_Functions",
      "description": "Techniques for approximating expectations in reinforcement learning algorithms."
    },
    {
      "id": "Algorithm 6",
      "type": "major",
      "parent": null,
      "description": "Variant of policy iteration using Procedure VE for intermediate updates between policies and values."
    },
    {
      "id": "VE Procedure",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Procedure used in Algorithm 6 to evaluate the value function under a given policy."
    },
    {
      "id": "Option 1 Initialization",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Initialization of V(s) to zero for all states s."
    },
    {
      "id": "Option 2 Initialization",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Uses current value function from the main algorithm as initialization."
    },
    {
      "id": "k Hyperparameter",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Controls number of iterations for updating state values before policy improvement."
    },
    {
      "id": "Policy Update",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Update the policy based on the current value function using equation (15.13)."
    },
    {
      "id": "Value Function Update",
      "type": "subnode",
      "parent": "Algorithm 6",
      "description": "Update the value function for each state using Bellman expectation equation (15.12)."
    },
    {
      "id": "Chapter_15_Summary",
      "type": "major",
      "parent": null,
      "description": "Summary of Chapter 15 on MDPs and iteration methods."
    },
    {
      "id": "k_Update_Frequency",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Discussion on optimal update frequency k in iterative procedures."
    },
    {
      "id": "Policy_Iteration_Speedup",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Explanation of how policy iteration can speed up computation with linear system solvers."
    },
    {
      "id": "Value_Iteration_Preference",
      "type": "subnode",
      "parent": "Chapter_15_Summary",
      "description": "Conditions under which value iteration is preferred over other methods."
    },
    {
      "id": "Chapter_16_LQR_DDP_LQG",
      "type": "major",
      "parent": null,
      "description": "Introduction to Chapter 16 covering LQR, DDP and LQG concepts."
    },
    {
      "id": "Optimal_Bellman_Equation",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Definition and explanation of the optimal Bellman equation for value function V^π*."
    },
    {
      "id": "Recovering_Optimal_Policy",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Method to recover the optimal policy π* from the optimal value function V^π*."
    },
    {
      "id": "General_Equations_Discussion",
      "type": "subnode",
      "parent": "Finite_Horizon_MDPs",
      "description": "Discussion on writing equations applicable for both discrete and continuous cases using expectations."
    },
    {
      "id": "ExpectationRewriting",
      "type": "major",
      "parent": null,
      "description": "Description of how expectation is rewritten in finite and continuous cases."
    },
    {
      "id": "RewardFunction",
      "type": "subnode",
      "parent": "ExpectationRewriting",
      "description": "A function that assigns a scalar value to each trajectory or state-action pair, indicating the desirability of the outcome."
    },
    {
      "id": "OptimalActionComputation",
      "type": "subnode",
      "parent": "RewardFunction",
      "description": "Formula for computing optimal action considering rewards and state transitions."
    },
    {
      "id": "FiniteHorizonMDP",
      "type": "major",
      "parent": null,
      "description": "Introduction to finite horizon Markov Decision Processes (MDPs)."
    },
    {
      "id": "TimeHorizonDefinition",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Definition of the time horizon T in a finite MDP."
    },
    {
      "id": "PayoffCalculation",
      "type": "subnode",
      "parent": "FiniteHorizonMDP",
      "description": "Explanation of how payoff is calculated in a finite horizon setting."
    },
    {
      "id": "DiscountFactorRole",
      "type": "subnode",
      "parent": "PayoffCalculation",
      "description": "Discussion on the role and necessity of the discount factor gamma in finite vs infinite MDPs."
    },
    {
      "id": "OptimalPolicyBehavior",
      "type": "major",
      "parent": null,
      "description": "Description of how optimal policies behave differently in a finite horizon setting compared to an infinite one."
    },
    {
      "id": "Non-Stationary Policies",
      "type": "major",
      "parent": null,
      "description": "Policies that change over time in finite-horizon MDPs."
    },
    {
      "id": "Policy Dynamics",
      "type": "subnode",
      "parent": "Non-Stationary Policies",
      "description": "How policies evolve and affect state transitions."
    },
    {
      "id": "Finite Horizon Setting",
      "type": "subnode",
      "parent": "Non-Stationary Policies",
      "description": "Context where the number of actions is limited, influencing policy strategy."
    },
    {
      "id": "Time Dependent Dynamics",
      "type": "major",
      "parent": null,
      "description": "Dynamics that change over time in MDPs to model real-life scenarios."
    },
    {
      "id": "Optimal_Value_Function",
      "type": "subnode",
      "parent": "Value_Functions",
      "description": "The best possible value function for a given problem setting."
    },
    {
      "id": "Bellman_Equation",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Equation defining the relationship between state values in sequential decision processes."
    },
    {
      "id": "Dynamic_Programming",
      "type": "subnode",
      "parent": "Reinforcement_Learning",
      "description": "Technique for solving complex problems by breaking them down into simpler subproblems."
    },
    {
      "id": "Bellman Operator",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Operator that maps a value function to another based on expected rewards and transitions."
    },
    {
      "id": "Geometric Convergence",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Rate at which the approximation improves with each iteration, characterized by γ^T."
    },
    {
      "id": "Continuous Setting Assumptions",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Assumes state and action spaces as continuous real vectors with linear transitions and quadratic rewards."
    },
    {
      "id": "Linear Transitions",
      "type": "subnode",
      "parent": "Continuous Setting Assumptions",
      "description": "State transition model given by s_{t+1}=A_t*s_t+B_t*a_t+w_t, where A_t and B_t are matrices, w_t is Gaussian noise."
    },
    {
      "id": "Quadratic Rewards",
      "type": "subnode",
      "parent": "Continuous Setting Assumptions",
      "description": "Reward function defined as -s_t^T*U_t*s_t-a_t^T*W_t*a_t with U_t and W_t positive definite matrices."
    },
    {
      "id": "LQRModelAssumptions",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Assumptions made for the Linear Quadratic Regulator (LQR) model in reinforcement learning."
    },
    {
      "id": "LQRAlgorithmSteps",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Two main steps of the LQR algorithm: parameter estimation and optimal policy derivation."
    },
    {
      "id": "ParameterEstimationStep1",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "First step involves estimating model parameters using transitions from an arbitrary policy and linear regression."
    },
    {
      "id": "OptimalPolicyDerivationStep2",
      "type": "subnode",
      "parent": "LQRAlgorithmSteps",
      "description": "Second step derives the optimal policy assuming known or estimated model parameters."
    },
    {
      "id": "DynamicProgrammingApplication",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "Application of dynamic programming to compute the optimal value function in LQR models."
    },
    {
      "id": "Quadratic Assumption",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Assumption that the value function is quadratic in state space."
    },
    {
      "id": "Dynamics Model",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Model describing how states evolve over time under actions."
    },
    {
      "id": "Linear Optimal Policy",
      "type": "subnode",
      "parent": "Optimal Policy",
      "description": "Resulting policy is linear in state, derived from quadratic assumption on value functions."
    },
    {
      "id": "OptimalPolicyLinearInSt",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "The optimal policy is linear in state variable st."
    },
    {
      "id": "DiscreteRicattiEquations",
      "type": "subnode",
      "parent": "OptimalPolicyLinearInSt",
      "description": "Set of equations used to find the optimal control policy for a discrete-time system."
    },
    {
      "id": "PhiTDependency",
      "type": "subnode",
      "parent": "DiscreteRicattiEquations",
      "description": "Phi_t does not depend on Psi or noise Sigma_t, simplifying calculations."
    },
    {
      "id": "NonlinearDynamicsToLQR",
      "type": "major",
      "parent": null,
      "description": "Reduction of non-linear dynamics problems to LQR framework."
    },
    {
      "id": "InvertedPendulumExample",
      "type": "subnode",
      "parent": "NonlinearDynamicsToLQR",
      "description": "Application of LQR principles to the inverted pendulum problem."
    },
    {
      "id": "Inverted_Pendulum_Model",
      "type": "subnode",
      "parent": "Dynamics_and_Control_Systems",
      "description": "Model describing the dynamics of an inverted pendulum system."
    },
    {
      "id": "State_Transitions",
      "type": "subnode",
      "parent": "Inverted_Pendulum_Model",
      "description": "Mathematical representation of state transitions in the inverted pendulum model."
    },
    {
      "id": "Linearization_of_Dynamics",
      "type": "major",
      "parent": null,
      "description": "Techniques for linearizing nonlinear dynamic systems."
    },
    {
      "id": "Taylor_Expansion_Method",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Use of Taylor expansion to approximate nonlinear dynamics with linear functions."
    },
    {
      "id": "Differential_Dynamic_Programming",
      "type": "major",
      "parent": null,
      "description": "Optimization technique for solving optimal control problems in dynamic systems."
    },
    {
      "id": "LQR_Relationship",
      "type": "subnode",
      "parent": "Linearization_of_Dynamics",
      "description": "Connection between linearized dynamics and Linear Quadratic Regulator (LQR) theory."
    },
    {
      "id": "DifferentialDynamicProgramming",
      "type": "major",
      "parent": null,
      "description": "Method used to optimize trajectories in dynamic systems."
    },
    {
      "id": "NominalTrajectoryGeneration",
      "type": "subnode",
      "parent": "DifferentialDynamicProgramming",
      "description": "Process of creating an initial trajectory approximation using a naive controller."
    },
    {
      "id": "LinearizationOfDynamics",
      "type": "subnode",
      "parent": "DifferentialDynamicProgramming",
      "description": "Technique to linearize system dynamics around each point in the trajectory."
    },
    {
      "id": "RewritingDynamics",
      "type": "subnode",
      "parent": "LinearizationOfDynamics",
      "description": "Expressing dynamics using matrices A and B for state and action transformations."
    },
    {
      "id": "RewardFunctionApproximation",
      "type": "subnode",
      "parent": "DifferentialDynamicProgramming",
      "description": "Approximating the reward function with a second-order Taylor expansion around each trajectory point."
    },
    {
      "id": "Optimization_Frameworks",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Frameworks used for optimization in machine learning problems."
    },
    {
      "id": "LQR_Framework",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "Linear Quadratic Regulator framework for finding optimal policies."
    },
    {
      "id": "Hessian_Matrix",
      "type": "subnode",
      "parent": "Optimization_Frameworks",
      "description": "Matrix of second-order partial derivatives used in optimization problems."
    },
    {
      "id": "LQG",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Gaussian framework for dealing with state uncertainty in real-world applications."
    },
    {
      "id": "Partially_Observable_MDPs",
      "type": "subnode",
      "parent": "Machine_Learning_Topics",
      "description": "MDPs with an observation layer to handle partial observability."
    },
    {
      "id": "Observation_Model",
      "type": "subnode",
      "parent": "Partially_Observable_MDPs",
      "description": "Conditional distribution of observations given the state."
    },
    {
      "id": "Belief_State",
      "type": "subnode",
      "parent": "Partially_Observable_MDPs",
      "description": "Distribution over states based on past observations."
    },
    {
      "id": "Policy_in_POMDP",
      "type": "subnode",
      "parent": "Partially_Observable_MDPs",
      "description": "Mapping belief state to actions in POMDP."
    },
    {
      "id": "LQR_Extension",
      "type": "subnode",
      "parent": "Partially_Observable_MDPs",
      "description": "Extending Linear Quadratic Regulator for partial observability."
    },
    {
      "id": "Kalman_Filter",
      "type": "subnode",
      "parent": "LQR_Extension",
      "description": "Algorithm used to efficiently compute belief state over time."
    },
    {
      "id": "Kalman Filter",
      "type": "major",
      "parent": null,
      "description": "Algorithm for efficient computation of state estimates in dynamic systems with Gaussian noise."
    },
    {
      "id": "Step 1",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Initial step to establish the system dynamics and noise characteristics."
    },
    {
      "id": "System Dynamics",
      "type": "subnode",
      "parent": "Step 1",
      "description": "Description of how state evolves over time with added Gaussian noise."
    },
    {
      "id": "Gaussian Distribution",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Joint distribution of states and observations is Gaussian, enabling efficient computation."
    },
    {
      "id": "Predict Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Computes the next state distribution given current observations."
    },
    {
      "id": "Update Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Updates the state estimate based on new measurements."
    },
    {
      "id": "Computational Efficiency",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Algorithm reduces computational complexity compared to direct computation of marginal distributions."
    },
    {
      "id": "Belief States Update",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Process of updating belief states through predict and update steps."
    },
    {
      "id": "Kalman Gain",
      "type": "subnode",
      "parent": "Update Step",
      "description": "Matrix that determines how much new measurements are trusted over previous estimates."
    },
    {
      "id": "Backward Pass (LQR Updates)",
      "type": "subnode",
      "parent": "Belief States Update",
      "description": "Refines estimates using previous computations to improve accuracy."
    },
    {
      "id": "Chapter 17 Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Introduces REINFORCE algorithm for model-free reinforcement learning."
    },
    {
      "id": "Model-Free Algorithm",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Algorithm that does not require value functions or Q-functions."
    },
    {
      "id": "Finite Horizon Case",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Assumes a finite length trajectory for simplicity."
    },
    {
      "id": "Randomized Policy",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Policy that outputs actions probabilistically based on state."
    },
    {
      "id": "Transition Probabilities and Reward Function",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Only requires sampling from transition probabilities and querying rewards, not their analytical forms."
    },
    {
      "id": "Expected Total Payoff",
      "type": "subnode",
      "parent": "Chapter 17 Policy Gradient (REINFORCE)",
      "description": "Objective function to optimize over policy parameters θ."
    },
    {
      "id": "Policy_Gradient_Methods",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Methods for optimizing policies in RL using gradients."
    },
    {
      "id": "REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Algorithm for estimating policy gradients without knowing the reward function."
    },
    {
      "id": "Gradient_Ascend_Optimization",
      "type": "subnode",
      "parent": "Machine_Learning_Optimization",
      "description": "Using gradient ascent to maximize expected return in RL."
    },
    {
      "id": "Reward_Function_Estimation",
      "type": "subnode",
      "parent": "Policy_Gradient_Methods",
      "description": "Estimating gradients of the reward function under policy distribution."
    },
    {
      "id": "Variational_Autoencoder_Analogy",
      "type": "subnode",
      "parent": "REINFORCE_Algorithm",
      "description": "Comparison with VAE for handling expectations over parameterized distributions."
    },
    {
      "id": "Policy_Gradient_Theorem",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Derivation and application of the policy gradient theorem."
    },
    {
      "id": "Expectation_Calculation",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Calculation of expectation involving policy probability."
    },
    {
      "id": "Gradient_Estimation",
      "type": "subnode",
      "parent": "Policy_Gradient_Theorem",
      "description": "Estimating the gradient using samples from a policy."
    },
    {
      "id": "Log_Probability_Calculation",
      "type": "subnode",
      "parent": "Gradient_Estimation",
      "description": "Calculation of log probability for state-action trajectories."
    },
    {
      "id": "Auto_Differentiation",
      "type": "subnode",
      "parent": "Log_Probability_Calculation",
      "description": "Use of auto-differentiation to compute gradients."
    },
    {
      "id": "Policy_Gradient_Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical underpinnings of policy gradients in reinforcement learning."
    },
    {
      "id": "Log_Probability_Gradient",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Gradient calculation for log probability with respect to parameters θ."
    },
    {
      "id": "Vanilla_REINFORCE_Algorithm",
      "type": "subnode",
      "parent": "Policy_Gradient_Theory",
      "description": "Basic algorithm that uses policy gradients for parameter updates."
    },
    {
      "id": "Trajectory_Probability_Change",
      "type": "subnode",
      "parent": "Log_Probability_Gradient",
      "description": "Direction of change in θ to increase the likelihood of a trajectory."
    },
    {
      "id": "Empirical_Estimation",
      "type": "subnode",
      "parent": "Vanilla_REINFORCE_Algorithm",
      "description": "Estimating gradients using sample trajectories for unbiased updates."
    },
    {
      "id": "ReinforcementLearning",
      "type": "subnode",
      "parent": "MachineLearningOverview",
      "description": "A type of machine learning where an agent learns to make decisions by performing actions in an environment."
    },
    {
      "id": "PolicyGradientMethods",
      "type": "subnode",
      "parent": "ReinforcementLearning",
      "description": "Techniques that optimize the policy directly, aiming to maximize expected reward."
    },
    {
      "id": "TrajectoryProbability",
      "type": "subnode",
      "parent": "PolicyGradientMethods",
      "description": "The probability of a sequence of actions and states in an environment under a given policy."
    },
    {
      "id": "ExpectationEquations",
      "type": "subnode",
      "parent": "PolicyGradientMethods",
      "description": "Mathematical equations used to calculate expected values under different policies."
    },
    {
      "id": "BellmanEquations",
      "type": "subnode",
      "parent": "ReinforcementLearning",
      "description": "A set of recursive equations that define the optimal value function for a given policy in reinforcement learning."
    },
    {
      "id": "Law of Total Expectation",
      "type": "major",
      "parent": null,
      "description": "A fundamental theorem in probability theory used to simplify complex expectations."
    },
    {
      "id": "Estimator Simplification",
      "type": "subnode",
      "parent": "Law of Total Expectation",
      "description": "Simplifying the estimator using the law of total expectation."
    },
    {
      "id": "Conditional Expectations",
      "type": "subnode",
      "parent": "Law of Total Expectation",
      "description": "Expectations conditioned on specific states and actions."
    },
    {
      "id": "Value Function Derivation",
      "type": "major",
      "parent": null,
      "description": "Deriving the gradient of a value function in reinforcement learning contexts."
    },
    {
      "id": "Estimator Variance Reduction",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Methods to reduce variance in policy gradient estimators."
    },
    {
      "id": "Baseline Function",
      "type": "subnode",
      "parent": "Estimator Variance Reduction",
      "description": "Function used to reduce the variance of policy gradient estimates."
    },
    {
      "id": "Value Function Estimation",
      "type": "subnode",
      "parent": "Baseline Function",
      "description": "Crude estimation of value functions for baseline purposes."
    },
    {
      "id": "Algorithm 7.4",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Vanilla policy gradient algorithm with baseline implementation."
    },
    {
      "id": "Machine_Learning_Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of papers related to machine learning and statistical learning theory."
    },
    {
      "id": "Double_Descent_Weak_Features",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Analysis of double descent phenomenon in machine learning models using weak features."
    },
    {
      "id": "Variational_Inference_Review",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Overview and review of variational inference techniques for statisticians."
    },
    {
      "id": "Foundation_Models_Opportunities_Risks",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Discussion on opportunities and risks associated with foundation models in AI."
    },
    {
      "id": "Few_Shot_Learning",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Research on the capability of language models to learn from few examples."
    },
    {
      "id": "Contrastive_Learning_Visual_Representations",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Framework for contrastive learning applied to visual representation tasks."
    },
    {
      "id": "BERT_Pretraining",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Introduction of BERT model for pre-training deep bidirectional transformers in language understanding."
    },
    {
      "id": "Implicit_Bias_Noise_Covariance",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Study on the implicit bias introduced by noise covariance in machine learning models."
    },
    {
      "id": "High_Dimensional_Statistics",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Discussion on surprising phenomena observed in high-dimensional statistical analysis."
    },
    {
      "id": "Chen_et_al_2020",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Paper discussing implicit bias in noise covariance for deep learning models."
    },
    {
      "id": "Hastie_et_al_2019",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Preliminary version of the paper on high-dimensional ridgeless least squares interpolation."
    },
    {
      "id": "Hastie_et_al_2022",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Published paper detailing surprises in high-dimensional ridgeless least squares interpolation."
    },
    {
      "id": "He_et_al_2016",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Paper on deep residual learning for image recognition using ResNet architecture."
    },
    {
      "id": "James_et_al_2021",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Textbook introducing statistical learning methods and their applications."
    },
    {
      "id": "Kingma_Ba_2014",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Paper presenting the Adam optimization algorithm for deep learning models."
    },
    {
      "id": "Kingma_Welling_2013",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Introduction of variational autoencoders (VAEs) and their applications."
    },
    {
      "id": "Luo_et_al_2018",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Framework for model-based deep reinforcement learning with theoretical guarantees."
    },
    {
      "id": "Mei_Montanari_2022",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Analysis of generalization error in random features regression models."
    },
    {
      "id": "Nakkiran_2019",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Study on the impact of data quantity on linear regression model performance."
    },
    {
      "id": "Nakkiran_et_al_2020",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Research on optimal regularization techniques to mitigate double descent in models."
    },
    {
      "id": "Opper_1995",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Discussion of generalization in statistical mechanics of learning systems."
    },
    {
      "id": "Opper_2000",
      "type": "subnode",
      "parent": "Machine_Learning_Papers",
      "description": "Exploration of the principles behind effective learning and generalization in complex systems."
    },
    {
      "id": "Double_Descent",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Phenomenon where model performance initially improves then worsens before improving again as complexity increases."
    },
    {
      "id": "Statistical_Mechanics_of_Learning",
      "type": "subnode",
      "parent": "Double_Descent",
      "description": "Application of statistical mechanics principles to understand learning processes in neural networks."
    },
    {
      "id": "Learning_to_Generalize",
      "type": "subnode",
      "parent": "Double_Descent",
      "description": "Study of how learning algorithms can improve their ability to generalize from specific examples to broader patterns."
    }
  ],
  "edges": [
    {
      "from": "Policy",
      "to": "Value Function",
      "relationship": "defines"
    },
    {
      "from": "LayerNormalization",
      "to": "ScalingInvariantProperty",
      "relationship": "subtopic"
    },
    {
      "from": "Corollary on Sample Complexity",
      "to": "Vapnik's Theorem",
      "relationship": "follows_from"
    },
    {
      "from": "SGD",
      "to": "Linear Probe",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous State MDPs",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ClassificationModels",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "MatrixNotation",
      "relationship": "related_to"
    },
    {
      "from": "Gradient_Estimation",
      "to": "Log_Probability_Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization_Model_Selection",
      "to": "Implicit_Regularization_Effect",
      "relationship": "subtopic"
    },
    {
      "from": "ICA_Overview",
      "to": "Mixing_Matrix",
      "relationship": "defines"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworks",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Selection",
      "relationship": "related_to"
    },
    {
      "from": "Gradient_Descent_Optimizer",
      "to": "Minimum_Norm_Solution",
      "relationship": "depends_on"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Finite_Hypothesis_Class",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Regularization Techniques",
      "to": "Weight Decay",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization_Problems",
      "to": "Lagrangian_Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.2 Logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Transition Probabilities and Reward Function",
      "to": "Chapter 17 Policy Gradient (REINFORCE)",
      "relationship": "subtopic"
    },
    {
      "from": "PhiParameterUpdate",
      "to": "LagrangianMethod",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "GDA",
      "relationship": "contains"
    },
    {
      "from": "Continuous_State_MDPs",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Complementarity",
      "to": "KKT_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Validation Set Size",
      "relationship": "has_subtopic"
    },
    {
      "from": "Evidence_Lower_Bound_(ELBO)",
      "to": "Q_Distributions",
      "relationship": "depends_on"
    },
    {
      "from": "Self-Supervised Pretraining",
      "to": "Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Unlabeled_Data",
      "to": "Pretraining",
      "relationship": "related_to"
    },
    {
      "from": "Pretraining_Loss",
      "to": "Pretraining",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Implicit Regularization Effect",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LinearRegression",
      "relationship": "contains"
    },
    {
      "from": "NormalizationLayers",
      "to": "OtherNormalizationTechniques",
      "relationship": "contains"
    },
    {
      "from": "Gaussian Mixture Models",
      "to": "EM Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Linear Constraints",
      "relationship": "contains"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Non-Convex Constraint Issue",
      "relationship": "subtopic"
    },
    {
      "from": "Spam_Filtering",
      "to": "Feature_Vector",
      "relationship": "uses"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Inverted_Pendulum_Model",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Computational Efficiency",
      "relationship": "related_to"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "Re-parametrization Trick",
      "relationship": "introduces"
    },
    {
      "from": "ReinforcementLearning",
      "to": "BellmanEquations",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Geometric Margins",
      "relationship": "has_subtopic"
    },
    {
      "from": "EfficiencyConcerns",
      "to": "VectorizationInNN",
      "relationship": "depends_on"
    },
    {
      "from": "MatrixNotation",
      "to": "Broadcasting",
      "relationship": "related_to"
    },
    {
      "from": "TextGenerationModels",
      "to": "SoftmaxFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Update",
      "to": "Algorithm 6",
      "relationship": "subtopic"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "BernoulliEventModel",
      "relationship": "contains"
    },
    {
      "from": "EMAlgorithm",
      "to": "EvidenceLowerBoundELBO",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "Logit",
      "relationship": "depends_on"
    },
    {
      "from": "ActivationFunctions",
      "to": "ScalarToScalarActivation",
      "relationship": "subtopic"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.1 The exponential family",
      "relationship": "has_subtopic"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "Variational Inference",
      "relationship": "uses"
    },
    {
      "from": "inner_loop_steps",
      "to": "k-means_algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "KernelFunctionExample2",
      "to": "FeatureMappingPhiWithC",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization_Problems",
      "to": "KKT_Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Conferences",
      "to": "NeurIPS_Paper_Submission",
      "relationship": "subtopic"
    },
    {
      "from": "Theorem_Statement",
      "to": "Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearning",
      "to": "VariationalInference",
      "relationship": "contains"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Hypothesis_Class_Size",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Regularization Techniques",
      "to": "Dropout",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Overfitting",
      "relationship": "contains"
    },
    {
      "from": "KernelsInML",
      "to": "ComputationalEfficiency",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Decision Boundaries",
      "relationship": "has_subtopic"
    },
    {
      "from": "JointDistribution",
      "to": "LatentVariables",
      "relationship": "related_to"
    },
    {
      "from": "BiologicalInspiration",
      "to": "TwoLayerNN",
      "relationship": "subtopic"
    },
    {
      "from": "Markov_Decision_Processes",
      "to": "Actions",
      "relationship": "includes"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "Binary_Classification",
      "to": "Loss_Function",
      "relationship": "related_to"
    },
    {
      "from": "WordGenerationProcess",
      "to": "ProbabilityFormula",
      "relationship": "subtopic"
    },
    {
      "from": "RewardFunction",
      "to": "OptimalActionComputation",
      "relationship": "subtopic"
    },
    {
      "from": "Prediction on New Data",
      "to": "Posterior Distribution on Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Implementation Subtleties",
      "to": "Data Representation",
      "relationship": "explains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "LQR_Framework",
      "relationship": "subtopic"
    },
    {
      "from": "He_et_al_2016",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "K_Means_Clustering",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Margins_Intuition",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "Reward_Function_Estimation",
      "relationship": "related_to"
    },
    {
      "from": "ICA_Overview",
      "to": "Unmixing_Matrix",
      "relationship": "defines"
    },
    {
      "from": "Optimization_in_Machine_Learning",
      "to": "Alpha_Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "KernelsInML",
      "to": "KernelFunctionExample2",
      "relationship": "has_subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Gaussian (LQG)",
      "relationship": "subtopic"
    },
    {
      "from": "JointLikelihood",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional Probability",
      "to": "Softmax Function",
      "relationship": "related_to"
    },
    {
      "from": "Overfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Discount Factor (γ)",
      "relationship": "includes"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "Lagrange_Multipliers",
      "relationship": "subtopic"
    },
    {
      "from": "Chain Rule",
      "to": "Backprop Algorithm MLPs",
      "relationship": "subtopic"
    },
    {
      "from": "DualFormulation",
      "to": "InnerProduct",
      "relationship": "contains"
    },
    {
      "from": "Bayesian Statistics",
      "to": "MLE",
      "relationship": "contrasts_with"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Binary_Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Feature_Engineering",
      "to": "Feature_Maps",
      "relationship": "depends_on"
    },
    {
      "from": "ParameterEstimation",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Dynamics Model",
      "to": "Optimal Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "ICA",
      "to": "Densities_and_Transformations",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Probability_Calculation",
      "to": "Auto_Differentiation",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Continuous-State MDP",
      "relationship": "depends_on"
    },
    {
      "from": "FeatureMapPhiX",
      "to": "HighDimensionalFeatures",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "VarianceMaximization",
      "to": "PrincipalEigenvector",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "BiasVectors",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Sample_Complexity_Bounds",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Kernels",
      "to": "Gaussian_Kernel",
      "relationship": "subtopic"
    },
    {
      "from": "Action Estimation (q(a))",
      "to": "Reward and Future Value Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Test Error Decomposition",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "SigmoidFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Model Selection",
      "to": "Cross Validation",
      "relationship": "depends_on"
    },
    {
      "from": "L2 Norm Regularization",
      "to": "Kernel Methods Compatibility",
      "relationship": "related_to"
    },
    {
      "from": "Primal_Problem",
      "to": "Optimal_Value_Primal",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworksParameters",
      "to": "BiologicalInspiration",
      "relationship": "related_to"
    },
    {
      "from": "Valid Kernels",
      "to": "Testing Kernel Validity",
      "relationship": "related_to"
    },
    {
      "from": "ParameterEstimationStep1",
      "to": "OptimalPolicyDerivationStep2",
      "relationship": "follows"
    },
    {
      "from": "Text_Classification",
      "to": "Naive_Bayes_Assumption",
      "relationship": "assumes"
    },
    {
      "from": "reward_estimation",
      "to": "mdp_model_learning",
      "relationship": "depends_on"
    },
    {
      "from": "Opper_2000",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Support_Vector_Machines_SVMs",
      "relationship": "contains"
    },
    {
      "from": "Option 1 Initialization",
      "to": "VE Procedure",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Error_Guarantees",
      "to": "Upper_Bound_on_Generalization_Error",
      "relationship": "subtopic"
    },
    {
      "from": "BernoulliEventModel",
      "to": "NaiveBayes",
      "relationship": "related_to"
    },
    {
      "from": "Geometric Margins",
      "to": "Decision Boundaries",
      "relationship": "depends_on"
    },
    {
      "from": "1D_Convolution",
      "to": "Bias_Scalar",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "DecisionBoundary",
      "relationship": "related_to"
    },
    {
      "from": "BernoulliDistribution",
      "to": "NaturalParameterForBernoulli",
      "relationship": "subtopic_of"
    },
    {
      "from": "HighDegreePolynomialLimitations",
      "to": "PolynomialModelFitting",
      "relationship": "subtopic"
    },
    {
      "from": "Training_Test_Distributions",
      "to": "Domain_Shift",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Bellman Equations",
      "relationship": "uses"
    },
    {
      "from": "Unsupervised_Learning",
      "to": "EM_Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "WeightsCalculation",
      "relationship": "depends_on"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "MultinomialEventModel",
      "relationship": "contains"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "Dual_Problem_Formulation",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "LNParameters",
      "relationship": "contains"
    },
    {
      "from": "EM_Algorithm",
      "to": "LogLikelihood",
      "relationship": "improves"
    },
    {
      "from": "Finetuning",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "MStepUpdateRule",
      "to": "PhiParameterUpdate",
      "relationship": "subtopic"
    },
    {
      "from": "GradientDescentUpdateRule",
      "to": "KernelTrickIntroduction",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "String_Features",
      "relationship": "has_subtopic"
    },
    {
      "from": "TwoLayerNetworkExample",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "MLP_Architecture",
      "to": "Nonlinear_Activation_Module",
      "relationship": "depends_on"
    },
    {
      "from": "Convergence_and_Sources_Recovery",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Major Axis of Variation",
      "to": "Projection Direction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "subtopic"
    },
    {
      "from": "RewritingDynamics",
      "to": "LinearizationOfDynamics",
      "relationship": "subtopic"
    },
    {
      "from": "MultiClassClassification",
      "to": "NegativeLogLikelihoodLossMulticlass",
      "relationship": "related_to"
    },
    {
      "from": "KernelsInML",
      "to": "GeneralKernels",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "depends_on"
    },
    {
      "from": "Softmax function",
      "to": "2.3 Multi-class classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Support_Vector_Machines_SVM",
      "relationship": "related_to"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Optimizer Impact on Generalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "ProjectionOntoDirectionU",
      "relationship": "depends_on"
    },
    {
      "from": "4 Generative learning algorithms",
      "to": "4.1 Gaussian discriminant analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Non_Convex_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Chain Rule Application",
      "relationship": "depends_on"
    },
    {
      "from": "Feature_Mapping",
      "to": "Kernel_Similarity_Metrics",
      "relationship": "related_to"
    },
    {
      "from": "GeometricMargin",
      "to": "MachineLearningConcepts",
      "relationship": "related_to"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Generalization Error",
      "relationship": "subtopic"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "General_Equations_Discussion",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LinearRegression",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Sample_Complexity",
      "relationship": "depends_on"
    },
    {
      "from": "Algorithm 6",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Model Selection",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Natural_Language_Processing",
      "to": "Language_Models",
      "relationship": "contains"
    },
    {
      "from": "SIMCLR",
      "to": "Loss_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Preliminaries Partial Derivatives",
      "to": "Scalar Variable Dependence",
      "relationship": "subtopic"
    },
    {
      "from": "L2 Regularization",
      "to": "Regularizer R(θ)",
      "relationship": "subtopic"
    },
    {
      "from": "MultiLayer_Perceptron_(MLP)",
      "to": "Modules_in_MLP",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDistribution",
      "to": "ExponentialFamilyDistributions",
      "relationship": "is_a"
    },
    {
      "from": "Machine_Learning_Adaptation_Methods",
      "to": "Zero-Shot_Adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LikelihoodFunction",
      "to": "ParameterEstimation",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning",
      "to": "Machine Learning Models",
      "relationship": "subtopic_of"
    },
    {
      "from": "Bias_Variance_Tradoff",
      "to": "Double_Descent_Phenomenon",
      "relationship": "contains"
    },
    {
      "from": "Partially_Observable_MDPs",
      "to": "Observation_Model",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Markov_Decision_Processes",
      "relationship": "subtopic"
    },
    {
      "from": "State_Space_Representation",
      "to": "Curse_of_Dimensionality",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConditionalProbabilityDistribution",
      "to": "DesignMatrixX",
      "relationship": "depends_on"
    },
    {
      "from": "Kernels in Machine Learning",
      "to": "Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Policy Execution",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization_Error",
      "to": "Training_Error",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "contains"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.4 Another algorithm for maximizing λ(θ)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern_Neural_Networks_Modules",
      "to": "MLP_Basics",
      "relationship": "subtopic"
    },
    {
      "from": "Test_Error_Training_Error",
      "to": "Overfitting_Underfitting",
      "relationship": "explains"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "ICA_Overview",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "FunctionalMargin",
      "relationship": "contains"
    },
    {
      "from": "Hold Out Cross Validation",
      "to": "Validation Set Size",
      "relationship": "describes_specific_case"
    },
    {
      "from": "BackpropagationDiscussion",
      "to": "MatrixMultiplicationModule",
      "relationship": "subtopic"
    },
    {
      "from": "MSE Decomposition",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "KernelsInML",
      "relationship": "contains"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "Functional_and_Geometric_Margins",
      "relationship": "subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.2 The Gaussian discriminant analysis model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Average Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Backward_Function_Linear_Map",
      "to": "Jacobian_Matrix_Transpose",
      "relationship": "depends_on"
    },
    {
      "from": "KKT_Conditions",
      "to": "Dual_Problem",
      "relationship": "satisfies"
    },
    {
      "from": "Few_Shot_Learning",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "REINFORCE Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "KernelsInML",
      "to": "Algorithm5.11",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalDistribution",
      "to": "LogisticRegression",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Training and Test Datasets",
      "relationship": "contains"
    },
    {
      "from": "Valid Kernels",
      "to": "Mercer's Theorem",
      "relationship": "depends_on"
    },
    {
      "from": "Density Transformation",
      "to": "ICA Algorithm Introduction",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "OptimalPolicyLinearInSt",
      "relationship": "contains"
    },
    {
      "from": "EM_Algorithm",
      "to": "Log_Likelihood_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Optimal Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "Inverted_Pendulum_Model",
      "to": "State_Transitions",
      "relationship": "subtopic"
    },
    {
      "from": "FunctionalMargin",
      "to": "NormalizationCondition",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Bias-Variance Tradeoff",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Loss Function Composition",
      "relationship": "subtopic"
    },
    {
      "from": "KernelsInML",
      "to": "KernelFunctionExample1",
      "relationship": "has_subtopic"
    },
    {
      "from": "k_fold_cross_validation",
      "to": "leave_one_out_cv",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Bellman Operator",
      "relationship": "depends_on"
    },
    {
      "from": "GeometricMargin",
      "to": "FunctionalMargin",
      "relationship": "related_to"
    },
    {
      "from": "LogisticRegression",
      "to": "GradientAscentRule",
      "relationship": "subtopic"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "BatchGradientDescent",
      "relationship": "compared_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Machine_Learning_Papers",
      "relationship": "depends_on"
    },
    {
      "from": "Value Iteration",
      "to": "Asynchronous Updates",
      "relationship": "subtopic_of"
    },
    {
      "from": "I Supervised learning",
      "to": "3 Generalized linear models",
      "relationship": "has_subtopic"
    },
    {
      "from": "distortion_function",
      "to": "k-means_algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Prediction_Model_Structure",
      "to": "Optimization_Objective",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Efficiency and Modularity",
      "relationship": "discusses"
    },
    {
      "from": "Backpropagation",
      "to": "Modules",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Pre-Computation Optimization",
      "to": "Efficient Inner Product Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization_Error_Guarantees",
      "to": "Reliability_of_Training_Error_Estimate",
      "relationship": "depends_on"
    },
    {
      "from": "Cross Validation",
      "to": "k-fold Cross Validation",
      "relationship": "alternative_method"
    },
    {
      "from": "GradientAscentRule",
      "to": "Logit",
      "relationship": "defines"
    },
    {
      "from": "TanhFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Zero-Shot_Adaptation",
      "to": "Language_Model_Prediction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Data Augmentation",
      "to": "Negative Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Variance",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Overparameterized Models Generalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LogLikelihood",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Union_Bound",
      "relationship": "depends_on"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "Non_Gaussian_Sources",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov_Decision_Processes",
      "to": "States",
      "relationship": "includes"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Clustering",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Single_Example_Update_Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Action Selection",
      "relationship": "includes"
    },
    {
      "from": "Floating Point Representation",
      "to": "Infinite Hypothesis Classes",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "GenerativeApproach",
      "relationship": "subtopic"
    },
    {
      "from": "Underfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "TotalParametersConv2D",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Kernels_in_SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "MLP_Architecture",
      "to": "Layer_Composition",
      "relationship": "related_to"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "Scaling_Impact",
      "relationship": "has_subtopic"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "PredictionGoal",
      "relationship": "subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Learning from Data",
      "relationship": "includes"
    },
    {
      "from": "VariationalInference",
      "to": "VariationalAutoEncoder",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Function Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Overparameterized Models",
      "relationship": "depends_on"
    },
    {
      "from": "Loss_Functions",
      "to": "Test_Error",
      "relationship": "contains"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Inner_Product_Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Nakkiran_2019",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "GradientDescentAlgorithm",
      "to": "LearningRate",
      "relationship": "depends_on"
    },
    {
      "from": "7 Deep learning",
      "to": "7.1 Supervised learning with non-linear models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Double_Descent_Weak_Features",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Option 2 Initialization",
      "to": "VE Procedure",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Linearization of Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningArchitecture",
      "to": "ResNetArchitecture",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Convolutional_Neural_Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "FittedValueIteration",
      "to": "ValueIterationUpdate",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Sparsity Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "Discretization",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "KernelsInML",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Generalization",
      "relationship": "related_to"
    },
    {
      "from": "Functional Margins",
      "to": "Decision Boundaries",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "VariationalInference",
      "relationship": "contains"
    },
    {
      "from": "ELBO",
      "to": "Variational Inference",
      "relationship": "depends_on"
    },
    {
      "from": "Adaptation",
      "to": "Transfer_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Deterministic Simulations",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVM",
      "to": "SMO_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "II Deep learning",
      "to": "7 Deep learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Transformation to Convex Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Dynamic_Programming",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Gradient_Estimation",
      "relationship": "related_to"
    },
    {
      "from": "Sample_Complexity",
      "to": "Empirical_Risk_Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Parametric Algorithms",
      "relationship": "contains"
    },
    {
      "from": "General_EM_Algorithms",
      "to": "ELBO_Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "Latent_Variables",
      "relationship": "relates_to"
    },
    {
      "from": "MLP Backpropagation",
      "to": "Forward Pass",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "StochasticGradientDescent",
      "relationship": "related_to"
    },
    {
      "from": "BernoulliDistribution",
      "to": "ExponentialFamilyDistributions",
      "relationship": "is_a"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "ELBO_Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Test_Data_Set",
      "relationship": "contains"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Ambiguity",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gaussian_Data_Issue",
      "to": "Mixing_Matrix_Rotation",
      "relationship": "has_subtopic"
    },
    {
      "from": "PolicyGradientMethods",
      "to": "RewardFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Class Priors",
      "relationship": "uses"
    },
    {
      "from": "Vanilla_REINFORCE_Algorithm",
      "to": "Empirical_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Text_Classification",
      "to": "Spam_Filtering",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LQRAlgorithmSteps",
      "relationship": "has_subtopic"
    },
    {
      "from": "Auto-Differentiation",
      "to": "Deep Learning Packages",
      "relationship": "implemented_in"
    },
    {
      "from": "Binary_Classification",
      "to": "MultiLayer_Perceptron_(MLP)",
      "relationship": "depends_on"
    },
    {
      "from": "Hastie_et_al_2022",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Matricization Approach",
      "to": "Implementation Subtleties",
      "relationship": "details_of"
    },
    {
      "from": "Few-Shot Learning",
      "to": "Downstream Task",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Empirical_Risk_Minimization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Modern_Neural_Networks",
      "to": "Vectorization_Training_Examples",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalDistributionModeling",
      "to": "HypothesisFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Density Transformation",
      "to": "1D Example",
      "relationship": "has_subtopic"
    },
    {
      "from": "1 Linear regression",
      "to": "1.3 Probabilistic interpretation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Hypothesis_Class",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianMixtureModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "VariationalInference",
      "relationship": "related_to"
    },
    {
      "from": "VE Procedure",
      "to": "Algorithm 6",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworksComposition",
      "to": "BackpropagationDiscussion",
      "relationship": "depends_on"
    },
    {
      "from": "Latent_Variables",
      "to": "Variational_Autoencoder",
      "relationship": "related_to"
    },
    {
      "from": "E_Step",
      "to": "Gaussian_Mixture_Models",
      "relationship": "subtopic"
    },
    {
      "from": "BiasVsVarianceTradeoff",
      "to": "MachineLearningChallenges",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning",
      "to": "Reinforcement_Learning",
      "relationship": "contains"
    },
    {
      "from": "Parameter_Estimation",
      "to": "Laplace_Smoothing",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "SMO_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "ParametersDefinition",
      "to": "LikelihoodFunction",
      "relationship": "related_to"
    },
    {
      "from": "Learning Theory",
      "to": "Model Selection",
      "relationship": "related_to"
    },
    {
      "from": "Large_Language_Models",
      "to": "Zero_Shot_InContext_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Implicit_Bias_Noise_Covariance",
      "to": "Machine_Learning_Papers",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning",
      "to": "Piecewise Constant Representation",
      "relationship": "depends_on"
    },
    {
      "from": "ResNetArchitecture",
      "to": "BatchNormalization",
      "relationship": "subtopic"
    },
    {
      "from": "ExpectationRewriting",
      "to": "RewardFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out CV",
      "relationship": "has_subtopic"
    },
    {
      "from": "Log_Likelihood",
      "to": "Model_Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "ICA",
      "to": "Cocktail_Party_Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Housing Example Dataset",
      "relationship": "example_of"
    },
    {
      "from": "LagrangianFunction",
      "to": "SupportVectors",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "Likelihood_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "Taylor_Expansion_Method",
      "relationship": "subtopic"
    },
    {
      "from": "Deep_Learning_Features",
      "to": "House_Price_Prediction",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ConvolutionalLayers",
      "relationship": "has_subtopic"
    },
    {
      "from": "ELBO Interpretations",
      "to": "Rewritten Form",
      "relationship": "subtopic"
    },
    {
      "from": "OptimalPolicyBehavior",
      "to": "TimeHorizonDefinition",
      "relationship": "depends_on"
    },
    {
      "from": "MLP_Architecture",
      "to": "Matrix_Multiplication_Module",
      "relationship": "depends_on"
    },
    {
      "from": "Large_Language_Models",
      "to": "Transformers_Uncovered",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Multiplication Backward Function",
      "to": "Vectorized Notation",
      "relationship": "depends_on"
    },
    {
      "from": "Quantities_of_Interest",
      "to": "Uniform_Convergence",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "Chapter 9 Regularization and Model Selection",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "CostFunction",
      "relationship": "related_to"
    },
    {
      "from": "Algorithmic_Methods",
      "to": "Value_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Inductive Bias",
      "to": "Regularizer R(θ)",
      "relationship": "related_to"
    },
    {
      "from": "Transformation to Convex Formulation",
      "to": "Scaling Constraint",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Components",
      "to": "PCA",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bayesian Classification",
      "to": "Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "EM Algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "SoftmaxFunction",
      "to": "Logits",
      "relationship": "depends_on"
    },
    {
      "from": "General EM Algorithm",
      "to": "Training Set",
      "relationship": "subtopic"
    },
    {
      "from": "NormalizationLayers",
      "to": "ScaleInvariantProperty",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimization Problem in SVM",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "NegativeLikelihoodLoss",
      "relationship": "subtopic"
    },
    {
      "from": "Self_Supervised_Learning_Foundation_Models",
      "to": "Pretraining_Adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "Non-Separable Case",
      "to": "Optimization Problem",
      "relationship": "describes"
    },
    {
      "from": "EM_Algorithms",
      "to": "Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "ResNetArchitecture",
      "to": "ConvolutionalLayers",
      "relationship": "subtopic"
    },
    {
      "from": "Stacking Neurons",
      "to": "Housing Prediction Example",
      "relationship": "related_to"
    },
    {
      "from": "Luo_et_al_2018",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "SufficientStatistic",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep_Learning",
      "to": "Learned_Features",
      "relationship": "produces"
    },
    {
      "from": "LinearRegression",
      "to": "MachineLearningOverview",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "L2 Norm Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Objective_Function_W",
      "to": "Quadratic_Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "CanonicalResponseFunction",
      "to": "CanonicalLinkFunction",
      "relationship": "defines"
    },
    {
      "from": "Prediction",
      "to": "NaiveBayesAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression Models",
      "to": "Example Linear Model Fit",
      "relationship": "contains"
    },
    {
      "from": "Matrix Multiplication Backward Function",
      "to": "Efficiency Considerations",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Pretrained_Models",
      "relationship": "contains"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Policy_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Initialization Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "HoldOutCrossValidation",
      "to": "TrainingSetSplitting",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Chain_Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Feature_Mapping",
      "to": "Linear_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "NewtonMethod",
      "to": "FisherScoring",
      "relationship": "related_to"
    },
    {
      "from": "Other Activation Functions",
      "to": "Activation Functions",
      "relationship": "related_to"
    },
    {
      "from": "Transfer_Learning",
      "to": "Machine_Learning",
      "relationship": "related_to"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "NaturalParameter",
      "relationship": "has_subtopic"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Gradient_Descent",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "FeatureSelection",
      "relationship": "subtopic"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "Optimal_Bellman_Equation",
      "relationship": "subtopic"
    },
    {
      "from": "Training Dataset",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Geometric Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Optimal Policy",
      "to": "Optimal Policy",
      "relationship": "subtopic"
    },
    {
      "from": "Estimator Variance Reduction",
      "to": "Baseline Function",
      "relationship": "depends_on"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "subtopic"
    },
    {
      "from": "MultivariateDistributions",
      "to": "CovarianceMatrixImpact",
      "relationship": "depends_on"
    },
    {
      "from": "Training Loss/Cost Function",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Efficient_Update_Strategy",
      "to": "Constraints_on_Alphas",
      "relationship": "depends_on"
    },
    {
      "from": "Kernel_Methods",
      "to": "Feature_Maps",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Conv1D-S",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Training Loss Function",
      "relationship": "contains"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "LogLikelihood",
      "relationship": "has_subtopic"
    },
    {
      "from": "James_et_al_2021",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "Deep_Learning_Features",
      "to": "Feature_Representation",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LeastSquaresRegression",
      "relationship": "depends_on"
    },
    {
      "from": "1 Linear regression",
      "to": "1.2 The normal equations",
      "relationship": "has_subtopic"
    },
    {
      "from": "SingleNeuronNN",
      "to": "HousingPricePrediction",
      "relationship": "related_to"
    },
    {
      "from": "Generalization",
      "to": "Bias_Variance_Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Dual_Problem",
      "relationship": "related_to"
    },
    {
      "from": "Kalman Filter",
      "to": "Gaussian Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Sample-wise Double Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Convexity_Conditions",
      "to": "Feasibility_Constraints",
      "relationship": "related_to"
    },
    {
      "from": "Log_Likelihood_Optimization",
      "to": "Single_Example_Case",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization Parameter λ",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "NormalEquations",
      "to": "MatrixDerivatives",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "HypothesisFunction",
      "relationship": "depends_on"
    },
    {
      "from": "True Error",
      "to": "Vapnik's Theorem",
      "relationship": "describes"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.2 General strategy of backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "ContinuousStateSpace",
      "to": "DiscretizationIssues",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "ConditionalProbabilityModeling",
      "relationship": "contains"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ZeroShotLearning",
      "relationship": "subtopic"
    },
    {
      "from": "PayoffCalculation",
      "to": "DiscountFactorRole",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "RegularizationAndNonSeparableCase",
      "relationship": "subtopic"
    },
    {
      "from": "ICA_Ambiguities",
      "to": "Gaussian_Data_Issue",
      "relationship": "has_subtopic"
    },
    {
      "from": "Initialization",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "Eigenvectors and Eigenvalues",
      "to": "PCA",
      "relationship": "subtopic"
    },
    {
      "from": "Loss_Functions",
      "to": "Training_Loss",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Decision Boundaries",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kernels in Machine Learning",
      "to": "Kernel Examples",
      "relationship": "subtopic"
    },
    {
      "from": "L0 Norm",
      "to": "Gradient Descent Incompatibility",
      "relationship": "depends_on"
    },
    {
      "from": "Training Methods",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Target Vector",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "InvertibilityAssumption",
      "to": "LinearRegression",
      "relationship": "depends_on"
    },
    {
      "from": "GLMDesignChoices",
      "to": "OrdinaryLeastSquares",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vectors",
      "to": "KKT_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Backward_Functions_BM",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Widrow_Hoff_Learning_Rule",
      "relationship": "related_to"
    },
    {
      "from": "GeneralizationFailure",
      "to": "HighDegreePolynomialLimitations",
      "relationship": "subtopic"
    },
    {
      "from": "GradientDescentAlgorithm",
      "to": "UpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalDistributionModeling",
      "to": "BernoulliDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrange Duality",
      "to": "Lagrange Multipliers",
      "relationship": "uses"
    },
    {
      "from": "State_Space_Representation",
      "to": "Grid_Cells_Method",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "subtopic"
    },
    {
      "from": "TextGenerationModels",
      "to": "TemperatureParameter",
      "relationship": "related_to"
    },
    {
      "from": "NonLinearModel",
      "to": "LeastSquareCostFunction",
      "relationship": "related_to"
    },
    {
      "from": "ReinforcementLearning",
      "to": "PolicyGradientMethods",
      "relationship": "contains"
    },
    {
      "from": "StateTransitionModel",
      "to": "StochasticModel",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO Interpretations",
      "to": "Marginal Distribution Independence",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation",
      "to": "Finetuning_Pretrained_Models",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ExampleOfShattering",
      "relationship": "example_of"
    },
    {
      "from": "Chain Rule",
      "to": "Backpropagation Strategy",
      "relationship": "subtopic"
    },
    {
      "from": "BiasDefinition",
      "to": "LinearModelLimitations",
      "relationship": "subtopic"
    },
    {
      "from": "Probability_Theory",
      "to": "Union_Bound",
      "relationship": "contains"
    },
    {
      "from": "Implicit Regularization",
      "to": "Generalization Performance",
      "relationship": "depends_on"
    },
    {
      "from": "7 Deep learning",
      "to": "7.2 Neural networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Comparison_Value_Policy_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Partial_Derivatives",
      "to": "Mathematical_Notations",
      "relationship": "depends_on"
    },
    {
      "from": "Single Neuron Model",
      "to": "Weight Vector",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Test_Error_Training_Error",
      "relationship": "contains"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.3 Backward functions for basic modules",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LossFunctions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Probe",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithms",
      "to": "General_EM_Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Contrastive_Learning",
      "to": "Negative_Pair",
      "relationship": "defines"
    },
    {
      "from": "ELBO",
      "to": "GradientComputation",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Bellman's Equation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Stacking Neurons",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Policy_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "TrainingError",
      "to": "GeneralizationError",
      "relationship": "contrasts_with"
    },
    {
      "from": "NecessaryConditions",
      "to": "SymmetryProperty",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised_Learning",
      "to": "PCA",
      "relationship": "subtopic"
    },
    {
      "from": "Loss_Function",
      "to": "Intermediate_Variables",
      "relationship": "subtopic"
    },
    {
      "from": "NecessaryConditions",
      "to": "FeatureMapping",
      "relationship": "subtopic"
    },
    {
      "from": "ErrorTermAssumption",
      "to": "LeastSquaresCostFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Training_Test_Distributions",
      "relationship": "contains"
    },
    {
      "from": "Value Function Update",
      "to": "Algorithm 6",
      "relationship": "subtopic"
    },
    {
      "from": "Uniform Convergence",
      "to": "VC Dimension",
      "relationship": "depends_on"
    },
    {
      "from": "KernelTrick",
      "to": "PhiFunction",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "TotalLossFunction",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Flat Minima Hypothesis",
      "relationship": "related_to"
    },
    {
      "from": "Markov_Decision_Processes",
      "to": "State_Transition_Probabilities",
      "relationship": "defines"
    },
    {
      "from": "Value Function",
      "to": "Non-Stationary Policies",
      "relationship": "subtopic"
    },
    {
      "from": "Convergence_Guarantees",
      "to": "EM_Algorithm",
      "relationship": "follows_up"
    },
    {
      "from": "Data Visualization",
      "to": "PCA",
      "relationship": "subtopic"
    },
    {
      "from": "MLP Backpropagation",
      "to": "Backward Pass",
      "relationship": "depends_on"
    },
    {
      "from": "LQRModelAssumptions",
      "to": "ParameterEstimationStep1",
      "relationship": "depends_on"
    },
    {
      "from": "SVM_Derivation_SMO",
      "to": "Dual_Optimization_Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Training Set",
      "to": "Posterior Distribution on Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Explanation and Mitigation Strategy",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Variance Increase",
      "to": "Hypothesis Class Switching",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "FeatureMapPhiX",
      "relationship": "subtopic"
    },
    {
      "from": "Chain_Rule_Applications",
      "to": "Backward_Function_Linear_Map",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function Approximation",
      "to": "Feature Mapping (phi)",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "BernoulliDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "FullyConnectedNeuralNetwork",
      "to": "TwoLayerFCNN",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Gradient Descent (GD)",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "ConvexQuadraticFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "AffineTransformation",
      "relationship": "subtopic"
    },
    {
      "from": "OptimalPolicyLinearInSt",
      "to": "LQRAlgorithmSteps",
      "relationship": "subtopic"
    },
    {
      "from": "Output_Parameterization",
      "to": "Hidden_Units",
      "relationship": "depends_on"
    },
    {
      "from": "Hidden_Units",
      "to": "ReLU_Function",
      "relationship": "uses"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "NaturalParameterRelation",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureSelection",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "M_Step",
      "to": "Convergence_Criteria",
      "relationship": "leads_to"
    },
    {
      "from": "ELBOOptimization",
      "to": "GradientAscent",
      "relationship": "contains"
    },
    {
      "from": "Backpropagation",
      "to": "General_Strategy_BP",
      "relationship": "subtopic"
    },
    {
      "from": "Mei_Montanari_2022",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "REINFORCE_Algorithm",
      "to": "Variational_Autoencoder_Analogy",
      "relationship": "analogy_with"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "FullyConnectedNeuralNetwork",
      "relationship": "subtopic"
    },
    {
      "from": "FeatureSelection",
      "to": "PolynomialRegression",
      "relationship": "depends_on"
    },
    {
      "from": "Determining_n",
      "to": "Quantities_of_Interest",
      "relationship": "subtopic"
    },
    {
      "from": "Contrastive_Learning",
      "to": "SIMCLR",
      "relationship": "example_of"
    },
    {
      "from": "MachineLearningModels",
      "to": "MDPSimulators",
      "relationship": "has_subtopic"
    },
    {
      "from": "k-means Algorithm",
      "to": "Distortion Function J",
      "relationship": "depends_on"
    },
    {
      "from": "IndependenceAssumption",
      "to": "LikelihoodFunction",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "NormalizationLayers",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sparsity Regularization",
      "to": "L1 Norm (LASSO)",
      "relationship": "related_to"
    },
    {
      "from": "Spam_Filtering",
      "to": "Training_Set",
      "relationship": "depends_on"
    },
    {
      "from": "MiniBatchSGD",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "EvidenceLowerBoundELBO",
      "to": "JensensInequality",
      "relationship": "depends_on"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Theta Representation",
      "relationship": "related_to"
    },
    {
      "from": "k Hyperparameter",
      "to": "Algorithm 6",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic_Function",
      "to": "Probability_Density_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "MDP_Models",
      "to": "Learning_Model_for_MDP",
      "relationship": "has_subtopic"
    },
    {
      "from": "LikelihoodFunction",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "From non-linear dynamics to LQR",
      "relationship": "subtopic"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.1 Preliminaries on partial derivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "BandwidthParameter",
      "relationship": "related_to"
    },
    {
      "from": "Back-propagation Algorithm",
      "to": "MLP Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "PoissonData",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "1D_Convolution",
      "to": "Filter_Vector",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LossFunctionBackwardPass",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Vanilla_REINFORCE_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "NecessaryConditions",
      "to": "PositiveSemiDefinite",
      "relationship": "subtopic"
    },
    {
      "from": "FullyConnectedNeuralNetwork",
      "to": "IntermediateVariablesDependence",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "VCDimensionDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Iteration",
      "to": "Policy_Evaluation",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO",
      "to": "Jensen's_Inequality",
      "relationship": "related_to"
    },
    {
      "from": "I Supervised learning",
      "to": "4 Generative learning algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearning",
      "to": "LayerNormalization",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "CanonicalResponseFunction",
      "relationship": "introduces"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Implicit Regularization",
      "relationship": "related_to"
    },
    {
      "from": "Reliability_of_Training_Error_Estimate",
      "to": "Hoeffding_Inequality_Application",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Algorithmic_Methods",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning Algorithm",
      "to": "Parameter Update (theta)",
      "relationship": "depends_on"
    },
    {
      "from": "SingleNeuronNN",
      "to": "ReLUActivationFunction",
      "relationship": "related_to"
    },
    {
      "from": "Model Creation Methods",
      "to": "Physics Simulation",
      "relationship": "includes"
    },
    {
      "from": "Value Iteration",
      "to": "Geometric Convergence",
      "relationship": "related_to"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Pre-Computation Optimization",
      "relationship": "optimizes_for"
    },
    {
      "from": "Bias Decrease",
      "to": "Hypothesis Class Switching",
      "relationship": "subtopic"
    },
    {
      "from": "LeastSquaresCostFunction",
      "to": "ProbabilisticInterpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Adaptation_Methods",
      "to": "In-Context_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "NonLinearModel",
      "to": "MeanSquaredLoss",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningBasics",
      "to": "LMSAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Partial_Derivatives",
      "to": "Scalar_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "DiscriminativeApproach",
      "to": "PerceptronAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "HypothesisClassParameterization",
      "to": "LinearClassifierDefinition",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization_Problems",
      "to": "Dual_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "TrainingSetExamples",
      "to": "LayerActivations",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "LMSUpdateRule",
      "relationship": "compares_to"
    },
    {
      "from": "Bayesian Logistic Regression",
      "to": "Training Set",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization",
      "to": "MultinomialGeneralization",
      "relationship": "enables"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Generalization_Error_Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Continuous_State_MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "MultinomialGeneralization",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "StateTransitionModel",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "M_Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Efficient_Representation_Update",
      "relationship": "subtopic"
    },
    {
      "from": "Step 1",
      "to": "System Dynamics",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised_Learning",
      "to": "Self_Supervised_Learning_Foundation_Models",
      "relationship": "subtopic"
    },
    {
      "from": "Finite Horizon Case",
      "to": "Chapter 17 Policy Gradient (REINFORCE)",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "NewtonMethod",
      "relationship": "subtopic"
    },
    {
      "from": "Normalization Process",
      "to": "Mean Subtraction",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Policy",
      "to": "Optimal Value Function",
      "relationship": "related_to"
    },
    {
      "from": "value_iteration_policy_optimization",
      "to": "mdp_model_learning",
      "relationship": "subtopic"
    },
    {
      "from": "Update Step",
      "to": "Kalman Gain",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "GeometricMargins",
      "relationship": "contains"
    },
    {
      "from": "ELBO Interpretations",
      "to": "Conditional Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "NewtonMethod",
      "to": "HessianMatrix",
      "relationship": "depends_on"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Lagrange_Duality",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "ConditionalDistributionModeling",
      "relationship": "contains"
    },
    {
      "from": "ClassificationModels",
      "to": "LogisticRegressionModel",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines",
      "to": "Non-Separable Case",
      "relationship": "has_subtopic"
    },
    {
      "from": "FunctionalMargin",
      "to": "FunctionMarginTrainingSet",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "Policy_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Expected Future Rewards",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Backward_Pass",
      "to": "Forward_Pass",
      "relationship": "related_to"
    },
    {
      "from": "PredictionUsingSupportVectors",
      "to": "SVMAlgorithm",
      "relationship": "related_to"
    },
    {
      "from": "Zero-Shot Learning",
      "to": "Downstream Task",
      "relationship": "subtopic"
    },
    {
      "from": "PosteriorDistribution",
      "to": "EvidenceLowerBoundELBO",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "optional_reading"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "UnitVectorW",
      "to": "VectorW",
      "relationship": "depends_on"
    },
    {
      "from": "StateTransitionModel",
      "to": "DeterministicModel",
      "relationship": "subtopic"
    },
    {
      "from": "Conv2D-S",
      "to": "TotalParametersConv2D",
      "relationship": "depends_on"
    },
    {
      "from": "Polynomial Degree",
      "to": "Variance",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Policy_Iteration_Speedup",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining_Methods",
      "to": "Contrastive_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "SMO Algorithm",
      "to": "Machine Learning Overview",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LikelihoodFunction",
      "relationship": "related_to"
    },
    {
      "from": "PoissonData",
      "to": "GDA",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningModels",
      "to": "NonLinearFeatureMappings",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "LinearMappingComparison",
      "relationship": "related_to"
    },
    {
      "from": "Empirical_Risk_Minimization",
      "to": "Non_ERM_Algorithms",
      "relationship": "contrasts_with"
    },
    {
      "from": "Nakkiran_et_al_2020",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised_Learning",
      "to": "Clustering_KMeans",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Primal_Problem",
      "relationship": "related_to"
    },
    {
      "from": "SigmoidFunction",
      "to": "TanhFunction",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Applications",
      "to": "Dimensionality_Reduction",
      "relationship": "depends_on"
    },
    {
      "from": "1D_Convolution",
      "to": "Matrix_Multiplication",
      "relationship": "related_to"
    },
    {
      "from": "VCDimensionDefinition",
      "to": "ExampleOfShattering",
      "relationship": "illustrates"
    },
    {
      "from": "3.2 Constructing GLMs",
      "to": "3.2.1 Ordinary least squares",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM_Algorithms",
      "to": "EM_for_Mixture_Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "Walkability",
      "relationship": "depends_on"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Optimal_Margin_Classifiers",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem",
      "to": "Lagrangian Function",
      "relationship": "uses"
    },
    {
      "from": "Dimensionality_Reduction",
      "to": "PCA",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Fitted Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Overparameterized_Regime",
      "relationship": "related_to"
    },
    {
      "from": "Partially_Observable_MDPs",
      "to": "Belief_State",
      "relationship": "depends_on"
    },
    {
      "from": "NaiveBayesAlgorithm",
      "to": "BinaryFeatures",
      "relationship": "has_subtopic"
    },
    {
      "from": "NonlinearDynamicsToLQR",
      "to": "InvertedPendulumExample",
      "relationship": "example_of"
    },
    {
      "from": "New_Words_in_Emails",
      "to": "Probability_Estimation_Issue",
      "relationship": "related_to"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "DynamicProgrammingApplication",
      "relationship": "related_to"
    },
    {
      "from": "ProbabilisticModel",
      "to": "SoftmaxFunction",
      "relationship": "uses"
    },
    {
      "from": "MAP Estimate",
      "to": "MLE vs. MAP",
      "relationship": "subtopic"
    },
    {
      "from": "E-step Calculation",
      "to": "θ Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ReinforcementLearning",
      "relationship": "contains"
    },
    {
      "from": "Policy Dynamics",
      "to": "Finite Horizon Setting",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "MultivariateDistributions",
      "to": "MeanVectorMovement",
      "relationship": "depends_on"
    },
    {
      "from": "OptimalPolicyLinearInSt",
      "to": "DiscreteRicattiEquations",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "School_Quality",
      "relationship": "depends_on"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "High-Dimensional Latent Variables",
      "relationship": "addresses"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "1DConvolution",
      "relationship": "contains"
    },
    {
      "from": "VarianceMaximization",
      "to": "LagrangeMultipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Gradient_Ascend_Optimization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Belief States Update",
      "to": "Backward Pass (LQR Updates)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Single Neuron Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Optimization_Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Residual_Connections",
      "to": "Simplified_ResNet",
      "relationship": "subtopic"
    },
    {
      "from": "Transformer Model",
      "to": "Conditional Probability",
      "relationship": "depends_on"
    },
    {
      "from": "MaximumLikelihoodEstimates",
      "to": "LaplaceSmoothing",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Adaptation",
      "to": "Pretraining_Methods",
      "relationship": "related_to"
    },
    {
      "from": "ParallelismInGPUs",
      "to": "VectorizationInNN",
      "relationship": "related_to"
    },
    {
      "from": "Self-Supervised Pretraining",
      "to": "Data Augmentation",
      "relationship": "depends_on"
    },
    {
      "from": "ZeroShotLearning",
      "to": "InContextLearning",
      "relationship": "subtopic"
    },
    {
      "from": "Neural_Networks",
      "to": "Model_H_Theta_Bar",
      "relationship": "generates"
    },
    {
      "from": "Contrastive_Learning_Visual_Representations",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning_Control",
      "to": "Reinforcement_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "M-step Maximization",
      "to": "Parameter Updates",
      "relationship": "has_subtopic"
    },
    {
      "from": "MultiClassClassification",
      "to": "LogitsInMulticlass",
      "relationship": "subtopic"
    },
    {
      "from": "Finite_Horizon_MDPs",
      "to": "Recovering_Optimal_Policy",
      "relationship": "subtopic"
    },
    {
      "from": "Independence_Assumption",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "FiniteHorizonMDP",
      "to": "PayoffCalculation",
      "relationship": "subtopic"
    },
    {
      "from": "Statistical_Mechanics_of_Learning",
      "to": "Generalization",
      "relationship": "focuses_on"
    },
    {
      "from": "Data Redundancy Detection",
      "to": "Feature Correlation Example",
      "relationship": "related_to"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.1 Matrix derivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "MultiClassClassification",
      "to": "SoftmaxFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Matrix Derivatives",
      "to": "Least Squares Revisited",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "Conv2D-S",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining_Methods",
      "to": "Supervised_Pretraining",
      "relationship": "has_subtopic"
    },
    {
      "from": "Union_Bound_Application",
      "to": "Probability_Bound",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Likelihood",
      "relationship": "models"
    },
    {
      "from": "MachineLearningModels",
      "to": "DecisionBoundaries",
      "relationship": "depends_on"
    },
    {
      "from": "OptimizationMethods",
      "to": "NewtonMethod",
      "relationship": "contains"
    },
    {
      "from": "ELBO Interpretations",
      "to": "Formulation of ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "PolicyGradientMethods",
      "to": "ExpectationEquations",
      "relationship": "contains"
    },
    {
      "from": "1 Linear regression",
      "to": "1.4 Locally weighted linear regression (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Finetuning_Pretrained_Models",
      "to": "Prediction_Model_Structure",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kingma_Ba_2014",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Average Model",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Complexity_of_Jacobian_Matrices",
      "to": "Equation_7.53_Usefulness",
      "relationship": "subtopic"
    },
    {
      "from": "Regularizer Term",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Rate Schedules",
      "to": "Initialization Parameters",
      "relationship": "related_to"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "EmpiricalCovarianceMatrix",
      "relationship": "related_to"
    },
    {
      "from": "Quadratic Assumption",
      "to": "Optimal Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "Partially_Observable_MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningBasics",
      "to": "TrainingSetExamples",
      "relationship": "depends_on"
    },
    {
      "from": "State Transition",
      "to": "Action Selection",
      "relationship": "depends_on"
    },
    {
      "from": "FiniteHorizonMDP",
      "to": "TimeHorizonDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Weight Matrices and Biases",
      "relationship": "depends_on"
    },
    {
      "from": "Test Example",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Local_Optima_Issue",
      "to": "EM_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Data Redundancy Detection",
      "to": "PCA Algorithm Introduction",
      "relationship": "depends_on"
    },
    {
      "from": "ScalingInvariance",
      "to": "MachineLearningConcepts",
      "relationship": "depends_on"
    },
    {
      "from": "Single Neuron Model",
      "to": "Bias Term",
      "relationship": "depends_on"
    },
    {
      "from": "ModelParameters",
      "to": "LikelihoodFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Regularized Loss Function",
      "to": "Training Loss/Cost Function",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Kernel_Functions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOptimization",
      "to": "EmpiricalRiskMinimization",
      "relationship": "depends_on"
    },
    {
      "from": "Value_Function_Approximation",
      "to": "Model_or_Simulator",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICAOnGaussianData",
      "to": "NonGaussianDataRecovery",
      "relationship": "related_to"
    },
    {
      "from": "EM_Algorithms",
      "to": "Variational_Inference_VAE",
      "relationship": "subtopic"
    },
    {
      "from": "NegativeLogLikelihood",
      "to": "ProbabilisticModel",
      "relationship": "evaluates"
    },
    {
      "from": "Convolutional_Neural_Networks",
      "to": "1D_Convolution",
      "relationship": "has_subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.1 The multivariate normal distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Geometric Margins",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self_Supervised_Learning_Foundation_Models",
      "to": "Computer_Vision_Pretraining",
      "relationship": "subtopic"
    },
    {
      "from": "SupportVectors",
      "to": "LagrangianFunction",
      "relationship": "related_to"
    },
    {
      "from": "KKT_Conditions",
      "to": "Primal_Problem",
      "relationship": "satisfies"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "7 Deep learning",
      "to": "7.4 Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Value_Functions",
      "to": "Optimal_Value_Function",
      "relationship": "subtopic_of"
    },
    {
      "from": "GradientCalculation",
      "to": "StochasticGradientDescent",
      "relationship": "related_to"
    },
    {
      "from": "KernelsInML",
      "to": "KernelFunctionProperties",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Error_Term",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning with Non-Linear Models",
      "to": "Deep Learning Introduction",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Problem",
      "to": "Dual_Objective_Function",
      "relationship": "subtopic"
    },
    {
      "from": "Bias Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Feature_Mapping",
      "to": "Cubic_Function_Example",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "Soft_Assignments",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Algorithm 7.4",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "ProbabilityPrediction",
      "relationship": "related_to"
    },
    {
      "from": "CostFunction",
      "to": "OrdinaryLeastSquares",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Matrix Multiplication Backward Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Policy_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Backpropagation and Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningOverview",
      "to": "PoissonDistributionModeling",
      "relationship": "depends_on"
    },
    {
      "from": "Approximation Error Minimization",
      "to": "PCA",
      "relationship": "related_to"
    },
    {
      "from": "Alpha Update",
      "to": "SMO Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Shattering",
      "to": "VC Dimension",
      "relationship": "defines"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Conditional Probabilistic Model",
      "relationship": "depends_on"
    },
    {
      "from": "Log_Probability_Gradient",
      "to": "Trajectory_Probability_Change",
      "relationship": "has_subtopic"
    },
    {
      "from": "d_star_p_star_Equality",
      "to": "Primal_Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Principal_Components_Analysis",
      "to": "Data_Subspace_Identification",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "SMO_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-layer Fully-Connected Neural Networks",
      "to": "Total Neurons and Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Support_Vector_Machines_SVMs",
      "relationship": "depends_on"
    },
    {
      "from": "GELUFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Optimal Value Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "kDimensionalSubspace",
      "to": "OrthogonalBasis",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning Introduction",
      "to": "Machine Learning Overview",
      "relationship": "related_to"
    },
    {
      "from": "Modern_Neural_Networks_Modules",
      "to": "Nonlinear_Activation_Blocks",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal_Margin_Classifiers",
      "to": "KKT_Conditions",
      "relationship": "depends_on"
    },
    {
      "from": "KernelFunctionProperties",
      "to": "NecessaryConditions",
      "relationship": "depends_on"
    },
    {
      "from": "Policy",
      "to": "Optimal Policy",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Classification Problem",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Challenges",
      "to": "model_evaluation",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworks",
      "to": "WeightMatrices",
      "relationship": "subtopic"
    },
    {
      "from": "Chen_et_al_2020",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "HypothesisFunction",
      "to": "EmpiricalRiskMinimization",
      "relationship": "applied_in"
    },
    {
      "from": "KernelFunctionExample2",
      "to": "ParameterCControl",
      "relationship": "related_to"
    },
    {
      "from": "ICAOnGaussianData",
      "to": "RotationalSymmetry",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Logistic Loss Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "4.1 Gaussian discriminant analysis",
      "to": "4.1.3 Discussion: GDA and logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "EventModelsForTextClassification",
      "to": "ProbabilityEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "PCA",
      "to": "Noise_Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Training Set",
      "relationship": "has_subtopic"
    },
    {
      "from": "SoftplusFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "RegressionProblem",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningOverview",
      "to": "GeneralizedLinearModels",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "BP_for_MLPs",
      "relationship": "subtopic"
    },
    {
      "from": "JointDistribution",
      "to": "MixtureOfGaussians",
      "relationship": "depends_on"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "LogisticFunction",
      "relationship": "derives_from"
    },
    {
      "from": "Optimal Policy",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "MachineLearningArchitecture",
      "to": "LayerNormalization",
      "relationship": "contains"
    },
    {
      "from": "Chain_Rule_Applications",
      "to": "Abstract_Problem_Description",
      "relationship": "subtopic"
    },
    {
      "from": "Naive_Bayes_Assumption",
      "to": "Conditional_Independence",
      "relationship": "defines"
    },
    {
      "from": "Optimizers",
      "to": "Generalization Performance",
      "relationship": "related_to"
    },
    {
      "from": "Training Process",
      "to": "Cross-Entropy Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Function Computation",
      "to": "Gradient Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Cross-Entropy Loss",
      "relationship": "related_to"
    },
    {
      "from": "Continuous State MDPs",
      "to": "Value Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Model_Parameters",
      "to": "Machine_Learning_Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm 6",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Multi-class Classification",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "EM Algorithms",
      "relationship": "extends"
    },
    {
      "from": "Policy_Gradient_Theory",
      "to": "Log_Probability_Gradient",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal_Margin_Classifiers",
      "to": "Dual_Formulation",
      "relationship": "related_to"
    },
    {
      "from": "ProbabilityEstimation",
      "to": "NaiveBayesClassifier",
      "relationship": "subtopic"
    },
    {
      "from": "Posterior Approximation",
      "to": "Prior Choice",
      "relationship": "depends_on"
    },
    {
      "from": "Regularizer R(θ)",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Predict Step",
      "to": "Gaussian Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Equation_7.53_Usefulness",
      "to": "Derivation_Section_7.4.3",
      "relationship": "related_to"
    },
    {
      "from": "FunctionalMargin",
      "to": "ConfidenceMeasure",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Continuous Setting Assumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Pretraining",
      "to": "Representation Function",
      "relationship": "depends_on"
    },
    {
      "from": "Compression",
      "to": "PCA",
      "relationship": "subtopic"
    },
    {
      "from": "Double_Descent",
      "to": "Learning_to_Generalize",
      "relationship": "includes"
    },
    {
      "from": "PrimalProblem",
      "to": "ThetaP",
      "relationship": "defines"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Bellman_Equations",
      "relationship": "depends_on"
    },
    {
      "from": "BernoulliDistribution",
      "to": "LogPartitionFunctionForBernoulli",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Feature_Maps_and_Kernels",
      "relationship": "depends_on"
    },
    {
      "from": "MatrixNotation",
      "to": "Vectorization",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Cross_Entropy_Loss",
      "relationship": "has_subtopic"
    },
    {
      "from": "Overfitting",
      "to": "Machine Learning Concepts",
      "relationship": "related_to"
    },
    {
      "from": "Deep_Learning",
      "to": "Neural_Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "Notation_for_SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "GradientDescentConvergence",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Evaluation",
      "relationship": "subtopic"
    },
    {
      "from": "MultinomialRandomVariable",
      "to": "MaximumLikelihoodEstimates",
      "relationship": "depends_on"
    },
    {
      "from": "Loss_Functions",
      "to": "Empirical_Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Vector",
      "to": "Vocabulary",
      "relationship": "contains"
    },
    {
      "from": "Belief States Update",
      "to": "Forward Pass",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Linear_Functions",
      "relationship": "related_to"
    },
    {
      "from": "MSEDecomposition",
      "to": "AverageModel",
      "relationship": "subtopic"
    },
    {
      "from": "Text_Classification",
      "to": "Generative_Models",
      "relationship": "uses"
    },
    {
      "from": "Convergence Issues",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization_Error",
      "to": "Uniform_Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "Value_Iteration_Preference",
      "relationship": "subtopic"
    },
    {
      "from": "Law of Total Expectation",
      "to": "Estimator Simplification",
      "relationship": "depends_on"
    },
    {
      "from": "GradientComputation",
      "to": "ReparameterizationTrick",
      "relationship": "solution_to"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "BernoulliDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "LQR_DDP_LQG",
      "to": "Linear_Q",
      "relationship": "subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Convergence_Criteria",
      "relationship": "depends_on"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "EfficiencyComparison",
      "relationship": "subtopic_of"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "ProbabilisticAssumptions",
      "relationship": "related_to"
    },
    {
      "from": "Neural_Network_Input",
      "to": "Hidden_Units",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConditionalProbability",
      "to": "NaiveBayesAlgorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient_Ascend_Rule",
      "to": "Model_Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "ReLUFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Gradient Computation",
      "relationship": "results_in"
    },
    {
      "from": "Model Complexity",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Classification",
      "to": "Class Priors",
      "relationship": "depends_on"
    },
    {
      "from": "Design Matrix",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrick",
      "to": "Initialization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Linearization_of_Dynamics",
      "relationship": "depends_on"
    },
    {
      "from": "Perceptron Algorithm",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter 9 Regularization and Model Selection",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Noise_Reduction",
      "to": "Eigenfaces_Method",
      "relationship": "related_to"
    },
    {
      "from": "KernelTrick",
      "to": "IterativeUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Double_Descent",
      "relationship": "contains"
    },
    {
      "from": "Neural Networks",
      "to": "Activation Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep_Learning_Features",
      "to": "Black_Box_Issue",
      "relationship": "subtopic"
    },
    {
      "from": "Value_Iteration",
      "to": "Convergence_to_V_star",
      "relationship": "depends_on"
    },
    {
      "from": "LogisticRegression",
      "to": "Robustness",
      "relationship": "subtopic"
    },
    {
      "from": "Parameters_Beta",
      "to": "Function_Phi_Beta",
      "relationship": "defines"
    },
    {
      "from": "Finding Root",
      "to": "Newton's Method",
      "relationship": "depends_on"
    },
    {
      "from": "Chapter_16_LQR_DDP_LQG",
      "to": "Finite_Horizon_MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ICAOnGaussianData",
      "relationship": "contains"
    },
    {
      "from": "Claim 8.1.1",
      "to": "MSE Decomposition",
      "relationship": "depends_on"
    },
    {
      "from": "EM_Algorithm",
      "to": "Evidence_Lower_Bound_(ELBO)",
      "relationship": "related_to"
    },
    {
      "from": "Chain_Rule",
      "to": "Vectorized_Notation",
      "relationship": "related_to"
    },
    {
      "from": "DensitiesAndLinearTransformations",
      "to": "EffectOfLinearTransformations",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning",
      "to": "Contrastive_Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Functional Margins",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Functional Margins",
      "relationship": "has_subtopic"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Convex Function",
      "relationship": "depends_on"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "PCA",
      "relationship": "depends_on"
    },
    {
      "from": "Fully Bayesian Prediction",
      "to": "Posterior Distribution on Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Baseline Function",
      "to": "Value Function Estimation",
      "relationship": "related_to"
    },
    {
      "from": "TrainingSet",
      "to": "HypothesisFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Alpha_Parameters",
      "to": "Constraints_on_Alphas",
      "relationship": "related_to"
    },
    {
      "from": "LinearModelLimitations",
      "to": "MachineLearningChallenges",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous Setting Assumptions",
      "to": "Quadratic Rewards",
      "relationship": "subtopic"
    },
    {
      "from": "ICA_Overview",
      "to": "ICA_Ambiguities",
      "relationship": "discusses"
    },
    {
      "from": "Modern_Neural_Networks",
      "to": "Modules_in_MNN",
      "relationship": "subtopic"
    },
    {
      "from": "BernoulliDistributions",
      "to": "ExponentialFamilyDistributions",
      "relationship": "contains"
    },
    {
      "from": "Multinomial distribution",
      "to": "2.3 Multi-class classification",
      "relationship": "subtopic"
    },
    {
      "from": "Density Transformation",
      "to": "Volume Mapping",
      "relationship": "has_subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "State Sampling",
      "relationship": "subtopic"
    },
    {
      "from": "optimization_in_value_iteration",
      "to": "value_iteration_policy_optimization",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression Derivation",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function Overview",
      "to": "Activation Functions Backward Function",
      "relationship": "subtopic"
    },
    {
      "from": "PACAssumption",
      "to": "GeneralizationError",
      "relationship": "defines_context_for"
    },
    {
      "from": "Adaptation Phase",
      "to": "Foundation Models",
      "relationship": "subtopic"
    },
    {
      "from": "Variational_Inference_Review",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "KernelTrick",
      "to": "ThetaVector",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines",
      "to": "Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "FeatureSelection",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "LinearSeparabilityAssumption",
      "to": "OptimalMarginClassifier",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianMixtureModel",
      "to": "EMAlgorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Latent_Variables",
      "to": "Gaussian_Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Naive_Bayes_Filter",
      "to": "New_Words_in_Emails",
      "relationship": "depends_on"
    },
    {
      "from": "Learning a Model for an MDP",
      "to": "Continuous State MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Multidimensional Generalization",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "NeuralNetworks",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Decay",
      "to": "L2 Regularization",
      "relationship": "related_to"
    },
    {
      "from": "Naive_Bayes_Classifier",
      "to": "Parameter_Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization_Frameworks",
      "to": "Hessian_Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Expected Total Payoff",
      "to": "Chapter 17 Policy Gradient (REINFORCE)",
      "relationship": "subtopic"
    },
    {
      "from": "Dimensionality_Reduction",
      "to": "Computational_Benefits",
      "relationship": "related_to"
    },
    {
      "from": "Posterior Approximation",
      "to": "MAP Estimate",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "related_to"
    },
    {
      "from": "Learning Theory",
      "to": "Generalization Error",
      "relationship": "subtopic"
    },
    {
      "from": "Neural_Networks",
      "to": "Parameters_Beta",
      "relationship": "contains"
    },
    {
      "from": "Value_Functions",
      "to": "Expectation_Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "Training_Loss",
      "to": "Mean_Squared_Error",
      "relationship": "example_of"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "E-step Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reliability_of_Training_Error_Estimate",
      "to": "Training_Error_Formula",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Linear Regression Models",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "VectorizationMotivation",
      "relationship": "subtopic"
    },
    {
      "from": "Loss_Functions",
      "to": "Population_Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Bellman Equations",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian_Distribution",
      "to": "Covariance_Matrix",
      "relationship": "related_to"
    },
    {
      "from": "ICA",
      "to": "ICA_Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Labeled_Data",
      "to": "Adaptation",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Theory",
      "to": "Hoeffding_Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Policy",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "TrainingExampleUpdate",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworks",
      "to": "StochasticGradientDescent",
      "relationship": "related_to"
    },
    {
      "from": "Value Function Derivation",
      "to": "Gradient Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Random_Variables",
      "to": "Mean_of_Gaussian",
      "relationship": "depends_on"
    },
    {
      "from": "DiscreteRicattiEquations",
      "to": "PhiTDependency",
      "relationship": "related_to"
    },
    {
      "from": "Policy_Iteration",
      "to": "Policy_Improvement",
      "relationship": "subtopic"
    },
    {
      "from": "Randomized Policy",
      "to": "Chapter 17 Policy Gradient (REINFORCE)",
      "relationship": "subtopic"
    },
    {
      "from": "Learning a Model for an MDP",
      "to": "Connections between Policy and Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "PolynomialModelFitting",
      "to": "MachineLearningChallenges",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Forward Pass",
      "relationship": "related_to"
    },
    {
      "from": "DensityFunctionForSources",
      "to": "SigmoidFunctionChoice",
      "relationship": "related_to"
    },
    {
      "from": "state_transition_probabilities_estimation",
      "to": "mdp_model_learning",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization_Model_Selection",
      "to": "Model_Selection_Cross_Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Quadratic Programming (QP)",
      "relationship": "can_be_solved_by"
    },
    {
      "from": "KernelsInML",
      "to": "FeatureMapPhi",
      "relationship": "subtopic"
    },
    {
      "from": "PositiveSemiDefinite",
      "to": "ArbitraryVectorZ",
      "relationship": "depends_on"
    },
    {
      "from": "VariationalInference",
      "to": "ELBO",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Estimator Variance Reduction",
      "to": "Algorithm 7.4",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.1 Logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Variance Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "JointDistributionModeling",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Kernels",
      "to": "Valid_Kernel_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "GenerativeApproach",
      "to": "ClassPriors",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "State Transition",
      "relationship": "includes"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Prior Distribution",
      "relationship": "includes_concept"
    },
    {
      "from": "LogLikelihood",
      "to": "ParameterEstimation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Value_Iteration",
      "to": "Optimal_Policy_Finding",
      "relationship": "related_to"
    },
    {
      "from": "Cross Validation Technique",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "OptimalMarginClassifier",
      "to": "MachineLearningConcepts",
      "relationship": "subtopic"
    },
    {
      "from": "Sample_Complexity",
      "to": "Training_Error",
      "relationship": "related_to"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "GradientDescentExample",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Differentiable Circuit",
      "relationship": "depends_on"
    },
    {
      "from": "KernelFunctionProperties",
      "to": "SufficientConditions",
      "relationship": "related_to"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "LikelihoodFunction",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "GradientDescentUpdateRule",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Learning_Model_MDP",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "DualFormulation",
      "to": "KernelTrick",
      "relationship": "leads_to"
    },
    {
      "from": "Soft_Assignments",
      "to": "K_Means_Clustering",
      "relationship": "contrasts_with"
    },
    {
      "from": "LeakyReLU",
      "to": "GELUFunction",
      "relationship": "related_to"
    },
    {
      "from": "NonLinearModel",
      "to": "TrainingExamples",
      "relationship": "depends_on"
    },
    {
      "from": "Markov_Decision_Processes",
      "to": "Discount_Factor",
      "relationship": "defines"
    },
    {
      "from": "Model_Complexity_Measurements",
      "to": "Norm_of_Models",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Bias-Variance Tradeoff",
      "relationship": "contains"
    },
    {
      "from": "LMS_Algorithm",
      "to": "Gradient_Descent_Update_Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Beta Update Equation",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Optimal Margin Classifier",
      "relationship": "leads_to"
    },
    {
      "from": "Pretraining Phase",
      "to": "Foundation Models",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian_Distribution",
      "to": "Standard_Normal_Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "CostFunctionJ",
      "to": "PartialDerivative",
      "relationship": "depends_on"
    },
    {
      "from": "Vocabulary",
      "to": "Stop_Words",
      "relationship": "includes"
    },
    {
      "from": "NeuralNetworks",
      "to": "ClassificationProblem",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression Derivation",
      "to": "Perceptron Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "subtopic"
    },
    {
      "from": "CrossEntropyLoss",
      "to": "NegativeLogLikelihood",
      "relationship": "related_to"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Curse of Dimensionality",
      "relationship": "has_consequence"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Explicit Regularization Techniques",
      "relationship": "has_subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Synchronous Updates",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Kernel_Trick",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworks",
      "to": "TwoLayerNN",
      "relationship": "subtopic"
    },
    {
      "from": "LQR_DDP_LQG",
      "to": "Finite_Horizon_MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Labeled Dataset",
      "to": "Downstream Task",
      "relationship": "depends_on"
    },
    {
      "from": "Conditional Probabilistic Model",
      "to": "Exponential Family",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Multidimensional Generalization",
      "relationship": "related_to"
    },
    {
      "from": "SIMCLR",
      "to": "Augmentation_Techniques",
      "relationship": "uses"
    },
    {
      "from": "Learning Theory",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "ParameterSharing",
      "relationship": "subtopic_of"
    },
    {
      "from": "KernelFunctionExample1",
      "to": "FeatureMappingPhi",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "ShatteringConcept",
      "relationship": "related_to"
    },
    {
      "from": "Data_Preprocessing",
      "to": "Probability_Density_Functions",
      "relationship": "depends_on"
    },
    {
      "from": "BinaryClassification",
      "to": "LogisticFunction",
      "relationship": "related_to"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "GaussianDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "NominalTrajectoryGeneration",
      "to": "DifferentialDynamicProgramming",
      "relationship": "subtopic"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "GaussianDistribution",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningOverview",
      "to": "ExpectationMaximizationAlgorithm",
      "relationship": "contains"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Policy_Value_Iteration_Connections",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningAlgorithms",
      "to": "ConditionalDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning Problem",
      "to": "Regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "BinaryFeatures",
      "to": "MultinomialGeneralization",
      "relationship": "generalizes_to"
    },
    {
      "from": "EM Algorithms",
      "to": "Mixture of Gaussians",
      "relationship": "depends_on"
    },
    {
      "from": "LikelihoodFunction",
      "to": "ConditionalProbabilityDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.2 Digression: the perceptron learning algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "ICA_Overview",
      "to": "Cocktail_Party_Problem",
      "relationship": "explains_with_example"
    },
    {
      "from": "EM_Algorithm",
      "to": "Single_Example_Optimization",
      "relationship": "relates_to"
    },
    {
      "from": "PolicyGradientMethods",
      "to": "TrajectoryProbability",
      "relationship": "depends_on"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Concave Function",
      "relationship": "related_to"
    },
    {
      "from": "NeuralNetworks",
      "to": "SingleNeuronNN",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Markov_Decision_Processes",
      "relationship": "defines"
    },
    {
      "from": "Modules_in_MLP",
      "to": "Parameters_and_Operations",
      "relationship": "depends_on"
    },
    {
      "from": "PCA Algorithm Introduction",
      "to": "Normalization Process",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "NonLinearModels",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Applications",
      "to": "ICA",
      "relationship": "next_topic"
    },
    {
      "from": "Chapter_15_Summary",
      "to": "k_Update_Frequency",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Value_Iteration_Policy_Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Likelihood_Optimization",
      "to": "Multiple_Examples_Case",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman's Equation",
      "to": "Optimal Value Function",
      "relationship": "defines"
    },
    {
      "from": "Test_Error_Training_Error",
      "to": "Generalization_Gap",
      "relationship": "defines"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Discrete-State MDP",
      "relationship": "related_to"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Coordinate_Ascent",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised_Learning",
      "to": "ICA",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "contains"
    },
    {
      "from": "Policy Definition",
      "to": "Supervised Learning",
      "relationship": "related_to"
    },
    {
      "from": "CrossEntropyLoss",
      "to": "GradientCalculation",
      "relationship": "depends_on"
    },
    {
      "from": "NormalizationLayers",
      "to": "LayerNorm",
      "relationship": "contains"
    },
    {
      "from": "Convergence_Criteria",
      "to": "KKT_Conditions",
      "relationship": "has_subtopic"
    },
    {
      "from": "MaximumLikelihoodEstimation",
      "to": "GradientAscent",
      "relationship": "subtopic"
    },
    {
      "from": "LMS_Update_Rule",
      "to": "Single_Training_Example",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining",
      "to": "Transfer_Learning",
      "relationship": "subtopic"
    },
    {
      "from": "ICAIndependenceAssumption",
      "to": "DensityFunctionForSources",
      "relationship": "subtopic"
    },
    {
      "from": "Mixture of Gaussians Example",
      "to": "E-step Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Immediate Reward",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Model-Free Algorithm",
      "to": "Chapter 17 Policy Gradient (REINFORCE)",
      "relationship": "subtopic"
    },
    {
      "from": "Self_Supervised_Learning_Foundation_Models",
      "to": "Large_Language_Models",
      "relationship": "subtopic"
    },
    {
      "from": "MSEDecomposition",
      "to": "VarianceDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "ModelAssumptions",
      "relationship": "subtopic"
    },
    {
      "from": "BackwardFunctionEfficiency",
      "to": "VectorizedBackwardFunction",
      "relationship": "depends_on"
    },
    {
      "from": "LayerNormalization",
      "to": "LN-S",
      "relationship": "subtopic"
    },
    {
      "from": "Primal_Problem",
      "to": "Dual_Problem",
      "relationship": "related_to"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization Parameter C",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Gradient_Theorem",
      "to": "Expectation_Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Gradient_Calculation",
      "relationship": "related_to"
    },
    {
      "from": "ExpectationMaximizationAlgorithm",
      "to": "MStepUpdateRule",
      "relationship": "subtopic"
    },
    {
      "from": "LinearizationOfDynamics",
      "to": "DifferentialDynamicProgramming",
      "relationship": "subtopic"
    },
    {
      "from": "ICA",
      "to": "ICA_Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "ReLU Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "In-Context_Learning",
      "to": "Prompt_Construction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "LQG",
      "relationship": "related_to"
    },
    {
      "from": "Data Augmentation",
      "to": "Positive Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Downstream Task Loss",
      "to": "Linear Probe",
      "relationship": "related_to"
    },
    {
      "from": "Corollary Proof",
      "to": "Sample Complexity Bound",
      "relationship": "subtopic"
    },
    {
      "from": "Double_Descent_Phenomenon",
      "to": "Model_Complexity_Measurements",
      "relationship": "subtopic"
    },
    {
      "from": "MatrixAlgebra",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "HypothesisFunction",
      "to": "TrainingError",
      "relationship": "related_to"
    },
    {
      "from": "Partially_Observable_MDPs",
      "to": "LQR_Extension",
      "relationship": "has_subtopic"
    },
    {
      "from": "BiasVarianceTradeoff",
      "to": "MSEDecomposition",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Function J(θ)",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "SpamNonSpamDetermination",
      "to": "WordGenerationProcess",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Descent (GD)",
      "to": "Learning Rate",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "Time Dependent Dynamics",
      "to": "Non-Stationary Policies",
      "relationship": "related_to"
    },
    {
      "from": "ParameterizedModels",
      "to": "TransformerModel",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Optimization",
      "to": "Policy_Gradient_Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Differential_Dynamic_Programming",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression Model",
      "relationship": "subtopic"
    },
    {
      "from": "Clustering",
      "to": "K-Means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian_Distribution",
      "to": "Succinct_Representation",
      "relationship": "subtopic"
    },
    {
      "from": "GradientDescentAlgorithm",
      "to": "CostFunctionJ",
      "relationship": "depends_on"
    },
    {
      "from": "Cross_Entropy_Loss",
      "to": "Softmax_Function",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problem",
      "to": "Convex Quadratic Objective",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Cross-Entropy Loss Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "BackwardFunctionEfficiency",
      "relationship": "related_to"
    },
    {
      "from": "Modern_Neural_Networks",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers",
      "to": "Pretraining",
      "relationship": "related_to"
    },
    {
      "from": "LinearHypothesis",
      "to": "ParametersWeights",
      "relationship": "subtopic"
    },
    {
      "from": "M_Step",
      "to": "Gaussian_Mixture_Models",
      "relationship": "subtopic"
    },
    {
      "from": "Bandwidth Parameter for LWR",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "StochasticGradientDescent",
      "to": "ConvergenceBehavior",
      "relationship": "related_to"
    },
    {
      "from": "KernelsInML",
      "to": "ValidKernelFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Empirical_Risk_Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Regularized Loss Function",
      "to": "Regularizer Term",
      "relationship": "depends_on"
    },
    {
      "from": "General EM Algorithm",
      "to": "Latent Variable Model",
      "relationship": "subtopic"
    },
    {
      "from": "Reward Function",
      "to": "Reinforcement Learning",
      "relationship": "subtopic_of"
    },
    {
      "from": "MachineLearningOverview",
      "to": "NeuralNetworksComposition",
      "relationship": "has_subtopic"
    },
    {
      "from": "VariationalInference",
      "to": "GaussianDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "kDimensionalSubspace",
      "relationship": "subtopic"
    },
    {
      "from": "Linearization_of_Dynamics",
      "to": "LQR_Relationship",
      "relationship": "related_to"
    },
    {
      "from": "Partially_Observable_MDPs",
      "to": "Policy_in_POMDP",
      "relationship": "related_to"
    },
    {
      "from": "PrimalProblem",
      "to": "GeneralizedLagrangian",
      "relationship": "related_to"
    },
    {
      "from": "EM_Algorithms",
      "to": "Mixture_Gaussians_Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous_State_MDPs",
      "to": "Value_Function_Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Dual_Problem",
      "to": "Optimal_Value_Dual",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning",
      "to": "Text_Classification",
      "relationship": "contains"
    },
    {
      "from": "Supervised Learning Algorithm",
      "to": "Linear Regression",
      "relationship": "related_to"
    },
    {
      "from": "Loss Function",
      "to": "Optimizers",
      "relationship": "related_to"
    },
    {
      "from": "UnderfittingExample",
      "to": "LinearModelLimitations",
      "relationship": "subtopic"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ExponentialFamilyDistributions",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningModels",
      "to": "BinaryClassification",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate_Ascent",
      "to": "Quadratic_Function_Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "UpdateRule",
      "to": "LMSUpdateRule",
      "relationship": "related_to"
    },
    {
      "from": "ParameterizedModels",
      "to": "EmbeddingsAndRepresentations",
      "relationship": "depends_on"
    },
    {
      "from": "Probability_Bound",
      "to": "Uniform_Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "VectorW",
      "to": "DecisionBoundary",
      "relationship": "related_to"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Models",
      "relationship": "is_a"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "AsymptoticEfficiency",
      "relationship": "related_to"
    },
    {
      "from": "Regularization in Deep Learning",
      "to": "Loss Landscape in Deep Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support_Vector_Machines_SVMs",
      "to": "Sequential_Minimal_Optimization_SMO",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Non-Parametric Algorithms",
      "relationship": "contains"
    },
    {
      "from": "Expectation_Maximization_Guarantees",
      "to": "Machine_Learning_Concepts",
      "relationship": "depends_on"
    },
    {
      "from": "Value Function Approximation",
      "to": "Max Action Selection",
      "relationship": "subtopic"
    },
    {
      "from": "Transformer Model",
      "to": "Training Process",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backpropagation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Training_Data_Set",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Loss_Functions",
      "relationship": "contains"
    },
    {
      "from": "d_star_p_star_Equality",
      "to": "Dual_Problem",
      "relationship": "depends_on"
    },
    {
      "from": "7.4 Backpropagation",
      "to": "7.4.4 Back-propagation for MLPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement_Learning",
      "to": "Bellman_Equation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hypothesis_Class_Size",
      "to": "Uniform_Convergence",
      "relationship": "related_to"
    },
    {
      "from": "Maximizing Function",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Relationship_Primal_Dual",
      "relationship": "related_to"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "M-step Maximization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Flat Minima Hypothesis",
      "to": "Generalization Performance",
      "relationship": "subtopic"
    },
    {
      "from": "Mean Field Assumption",
      "to": "Variational Inference",
      "relationship": "subtopic"
    },
    {
      "from": "2 Classification and logistic regression",
      "to": "2.3 Multi-class classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "E_Step",
      "to": "ELBO",
      "relationship": "uses"
    },
    {
      "from": "Bias_Variance_Tradeoff",
      "to": "Mathematical_Decomposition_BV",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation_Maximization",
      "to": "Reparameterization_Trick",
      "relationship": "depends_on"
    },
    {
      "from": "GLMDesignChoices",
      "to": "LogisticRegression",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Total Payoff",
      "relationship": "includes"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Deep Learning Regularization Techniques",
      "relationship": "related_to"
    },
    {
      "from": "Probability_Theory",
      "to": "Hoeffding_Inequality",
      "relationship": "contains"
    },
    {
      "from": "Machine_Learning_Concepts",
      "to": "Principal_Components_Analysis",
      "relationship": "related_to"
    },
    {
      "from": "Historical Context",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "MSEDecomposition",
      "to": "BiasDefinition",
      "relationship": "subtopic"
    },
    {
      "from": "EffectOfLinearTransformations",
      "to": "CorrectFormulaForDensityTransformation",
      "relationship": "depends_on"
    },
    {
      "from": "Modern_Neural_Networks_Modules",
      "to": "Matrix_Multiplication_Module",
      "relationship": "subtopic"
    },
    {
      "from": "Polynomial Regression Model",
      "to": "Bias and Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "NeuralNetworks",
      "to": "MultiLayerNN",
      "relationship": "subtopic"
    },
    {
      "from": "KernelsInML",
      "to": "ExampleKernels",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "GradientDescent",
      "relationship": "related_to"
    },
    {
      "from": "Support_Vector_Machines",
      "to": "Optimal_Margin_Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "IdentityFunction",
      "to": "ActivationFunctions",
      "relationship": "subtopic"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "ChannelConcepts",
      "relationship": "subtopic_of"
    },
    {
      "from": "JensensInequality",
      "to": "LogLikelihoodBound",
      "relationship": "related_to"
    },
    {
      "from": "Box Constraint",
      "to": "Alpha Update",
      "relationship": "related_to"
    },
    {
      "from": "I Supervised learning",
      "to": "2 Classification and logistic regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "OrdinaryLeastSquares",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "I Supervised learning",
      "to": "5 Kernel methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "FunctionRepresentation",
      "to": "LinearHypothesis",
      "relationship": "depends_on"
    },
    {
      "from": "LocallyWeightedLinearRegression",
      "to": "FeatureSelection",
      "relationship": "related_to"
    },
    {
      "from": "Markov_Decision_Processes",
      "to": "Reward_Function",
      "relationship": "includes"
    },
    {
      "from": "Reliability_of_Training_Error_Estimate",
      "to": "Bernoulli_Random_Variable_Z",
      "relationship": "related_to"
    },
    {
      "from": "Total Payoff",
      "to": "Discount Factor (γ)",
      "relationship": "related_to"
    },
    {
      "from": "Chain Rule",
      "to": "Basic Modules Backward Function",
      "relationship": "subtopic"
    },
    {
      "from": "1.2 The normal equations",
      "to": "1.2.2 Least squares revisited",
      "relationship": "has_subtopic"
    },
    {
      "from": "Normalization",
      "to": "Mean Removal",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine_Learning_Features",
      "to": "Family_Size",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries_on_PD",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Error",
      "to": "Vapnik's Theorem",
      "relationship": "describes"
    },
    {
      "from": "Continuous Setting Assumptions",
      "to": "Linear Transitions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Topics",
      "to": "State_Space_Representation",
      "relationship": "subtopic"
    },
    {
      "from": "ConvolutionalLayers",
      "to": "2DConvolution",
      "relationship": "contains"
    },
    {
      "from": "MachineLearningModels",
      "to": "GaussianDiscriminantAnalysis",
      "relationship": "related_to"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Sign Change Ambiguity",
      "relationship": "has_subtopic"
    },
    {
      "from": "FittedValueIteration",
      "to": "ContinuousStateSpace",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "NormalEquations",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Final Optimization Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Foundation_Models_Opportunities_Risks",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "SoftmaxFunction",
      "to": "ProbabilityVector",
      "relationship": "produces"
    },
    {
      "from": "Bellman_Equation",
      "to": "Value_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "3 Generalized linear models",
      "to": "3.2 Constructing GLMs",
      "relationship": "has_subtopic"
    },
    {
      "from": "LinearRegression",
      "to": "CostFunction",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning",
      "to": "Linear Regression",
      "relationship": "related_to"
    },
    {
      "from": "MaximizeGeometricMarginProblem",
      "to": "OptimalMarginClassifier",
      "relationship": "related_to"
    },
    {
      "from": "Normalization",
      "to": "Data Rescaling",
      "relationship": "has_subtopic"
    },
    {
      "from": "BERT_Pretraining",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "CrossValidation",
      "to": "HoldOutCrossValidation",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "EM_Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Double_Descent",
      "to": "Statistical_Mechanics_of_Learning",
      "relationship": "depends_on"
    },
    {
      "from": "GradientAscentRule",
      "to": "LogisticLossFunction",
      "relationship": "depends_on"
    },
    {
      "from": "EmpiricalRiskMinimization",
      "to": "CrossValidation",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Learning Rate Schedules",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Gradient Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Policy_Gradient_Methods",
      "to": "REINFORCE_Algorithm",
      "relationship": "subtopic_of"
    },
    {
      "from": "Hyperparameters",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "ConditionalProbabilityModeling",
      "to": "ParameterizedModels",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Algorithms",
      "to": "Value_Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Sample_Complexity",
      "to": "Hypothesis_Class_Size",
      "relationship": "depends_on"
    },
    {
      "from": "GeneralizedLinearModels",
      "to": "ExponentialFamilyDistributions",
      "relationship": "contains"
    },
    {
      "from": "Normalization Process",
      "to": "Variance Scaling",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization",
      "to": "GeneralizationToLayers",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilisticInterpretation",
      "to": "LinearRegression",
      "relationship": "related_to"
    },
    {
      "from": "Batch_Gradient_Descent",
      "to": "Multiple_Examples_Update_Rule",
      "relationship": "subtopic"
    },
    {
      "from": "LayerNormalization",
      "to": "LayerNormSubModule",
      "relationship": "contains"
    },
    {
      "from": "Kernel_Methods",
      "to": "Support_Vector_Machines",
      "relationship": "has_subtopic"
    },
    {
      "from": "BernoulliDistribution",
      "to": "SufficientStatisticForBernoulli",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Matricization Approach",
      "relationship": "contains"
    },
    {
      "from": "Hypothesis_Class_Size",
      "to": "Floating_Point_Precision",
      "relationship": "related_to"
    },
    {
      "from": "Classification Problem",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrange Duality",
      "to": "Constrained Optimization Problems",
      "relationship": "applies_to"
    },
    {
      "from": "LayerNormalization",
      "to": "LearnableParameters",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backpropagation Algorithm",
      "relationship": "contains"
    },
    {
      "from": "Dual_Problem_Formulation",
      "to": "KKT_Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Uniform_Convergence",
      "to": "Generalization_Error",
      "relationship": "implies"
    },
    {
      "from": "Regularization Parameter (λ)",
      "to": "Regularizer Term",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Double_Descent_Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "L1 Regularization",
      "relationship": "includes"
    },
    {
      "from": "OptimalValueForInterceptTerm",
      "to": "PredictionUsingSupportVectors",
      "relationship": "depends_on"
    },
    {
      "from": "UpdateRule",
      "to": "StochasticGradientDescent",
      "relationship": "subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.1 Feature maps",
      "relationship": "has_subtopic"
    },
    {
      "from": "Regularization_Model_Selection",
      "to": "Bayesian_Statistics_Reg",
      "relationship": "subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.2 LMS (least mean squares) with features",
      "relationship": "has_subtopic"
    },
    {
      "from": "DistanceToBoundary",
      "to": "DecisionBoundary",
      "relationship": "subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "MachineLearningOverview",
      "relationship": "subtopic"
    },
    {
      "from": "Policy_Iteration",
      "to": "Bellman_Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Lagrange Duality",
      "to": "Lagrangian",
      "relationship": "defines"
    },
    {
      "from": "VariationalInference",
      "to": "ELBOOptimization",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate_Ascend_Algorithm",
      "to": "Unconstrained_Optimization_Problem",
      "relationship": "depends_on"
    },
    {
      "from": "SVM_Derivation_SMO",
      "to": "SMO_Algorithm_Overview",
      "relationship": "subtopic"
    },
    {
      "from": "LQR_Extension",
      "to": "Kalman_Filter",
      "relationship": "uses"
    },
    {
      "from": "Kingma_Welling_2013",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Adaptation Algorithm",
      "to": "Downstream Task",
      "relationship": "depends_on"
    },
    {
      "from": "Value_Iteration",
      "to": "Bellman_Equations",
      "relationship": "depends_on"
    },
    {
      "from": "LeakyReLU",
      "to": "ReLUFunction",
      "relationship": "subtopic"
    },
    {
      "from": "MachineLearningConcepts",
      "to": "TotalParametersConv1D",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogisticRegression",
      "to": "ConditionalDistribution",
      "relationship": "subtopic"
    },
    {
      "from": "ProbabilityDistribution",
      "to": "JensensInequality",
      "relationship": "depends_on"
    },
    {
      "from": "BLASOptimization",
      "to": "VectorizationInNN",
      "relationship": "subtopic"
    },
    {
      "from": "ExponentialFamilyDistributions",
      "to": "LogPartitionFunction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Policy",
      "relationship": "includes"
    },
    {
      "from": "Opper_1995",
      "to": "Machine_Learning_Papers",
      "relationship": "related_to"
    },
    {
      "from": "LogisticLossFunction",
      "to": "NegativeLogLikelihood",
      "relationship": "related_to"
    },
    {
      "from": "Jacobian_Matrix_Transpose",
      "to": "Complexity_of_Jacobian_Matrices",
      "relationship": "related_to"
    },
    {
      "from": "1 Linear regression",
      "to": "1.1 LMS algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Negative Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "ThetaVector",
      "to": "BetaCoefficients",
      "relationship": "subtopic"
    },
    {
      "from": "EM_Algorithm",
      "to": "E_Step",
      "relationship": "depends_on"
    },
    {
      "from": "Optimizers",
      "to": "Implicit Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "LocallyWeightedLinearRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Belief States Update",
      "relationship": "has_subtopic"
    },
    {
      "from": "7 Deep learning",
      "to": "7.3 Modules in Modern Neural Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Primal_Problem",
      "to": "Primal_Objective_Function",
      "relationship": "subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "LagrangeMultipliers",
      "relationship": "depends_on"
    },
    {
      "from": "NaturalParameterForBernoulli",
      "to": "SigmoidFunction",
      "relationship": "related_to"
    },
    {
      "from": "Hastie_et_al_2019",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Law of Total Expectation",
      "to": "Conditional Expectations",
      "relationship": "related_to"
    },
    {
      "from": "Normalization",
      "to": "Variance Scaling",
      "relationship": "has_subtopic"
    },
    {
      "from": "LogLikelihoodFunction",
      "to": "MaximumLikelihoodEstimation",
      "relationship": "subtopic"
    },
    {
      "from": "Convex_Functions",
      "to": "Jensens_Inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Algorithm_for_Kernel_Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Permutation Matrix",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "k-means Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "High_Dimensional_Statistics",
      "to": "Machine_Learning_Papers",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Classification_Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.4 Properties of kernels",
      "relationship": "has_subtopic"
    },
    {
      "from": "Binary_Classification",
      "to": "Training_Set",
      "relationship": "has_part"
    },
    {
      "from": "General EM Algorithm",
      "to": "Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "DensityOfErrorTerm",
      "to": "ErrorTermAssumption",
      "relationship": "defines"
    },
    {
      "from": "Vectorization over Training Examples",
      "to": "Forward Pass",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Maps_and_Kernels",
      "to": "Prediction_From_Kernel",
      "relationship": "subtopic"
    },
    {
      "from": "SMO_Algorithm",
      "to": "Efficient_Update_Strategy",
      "relationship": "related_to"
    },
    {
      "from": "Conv1D-S",
      "to": "TotalParametersConv1D",
      "relationship": "depends_on"
    },
    {
      "from": "Physics Simulation",
      "to": "Open Dynamics Engine",
      "relationship": "depends_on"
    },
    {
      "from": "Text_Classification",
      "to": "Feature_Vector_Selection",
      "relationship": "depends_on"
    },
    {
      "from": "GaussianDiscriminantAnalysis",
      "to": "NormalDistributionsInGDA",
      "relationship": "subtopic"
    },
    {
      "from": "PrincipalComponentAnalysis",
      "to": "VarianceMaximization",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization_Model_Selection",
      "to": "Regularization_Techniques",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization in MDPs",
      "to": "Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "DensityFunctionForSources",
      "to": "CumulativeDistributionFunction",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Topic",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "ConstrainedOptimization",
      "to": "PrimalProblem",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "BatchGradientDescent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Upper_Bound_on_Generalization_Error",
      "to": "Simultaneous_Guarantees_for_All_hypotheses",
      "relationship": "subtopic"
    },
    {
      "from": "LinearRegressionOptimization",
      "to": "StochasticGradientDescent",
      "relationship": "has_subtopic"
    },
    {
      "from": "PCA",
      "to": "Clustering_Cars",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning_Challenges",
      "to": "k_fold_cross_validation",
      "relationship": "depends_on"
    },
    {
      "from": "I Supervised learning",
      "to": "1 Linear regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "5 Kernel methods",
      "to": "5.3 LMS with the kernel trick",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement Learning",
      "to": "Machine Learning Models",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Classification",
      "to": "Posterior Distribution",
      "relationship": "derives_from"
    },
    {
      "from": "MachineLearningModels",
      "to": "LogisticRegression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Expectation_Maximization",
      "to": "Gradient_Ascend_Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "k-means Algorithm",
      "to": "Convergence Properties",
      "relationship": "related_to"
    },
    {
      "from": "Self_Supervised_Learning",
      "to": "Pretraining_Loss",
      "relationship": "depends_on"
    },
    {
      "from": "MachineLearningModels",
      "to": "GeneralizedLinearModels",
      "relationship": "contains"
    },
    {
      "from": "E_Step",
      "to": "M_Step",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian_Distribution",
      "to": "Density_Properties",
      "relationship": "has_property"
    },
    {
      "from": "centroid_initialization",
      "to": "k-means_algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Density Transformation",
      "to": "General Case",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Calculation",
      "to": "Matrix Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "algorithm_for_unknown_probabilities",
      "to": "mdp_model_learning",
      "relationship": "subtopic"
    },
    {
      "from": "RewardFunctionApproximation",
      "to": "DifferentialDynamicProgramming",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic_Regression",
      "to": "Classification_Problem",
      "relationship": "is_solution_for"
    },
    {
      "from": "I Supervised learning",
      "to": "6 Support vector machines",
      "relationship": "has_subtopic"
    },
    {
      "from": "Self-supervised Learning",
      "to": "Machine Learning Concepts",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Underfitting",
      "relationship": "contains"
    }
  ]
}