{
  "nodes": [
    {
      "id": "Matrix Derivatives",
      "type": "major",
      "parent": null,
      "description": "Derivation of matrix derivatives for functions mapping matrices to real numbers."
    },
    {
      "id": "Gradient Definition",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Definition and example of the gradient of a function with respect to an n-by-d matrix."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares using matrix derivatives for closed-form solution of theta."
    },
    {
      "id": "Design Matrix",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Definition and use of the design matrix in regression problems."
    },
    {
      "id": "Vector y",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Description of vector y containing target values from training set."
    },
    {
      "id": "Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning to predict target values from input data."
    },
    {
      "id": "Hypothesis",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Function learned by the model to make predictions."
    },
    {
      "id": "Regression Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "A statistical approach for predicting the relationship between variables."
    },
    {
      "id": "Classification Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Categorizing inputs into discrete classes using neural networks."
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": null,
      "description": "Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables)."
    },
    {
      "id": "Housing Example",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Example dataset with living area and number of bedrooms as features."
    },
    {
      "id": "Feature Selection",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Process of selecting a subset of relevant features for use in model construction."
    },
    {
      "id": "Least-Squares Cost Function",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "A method to find the best fit line by minimizing the sum of squared differences between observed values and predicted values."
    },
    {
      "id": "Probabilistic Interpretation",
      "type": "major",
      "parent": null,
      "description": "Interpreting linear regression from a probabilistic perspective, assuming Gaussian noise."
    },
    {
      "id": "Error Term",
      "type": "subnode",
      "parent": "Probabilistic Interpretation",
      "description": "Difference between actual output and predicted value; crucial for parameter updates."
    },
    {
      "id": "Gaussian Distribution",
      "type": "subnode",
      "parent": "Probabilistic Interpretation",
      "description": "A probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean."
    },
    {
      "id": "Gradient Descent",
      "type": "major",
      "parent": null,
      "description": "Optimization algorithm used to minimize a function by iteratively moving towards the minimum value of that function."
    },
    {
      "id": "Learning Rate (\u03b1)",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Parameter controlling the size of each step during gradient descent."
    },
    {
      "id": "J(\u03b8)",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Cost function to be minimized in gradient descent."
    },
    {
      "id": "Update Rule",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Rule defining how parameters are updated during stochastic gradient ascent optimization."
    },
    {
      "id": "LMS Update Rule",
      "type": "subnode",
      "parent": "Update Rule",
      "description": "Least Mean Squares update rule for adjusting parameters in machine learning models."
    },
    {
      "id": "Widrow-Hoff Learning Rule",
      "type": "subnode",
      "parent": "LMS Update Rule",
      "description": "Alternative name for the LMS update rule, used in adaptive filters and neural networks."
    },
    {
      "id": "Machine Learning Basics",
      "type": "major",
      "parent": null,
      "description": "Fundamental concepts in machine learning including loss functions and model evaluation."
    },
    {
      "id": "Function Representation",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "How functions are represented and parameterized in ML models."
    },
    {
      "id": "Linear Function Approximation",
      "type": "subnode",
      "parent": "Function Representation",
      "description": "Approximating y as a linear function of x with parameters \u03b8."
    },
    {
      "id": "Parameters (Weights)",
      "type": "subnode",
      "parent": "Linear Function Approximation",
      "description": "\u03b8i's are the parameters or weights in the model."
    },
    {
      "id": "Cost Function",
      "type": "subnode",
      "parent": "Function Representation",
      "description": "Measures the performance of a machine learning model by quantifying the error between predicted and actual values."
    },
    {
      "id": "Ordinary Least Squares",
      "type": "subnode",
      "parent": "Cost Function",
      "description": "Least-squares cost function used in linear regression."
    },
    {
      "id": "LMS Algorithm",
      "type": "major",
      "parent": null,
      "description": "Algorithm for minimizing the cost function J(\u03b8)."
    },
    {
      "id": "Batch Gradient Descent",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Variant of gradient descent that uses the entire dataset to make a single update in each iteration."
    },
    {
      "id": "Gradient Descent Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques for minimizing cost function in machine learning."
    },
    {
      "id": "Stochastic Gradient Descent",
      "type": "subnode",
      "parent": "Gradient Descent Methods",
      "description": "Algorithm that updates parameters using a single or subset of data points at each step."
    },
    {
      "id": "Learning Rate Decay",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent",
      "description": "Decreases learning rate over time to ensure convergence."
    },
    {
      "id": "Normal Equations Method",
      "type": "major",
      "parent": null,
      "description": "Direct method for minimizing cost function without iteration."
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "General concepts in machine learning including probabilistic models and optimization techniques."
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Model fails to capture the underlying trend of data due to oversimplification."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Model performs well on training data but poorly on unseen data due to excessive complexity."
    },
    {
      "id": "Locally Weighted Linear Regression (LWR)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Regression algorithm that assigns more weight to nearby points when making predictions."
    },
    {
      "id": "Maximum Likelihood Estimation (MLE)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Estimating model parameters to maximize the probability of observed data."
    },
    {
      "id": "Least Squares Regression",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Method for fitting a linear model by minimizing squared errors."
    },
    {
      "id": "Probabilistic Assumptions",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation (MLE)",
      "description": "Assumptions about the probabilistic nature of classification outcomes."
    },
    {
      "id": "Cost Function J(\u03b8)",
      "type": "subnode",
      "parent": "Least Squares Regression",
      "description": "Sum of squared errors between predicted and actual values."
    },
    {
      "id": "Locally Weighted Linear Regression (LWLR)",
      "type": "major",
      "parent": null,
      "description": "Regression method that uses local data points to fit a model."
    },
    {
      "id": "Learning a model for an MDP",
      "type": "major",
      "parent": null,
      "description": "Techniques for estimating the transition and reward dynamics of an environment from data."
    },
    {
      "id": "Continuous state MDPs",
      "type": "subnode",
      "parent": "Learning a model for an MDP",
      "description": "Extension of reinforcement learning to environments with continuous states."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Process of converting continuous-valued attributes into discrete categories for use in Naive Bayes."
    },
    {
      "id": "Value function approximation",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Approaches to approximate value functions in environments with large or infinite state spaces."
    },
    {
      "id": "Connections between Policy and Value Iteration (Optional)",
      "type": "subnode",
      "parent": "Learning a model for an MDP",
      "description": "Analysis of the relationship and differences between policy iteration and value iteration methods."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": null,
      "description": "Advanced topics in control theory applied to reinforcement learning problems."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Formulation of Markov decision processes with a fixed time horizon for planning and control tasks."
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Special case of finite-horizon setting with linear transitions and quadratic rewards."
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Approaches to apply LQR to nonlinear systems"
    },
    {
      "id": "Linearization of dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Approximating nonlinear dynamics with linear models"
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "A method to solve trajectory optimization problems by discretizing the path into steps."
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Control strategy combining LQR with stochastic models"
    },
    {
      "id": "Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Model-free algorithm for learning policies without value functions."
    },
    {
      "id": "Supervised Learning Examples",
      "type": "major",
      "parent": null,
      "description": "Introduction to supervised learning through examples"
    },
    {
      "id": "Normal Equations",
      "type": "subnode",
      "parent": "Cost Function",
      "description": "Direct method for finding the optimal parameters that minimize the cost function without iteration."
    },
    {
      "id": "Matrix Operations",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Use of matrices and vectors in representing linear regression problems."
    },
    {
      "id": "Vector Calculus",
      "type": "subnode",
      "parent": "Cost Function",
      "description": "Application of calculus to find the minimum of a function, specifically in the context of machine learning optimization."
    },
    {
      "id": "Machine Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning algorithms including gradient descent and stochastic methods."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "A variant of linear regression where weights are assigned based on proximity to the query point."
    },
    {
      "id": "Weight Calculation",
      "type": "subnode",
      "parent": "Locally Weighted Linear Regression",
      "description": "Method for calculating weights used in locally weighted linear regression."
    },
    {
      "id": "Bandwidth Parameter (\u03c4)",
      "type": "subnode",
      "parent": "Weight Calculation",
      "description": "Parameter controlling the influence of training examples based on their distance from the query point."
    },
    {
      "id": "Probability Distribution",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Distribution of y given x and parameters \u03b8."
    },
    {
      "id": "Design Matrix X",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Matrix containing all input variables x(i)."
    },
    {
      "id": "Likelihood Function",
      "type": "subnode",
      "parent": "Probability Distribution",
      "description": "Function used in EM algorithm to calculate the probability of observed data given parameters."
    },
    {
      "id": "Log Likelihood Function",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation (MLE)",
      "description": "Function representing the log likelihood for a given set of data and parameter matrix W."
    },
    {
      "id": "Convex Quadratic Function",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Type of function that has only one global minimum and no local minima, ensuring gradient descent converges to the optimal solution."
    },
    {
      "id": "Optimization Problem",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Formulation to minimize misclassification while allowing some errors through \\(\\xi_{i}\\)."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "General introduction to machine learning concepts and algorithms."
    },
    {
      "id": "Non-parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms where the number of parameters grows with data size."
    },
    {
      "id": "Parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms with a fixed number of parameters independent of data size."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Classification problems where output is binary, using logistic function for probability estimation."
    },
    {
      "id": "Logistic Regression",
      "type": "major",
      "parent": null,
      "description": "A statistical method for binary classification problems, modeling the probability of an event by fitting data to a logistic curve."
    },
    {
      "id": "Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from and make predictions on data."
    },
    {
      "id": "Sigmoid Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Function mapping real values to [0,1] used in logistic regression."
    },
    {
      "id": "Hypothesis Representation",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Formulation of hypothesis function h_theta(x) using sigmoid function."
    },
    {
      "id": "Derivative of Sigmoid Function",
      "type": "subnode",
      "parent": "Sigmoid Function",
      "description": "Calculation showing g'(z)=g(z)(1-g(z))."
    },
    {
      "id": "Parameter Fitting",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Process of fitting parameters theta using maximum likelihood estimation."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning models including GLMs, OLS, and logistic regression."
    },
    {
      "id": "Classification Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Model predicting categorical outcomes based on input features."
    },
    {
      "id": "Log-Likelihood",
      "type": "subnode",
      "parent": "Likelihood Function",
      "description": "Natural logarithm of the likelihood function for easier computation."
    },
    {
      "id": "Gradient Ascent",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation (MLE)",
      "description": "Optimization technique to find the maximum value of a function by moving in the direction of steepest ascent."
    },
    {
      "id": "Gradient Ascent Rule",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Rule for updating parameters in logistic regression based on gradient ascent."
    },
    {
      "id": "Logistic Loss Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Function that measures the performance of a classification model where labels are binary or multiclass."
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Measure of model fit in probabilistic models, often used in maximum likelihood estimation."
    },
    {
      "id": "Chain Rule Application",
      "type": "subnode",
      "parent": "Gradient Ascent Rule",
      "description": "Application of the chain rule to derive parameter updates for logistic regression."
    },
    {
      "id": "Logistic Regression Derivation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Derivation and understanding of logistic regression equations."
    },
    {
      "id": "Perceptron Learning Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Historical algorithm for binary classification with a threshold function."
    },
    {
      "id": "Multi-class Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Classification problems where the response variable can take on multiple values."
    },
    {
      "id": "Classification",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Technique for predicting categorical response variables"
    },
    {
      "id": "Multinomial Distribution",
      "type": "subnode",
      "parent": "Multi-class Classification",
      "description": "Probability distribution over multiple discrete outcomes"
    },
    {
      "id": "Softmax Function",
      "type": "subnode",
      "parent": "Multi-class Classification",
      "description": "Function used to convert logits into a probability distribution over classes."
    },
    {
      "id": "Supervised learning",
      "type": "major",
      "parent": null,
      "description": "Learning with labeled data to predict outcomes."
    },
    {
      "id": "Linear regression",
      "type": "subnode",
      "parent": "Supervised learning",
      "description": "Predicting continuous values using linear functions."
    },
    {
      "id": "LMS algorithm",
      "type": "subnode",
      "parent": "Linear regression",
      "description": "Learning the parameters of a model by minimizing squared error."
    },
    {
      "id": "The normal equations",
      "type": "subnode",
      "parent": "Linear regression",
      "description": "Directly solving for optimal parameters without iterative methods."
    },
    {
      "id": "Matrix derivatives",
      "type": "subnode",
      "parent": "The normal equations",
      "description": "Derivatives of matrix expressions used in optimization."
    },
    {
      "id": "Least squares revisited",
      "type": "subnode",
      "parent": "The normal equations",
      "description": "Revisiting the least squares method for solving linear regression."
    },
    {
      "id": "Probabilistic interpretation",
      "type": "subnode",
      "parent": "Linear regression",
      "description": "Interpreting linear regression from a probabilistic perspective."
    },
    {
      "id": "Locally weighted linear regression",
      "type": "subnode",
      "parent": "Linear regression",
      "description": "A variant of linear regression that uses local data points for prediction."
    },
    {
      "id": "Classification and logistic regression",
      "type": "major",
      "parent": null,
      "description": "Classifying data into discrete categories using probabilistic models."
    },
    {
      "id": "Logistic regression",
      "type": "subnode",
      "parent": "Classification and logistic regression",
      "description": "Modeling the probability of a binary outcome using logistic functions."
    },
    {
      "id": "Perceptron learning algorithm",
      "type": "subnode",
      "parent": "Classification and logistic regression",
      "description": "A simple algorithm for binary classification tasks."
    },
    {
      "id": "Multi-class classification",
      "type": "subnode",
      "parent": "Classification and logistic regression",
      "description": "Extending binary classification to multiple classes."
    },
    {
      "id": "Maximizing ell(theta)",
      "type": "subnode",
      "parent": "Classification and logistic regression",
      "description": "Techniques for maximizing the likelihood function in classification models."
    },
    {
      "id": "Generalized linear models",
      "type": "major",
      "parent": null,
      "description": "A flexible generalization of ordinary linear regression to non-linear relationships."
    },
    {
      "id": "Exponential family",
      "type": "subnode",
      "parent": "Generalized linear models",
      "description": "A class of probability distributions that can be expressed in a common form."
    },
    {
      "id": "Constructing GLMs",
      "type": "subnode",
      "parent": "Generalized linear models",
      "description": "Methods for constructing generalized linear models from exponential family distributions."
    },
    {
      "id": "Ordinary least squares",
      "type": "subnode",
      "parent": "Constructing GLMs",
      "description": "A method of estimating the parameters in a linear regression model."
    },
    {
      "id": "Logistic regression (GLM)",
      "type": "subnode",
      "parent": "Constructing GLMs",
      "description": "An example of generalized linear models for binary classification."
    },
    {
      "id": "Generative learning algorithms",
      "type": "major",
      "parent": null,
      "description": "Algorithms that model the joint probability distribution of features and labels."
    },
    {
      "id": "Gaussian discriminant analysis",
      "type": "subnode",
      "parent": "Generative learning algorithms",
      "description": "A generative classification algorithm based on Gaussian distributions."
    },
    {
      "id": "Multivariate normal distribution",
      "type": "subnode",
      "parent": "Gaussian discriminant analysis",
      "description": "A generalization of the univariate normal distribution to multiple dimensions."
    },
    {
      "id": "The Gaussian discriminant analysis model",
      "type": "subnode",
      "parent": "Gaussian discriminant analysis",
      "description": "Modeling the joint probability distribution using multivariate Gaussians."
    },
    {
      "id": "Discussion: GDA and logistic regression",
      "type": "subnode",
      "parent": "Gaussian discriminant analysis",
      "description": "Comparative discussion between Gaussian Discriminant Analysis and Logistic Regression."
    },
    {
      "id": "Naive bayes (Option Reading)",
      "type": "subnode",
      "parent": "Generative learning algorithms",
      "description": "A simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions."
    },
    {
      "id": "Laplace smoothing",
      "type": "subnode",
      "parent": "Naive bayes (Option Reading)",
      "description": "Technique to handle zero probabilities in the Naive Bayes model."
    },
    {
      "id": "Event models for text classification",
      "type": "subnode",
      "parent": "Naive bayes (Option Reading)",
      "description": "Models used in naive Bayes classifiers for classifying documents based on word occurrences."
    },
    {
      "id": "Kernel methods",
      "type": "major",
      "parent": null,
      "description": "Techniques that use a kernel function to compute the similarity between data points in high-dimensional space."
    },
    {
      "id": "Feature maps",
      "type": "subnode",
      "parent": "Kernel methods",
      "description": "Mapping of input vectors into a higher dimensional feature space."
    },
    {
      "id": "LMS with features",
      "type": "subnode",
      "parent": "Kernel methods",
      "description": "Least mean squares algorithm applied to transformed data using feature maps."
    },
    {
      "id": "LMS with the kernel trick",
      "type": "subnode",
      "parent": "Kernel methods",
      "description": "Efficient computation of LMS in high-dimensional space using kernels."
    },
    {
      "id": "Properties of kernels",
      "type": "subnode",
      "parent": "Kernel methods",
      "description": "Mathematical properties and requirements for kernel functions."
    },
    {
      "id": "Support vector machines",
      "type": "major",
      "parent": null,
      "description": "Algorithms that find the hyperplane in an N-dimensional space that distinctly classifies data points."
    },
    {
      "id": "Margins: intuition",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Understanding the concept of margins in SVMs."
    },
    {
      "id": "Notation (option reading)",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Mathematical notation used in support vector machine formulations."
    },
    {
      "id": "Functional and geometric margins",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Different types of margins used in SVMs to measure the distance from a decision boundary."
    },
    {
      "id": "Optimal margin classifier (option reading)",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Finding the hyperplane that maximizes the margin between classes."
    },
    {
      "id": "Lagrange duality (optional reading)",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Using Lagrangian multipliers to solve optimization problems in SVMs."
    },
    {
      "id": "Dual form of optimal margin classifiers",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Formulation of the dual problem for finding support vectors."
    },
    {
      "id": "Regularization and non-separable case",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Handling cases where data is not linearly separable by introducing slack variables."
    },
    {
      "id": "SMO algorithm (optional reading)",
      "type": "subnode",
      "parent": "Support vector machines",
      "description": "Sequential minimal optimization for solving the quadratic programming problem in SVMs."
    },
    {
      "id": "Coordinate ascent",
      "type": "subnode",
      "parent": "SMO algorithm (optional reading)",
      "description": "Optimization technique used within SMO to update Lagrange multipliers."
    },
    {
      "id": "SMO",
      "type": "subnode",
      "parent": "SMO algorithm (optional reading)",
      "description": "Algorithm for efficiently solving the SVM optimization problem using coordinate ascent."
    },
    {
      "id": "Deep learning",
      "type": "major",
      "parent": null,
      "description": "A class of machine learning algorithms that use neural networks with many layers to learn representations of data."
    },
    {
      "id": "Supervised learning with non-linear models",
      "type": "subnode",
      "parent": "Deep learning",
      "description": "Using deep neural networks for supervised learning tasks involving complex, non-linear relationships."
    },
    {
      "id": "Neural networks",
      "type": "subnode",
      "parent": "Deep learning",
      "description": "Networks of interconnected nodes that process information in a manner inspired by biological neurons."
    },
    {
      "id": "Modules in modern neural networks",
      "type": "subnode",
      "parent": "Deep learning",
      "description": "Building blocks and components used to construct deep neural network architectures."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Deep learning",
      "description": "Algorithm for efficiently computing gradients in multi-layered networks for training purposes."
    },
    {
      "id": "Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to the mathematical concepts required for understanding backpropagation."
    },
    {
      "id": "General strategy of backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the process for computing gradients in neural networks using backpropagation."
    },
    {
      "id": "Backward functions for basic modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Details on how backward functions are implemented for fundamental network components."
    },
    {
      "id": "Back-propagation for MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of computing gradients through layers of an MLP using backward functions."
    },
    {
      "id": "Loss Functions",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Functions used to quantify the discrepancy between predicted values and actual outcomes."
    },
    {
      "id": "Cross-Entropy Loss",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "Loss function used in classification to evaluate the probability assigned to a correct label."
    },
    {
      "id": "Gradient Calculation",
      "type": "subnode",
      "parent": "Cross-Entropy Loss",
      "description": "Calculating gradients of loss function with respect to intermediate variables and parameters."
    },
    {
      "id": "Logits",
      "type": "subnode",
      "parent": "Softmax Function",
      "description": "Inputs to the softmax function, often used in classification tasks."
    },
    {
      "id": "Probability Vector",
      "type": "subnode",
      "parent": "Softmax Function",
      "description": "Output of the softmax function representing probabilities."
    },
    {
      "id": "Loss Function Gradient Calculation",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Calculating gradients of loss functions for optimization."
    },
    {
      "id": "Cross Entropy Loss",
      "type": "subnode",
      "parent": "Loss Function Gradient Calculation",
      "description": "Derivation and gradient calculation for cross entropy loss function."
    },
    {
      "id": "Newton's Method in ML",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Application of Newton's method for finding zeros of functions in machine learning context."
    },
    {
      "id": "Newton's Method",
      "type": "major",
      "parent": null,
      "description": "An iterative method for finding roots of a real-valued function."
    },
    {
      "id": "Finding Roots",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "The process of determining the value of \u03b8 where f(\u03b8) = 0."
    },
    {
      "id": "Maximizing Functions",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Using Newton's method to find maxima by setting first derivative to zero."
    },
    {
      "id": "Hessian Matrix",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Matrix of second-order partial derivatives used to generalize Newton's method."
    },
    {
      "id": "Convergence Rate",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Typically faster convergence compared to gradient descent, requiring fewer iterations."
    },
    {
      "id": "Optimization Methods",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Methods for stochastic optimization in machine learning models."
    },
    {
      "id": "Fisher Scoring",
      "type": "subnode",
      "parent": "Newton's Method",
      "description": "Application of Newton's method to logistic regression likelihood function."
    },
    {
      "id": "Generalized Linear Models (GLMs)",
      "type": "major",
      "parent": null,
      "description": "Models that extend linear regression to accommodate non-normal distributions and non-linear relationships."
    },
    {
      "id": "Exponential Family Distributions",
      "type": "subnode",
      "parent": "Generalized Linear Models (GLMs)",
      "description": "Family of distributions that includes Bernoulli, Gaussian, Poisson, etc., and can be expressed in a specific form."
    },
    {
      "id": "Bernoulli Distribution",
      "type": "subnode",
      "parent": "Generalized Linear Models (GLMs)",
      "description": "Binary distribution used in logistic regression as part of GLM framework."
    },
    {
      "id": "Poisson Distribution",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Discrete distribution suitable for modeling count data such as customer arrivals or page views."
    },
    {
      "id": "Generalized Linear Model (GLM)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A statistical model that extends linear models to accommodate non-normal distributions."
    },
    {
      "id": "Conditional Distribution Assumptions",
      "type": "subnode",
      "parent": "Generalized Linear Model (GLM)",
      "description": "Assumptions about the conditional distribution of y given x in GLMs."
    },
    {
      "id": "Natural Parameter and Inputs Relationship",
      "type": "subnode",
      "parent": "Conditional Distribution Assumptions",
      "description": "Linear relationship between natural parameter eta and inputs x."
    },
    {
      "id": "Natural Parameter (\u03b7)",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Canonical parameter defining a distribution within the exponential family."
    },
    {
      "id": "Sufficient Statistic (T(y))",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Statistic that captures all information about the parameter from the data."
    },
    {
      "id": "Log Partition Function (a(\u03b7))",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Function ensuring normalization of probability distribution."
    },
    {
      "id": "Natural Parameter for Bernoulli (\u03b7)",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "Logarithm of the odds ratio, \u03b7 = log(\u03c6/(1-\u03c6))."
    },
    {
      "id": "Sufficient Statistic for Bernoulli (T(y))",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "Outcome y itself."
    },
    {
      "id": "Log Partition Function for Bernoulli (a(\u03b7))",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "Function a(\u03b7) = log(1 + e^\u03b7)."
    },
    {
      "id": "Normalization Constant",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Ensures the distribution sums/integrates to 1 over y."
    },
    {
      "id": "Ordinary Least Squares (OLS)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Special case of GLMs for continuous target variables."
    },
    {
      "id": "Assumptions/Design Choices",
      "type": "subnode",
      "parent": "Generalized Linear Models (GLMs)",
      "description": "Three assumptions/design choices that lead to GLM derivation."
    },
    {
      "id": "Response Variable",
      "type": "subnode",
      "parent": "Ordinary Least Squares (OLS)",
      "description": "Continuous target variable modeled as Gaussian distribution."
    },
    {
      "id": "Conditional Distribution Modeling",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Modeling the distribution of y given x."
    },
    {
      "id": "Logistic Function",
      "type": "subnode",
      "parent": "Bernoulli Distribution",
      "description": "Function used to map real values to probabilities for binary classification."
    },
    {
      "id": "Canonical Response Function",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Maps natural parameter to distribution's mean."
    },
    {
      "id": "Canonical Link Function",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Inverse of the canonical response function, maps mean to natural parameter."
    },
    {
      "id": "Generative Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Models that generate data based on learned parameters and probabilities."
    },
    {
      "id": "Gaussian Family",
      "type": "subnode",
      "parent": "Exponential Family Distributions",
      "description": "Family of distributions with the identity function as canonical response."
    },
    {
      "id": "Discriminative Learning",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Algorithms that learn p(y|x) directly or mappings from inputs to labels."
    },
    {
      "id": "Generative Learning",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Algorithms that model p(x|y) and p(y)."
    },
    {
      "id": "Conditional Distribution",
      "type": "subnode",
      "parent": "Discriminative Learning",
      "description": "Distribution of y given x, modeled directly by discriminative algorithms."
    },
    {
      "id": "Perceptron Algorithm",
      "type": "subnode",
      "parent": "Discriminative Learning",
      "description": "Algorithm that finds decision boundaries for classification tasks."
    },
    {
      "id": "Class Priors",
      "type": "subnode",
      "parent": "Generative Learning",
      "description": "Probability of each class before considering any features."
    },
    {
      "id": "p(x|y)",
      "type": "subnode",
      "parent": "Generative Learning",
      "description": "Conditional probability distribution of x given y, models the distribution of features for each class."
    },
    {
      "id": "Bayes Rule",
      "type": "subnode",
      "parent": "Generative Learning",
      "description": "Rule used to derive posterior distribution p(y|x) from p(x|y) and p(y)."
    },
    {
      "id": "Modern Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Overview of modules and backpropagation techniques in neural networks."
    },
    {
      "id": "Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Describes building blocks and composition methods used in modern neural networks."
    },
    {
      "id": "Vectorization over training examples",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Techniques for efficiently processing multiple training samples simultaneously."
    },
    {
      "id": "Generalization and regularization",
      "type": "major",
      "parent": null,
      "description": "Focuses on methods to improve model performance on unseen data through generalization and regularization techniques."
    },
    {
      "id": "Generalization",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Explores concepts related to improving a model's ability to generalize from training data to new, unseen data."
    },
    {
      "id": "Bias-variance tradeoff",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Explains the balance between model complexity and error due to overfitting or underfitting."
    },
    {
      "id": "A mathematical decomposition (for regression)",
      "type": "subnode",
      "parent": "Bias-variance tradeoff",
      "description": "Provides a detailed breakdown of bias and variance in the context of regression models."
    },
    {
      "id": "The double descent phenomenon",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Discusses an empirical observation about model performance as complexity increases."
    },
    {
      "id": "Sample complexity bounds (optional readings)",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Analyzes the number of samples required for a model to achieve good generalization."
    },
    {
      "id": "Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Covers techniques used to prevent overfitting by penalizing overly complex models."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Technique used in machine learning to control the complexity of models and avoid overfitting."
    },
    {
      "id": "Implicit regularization effect (optional reading)",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Examines how certain training methods inherently regularize the learning process."
    },
    {
      "id": "Model selection via cross validation",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Describes a method for choosing between different models based on their performance on validation data."
    },
    {
      "id": "Bayesian statistics and regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Links Bayesian approaches to the concept of regularization in machine learning."
    },
    {
      "id": "Unsupervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning without labeled data to discover hidden structure in data."
    },
    {
      "id": "Clustering and k-means algorithm",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Introduction to clustering methods with a focus on the k-means algorithm."
    },
    {
      "id": "EM algorithms",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Explains Expectation-Maximization algorithms used in unsupervised learning scenarios."
    },
    {
      "id": "EM for mixture of Gaussians",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Application of EM to model data as a mixture of Gaussian distributions."
    },
    {
      "id": "Jensen's inequality",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Mathematical principle used in the derivation and understanding of EM algorithms."
    },
    {
      "id": "General EM algorithms",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Overview of more general forms of the Expectation-Maximization algorithm."
    },
    {
      "id": "Other interpretation of ELBO",
      "type": "subnode",
      "parent": "General EM algorithms",
      "description": "Alternative perspectives on the Evidence Lower Bound (ELBO) in variational inference."
    },
    {
      "id": "Mixture of Gaussians revisited",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Re-examination of mixture models with Gaussian components using EM."
    },
    {
      "id": "Variational inference and variational auto-encoder (optional reading)",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Advanced topics in probabilistic modeling including VAEs."
    },
    {
      "id": "Principal components analysis",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Dimensionality reduction technique that projects data onto a lower-dimensional space."
    },
    {
      "id": "Independent components analysis",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Technique for separating mixed signals into independent, non-Gaussian components."
    },
    {
      "id": "ICA ambiguities",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Discussion on the inherent limitations and challenges in ICA."
    },
    {
      "id": "Densities and linear transformations",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Analysis of how densities change under linear transformations in the context of ICA."
    },
    {
      "id": "ICA algorithm",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Detailed explanation and implementation of the Independent Components Analysis method."
    },
    {
      "id": "Self-supervised learning and foundation models",
      "type": "major",
      "parent": null,
      "description": "Techniques for training models on large datasets without explicit supervision."
    },
    {
      "id": "Pretraining and adaptation",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Overview of pretraining methods followed by fine-tuning or adapting to specific tasks."
    },
    {
      "id": "Pretraining methods in computer vision",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Specific approaches used for unsupervised learning in the field of image processing."
    },
    {
      "id": "Pretrained large language models",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Discussion on training and using large-scale language models with self-supervision."
    },
    {
      "id": "Open up the blackbox of Transformers",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Detailed look at the architecture and workings of Transformer-based models."
    },
    {
      "id": "Zero-shot learning and in-context learning",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Exploration of capabilities to perform tasks without explicit training data."
    },
    {
      "id": "Reinforcement Learning and Control",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning optimal policies through interaction with an environment."
    },
    {
      "id": "Reinforcement learning",
      "type": "subnode",
      "parent": "Reinforcement Learning and Control",
      "description": "Introduction to the field of reinforcement learning including key concepts and algorithms."
    },
    {
      "id": "Markov decision processes",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Foundation for understanding reinforcement learning problems through state transitions and rewards."
    },
    {
      "id": "Value iteration and policy iteration",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Algorithms used to find optimal policies in MDPs by iteratively improving value functions or policies."
    },
    {
      "id": "Probability Distributions",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Types of distributions used in ML."
    },
    {
      "id": "Mean",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Expected value of a Gaussian variable."
    },
    {
      "id": "Covariance Matrix",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Measure of how much two random variables change together."
    },
    {
      "id": "Standard Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Special case with mean 0 and identity covariance matrix."
    },
    {
      "id": "Density Visualization",
      "type": "subnode",
      "parent": "Gaussian Distribution",
      "description": "Graphical representation of Gaussian densities."
    },
    {
      "id": "Multivariate Normal Distribution",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Distribution parameterized by mean vector and covariance matrix."
    },
    {
      "id": "Covariance Matrix Impact",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Effect of covariance matrix on density contours."
    },
    {
      "id": "Mean Vector Influence",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Impact of mean vector changes on distribution location."
    },
    {
      "id": "Gaussian Discriminant Analysis (GDA)",
      "type": "major",
      "parent": null,
      "description": "A probabilistic model that assumes the data is generated from a Gaussian distribution with class-specific means but shared covariance matrix."
    },
    {
      "id": "Bayesian Classification",
      "type": "major",
      "parent": null,
      "description": "Classification using Bayes' theorem to calculate posterior probabilities."
    },
    {
      "id": "Conditional Probability",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "Probability of an event given that another event has occurred, central to Naive Bayes calculations."
    },
    {
      "id": "Posterior Distribution",
      "type": "subnode",
      "parent": "Bayesian Classification",
      "description": "The conditional probability distribution of latent variables given observed data and current parameters."
    },
    {
      "id": "Mean Vector",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Vector representing the average value of each feature in a dataset."
    },
    {
      "id": "Density Function",
      "type": "subnode",
      "parent": "Multivariate Normal Distribution",
      "description": "Function describing the probability density over a range of values."
    },
    {
      "id": "Decision Boundaries",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Boundaries that separate different classes in feature space."
    },
    {
      "id": "Model Assumptions",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Theoretical assumptions made by models about the data distribution."
    },
    {
      "id": "Asymptotic Efficiency",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Property of GDA when modeling assumptions are correct, leading to optimal performance with large datasets."
    },
    {
      "id": "Assumptions and Data Efficiency",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Discusses assumptions made by GDA and its efficiency with correct assumptions."
    },
    {
      "id": "Robustness of Logistic Regression",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Explains logistic regression's robustness to deviations from modeling assumptions."
    },
    {
      "id": "Naive Bayes Algorithm",
      "type": "major",
      "parent": null,
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "Model Parameters",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Parameters like phi, mu0, mu1, and Sigma that define the model's structure."
    },
    {
      "id": "Log-Likelihood Function",
      "type": "subnode",
      "parent": "Model Parameters",
      "description": "Function used to estimate parameters by maximizing likelihood of observed data given the model."
    },
    {
      "id": "Decision Boundary",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis (GDA)",
      "description": "Line that separates regions where different classes are predicted based on input features x."
    },
    {
      "id": "Machine_Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from data."
    },
    {
      "id": "Text_Classification",
      "type": "subnode",
      "parent": "Machine_Learning",
      "description": "Process of categorizing text into predefined classes."
    },
    {
      "id": "Spam_Filtering",
      "type": "subnode",
      "parent": "Text_Classification",
      "description": "Application of machine learning to classify emails as spam or non-spam."
    },
    {
      "id": "Feature_Vector",
      "type": "subnode",
      "parent": "Spam_Filtering",
      "description": "Representation of an email using a binary vector indicating presence of words in the vocabulary."
    },
    {
      "id": "Vocabulary",
      "type": "subnode",
      "parent": "Feature_Vector",
      "description": "Set of unique words used to represent emails as feature vectors."
    },
    {
      "id": "Stop_Words",
      "type": "subnode",
      "parent": "Vocabulary",
      "description": "Commonly excluded high-frequency words that do not contribute meaningful content."
    },
    {
      "id": "Feature Vector Selection",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of selecting relevant features for a model."
    },
    {
      "id": "Generative Models",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Models that generate data based on learned parameters."
    },
    {
      "id": "Naive Bayes Assumption",
      "type": "subnode",
      "parent": "Generative Models",
      "description": "Assumption of conditional independence in Naive Bayes classifiers."
    },
    {
      "id": "Stop Words",
      "type": "subnode",
      "parent": "Feature Vector Selection",
      "description": "High-frequency words excluded from analysis due to lack of content relevance."
    },
    {
      "id": "Naive Bayes Classifier",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "A probabilistic classifier based on applying Bayes' theorem with strong independence assumptions between the features."
    },
    {
      "id": "Spam Filtering",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Application of machine learning to distinguish spam emails from legitimate ones."
    },
    {
      "id": "NeurIPS Conference",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Top-tier academic conference for presenting research in neural networks and machine learning."
    },
    {
      "id": "Parameter Estimation",
      "type": "subnode",
      "parent": "Naive Bayes Classifier",
      "description": "Process of estimating parameters like \u03c6\u208a|y=1, \u03c6\u209b|y=0, and \u03c6\u209d from training data."
    },
    {
      "id": "Binary Features",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Features that can take only two values, typically 0 and 1, used in basic Naive Bayes formulation."
    },
    {
      "id": "Multinomial Features",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Extension to handle features with multiple discrete values."
    },
    {
      "id": "Laplace Smoothing",
      "type": "major",
      "parent": null,
      "description": "Technique to improve the performance of Naive Bayes by addressing zero probability problems."
    },
    {
      "id": "Spam Classification Example",
      "type": "subnode",
      "parent": "Laplace Smoothing",
      "description": "Illustration of how Laplace smoothing can be applied in email spam detection."
    },
    {
      "id": "Bayesian Inference",
      "type": "subnode",
      "parent": "Naive Bayes Algorithm",
      "description": "Statistical method for updating the probability estimate for a hypothesis as more evidence or information becomes available."
    },
    {
      "id": "Maximum Likelihood Estimation",
      "type": "subnode",
      "parent": "Parameter Estimation",
      "description": "Method for estimating the parameters of a model by maximizing the likelihood function based on observed data."
    },
    {
      "id": "Bernoulli Event Model",
      "type": "subnode",
      "parent": "Naive Bayes Classifier",
      "description": "Model for text classification assuming binary presence or absence of words."
    },
    {
      "id": "Multinomial Event Model",
      "type": "subnode",
      "parent": "Generative Learning Algorithms",
      "description": "Model for generating emails based on spam/non-spam classification and word distribution."
    },
    {
      "id": "Probability Estimation",
      "type": "major",
      "parent": null,
      "description": "Discusses the statistical issues with estimating probabilities as zero."
    },
    {
      "id": "Multinomial Random Variable",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "A random variable taking values in a finite set of outcomes."
    },
    {
      "id": "Maximum Likelihood Estimates",
      "type": "subnode",
      "parent": "Probability Estimation",
      "description": "Estimates for parameters based on observed data, can result in zero probabilities."
    },
    {
      "id": "Event Models for Text Classification",
      "type": "major",
      "parent": null,
      "description": "Models used to classify text based on event occurrences."
    },
    {
      "id": "Kernel Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques to extend algorithms to high-dimensional spaces without explicit computation."
    },
    {
      "id": "Feature Maps",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Transformation of input data into a higher-dimensional space to fit non-linear models using linear techniques."
    },
    {
      "id": "Spam/Non-Spam Classification",
      "type": "subnode",
      "parent": "Multinomial Event Model",
      "description": "Determines the type of email before generating content."
    },
    {
      "id": "Word Generation Process",
      "type": "subnode",
      "parent": "Multinomial Event Model",
      "description": "Process for selecting words independently from a multinomial distribution."
    },
    {
      "id": "Probability Calculation",
      "type": "subnode",
      "parent": "Multinomial Event Model",
      "description": "Calculates the overall probability of an email message based on word and type probabilities."
    },
    {
      "id": "Training Set",
      "type": "subnode",
      "parent": "Multinomial Event Model",
      "description": "Collection of training examples used to estimate model parameters."
    },
    {
      "id": "Linear Function Over Features",
      "type": "major",
      "parent": null,
      "description": "Describes a linear function over transformed feature variables."
    },
    {
      "id": "Feature Map Definition",
      "type": "subnode",
      "parent": "Linear Function Over Features",
      "description": "Definition of the transformation from attributes to features using \u03c6(x)."
    },
    {
      "id": "Cubic Function Example",
      "type": "subnode",
      "parent": "Linear Function Over Features",
      "description": "Example showing how a cubic function can be expressed as a linear combination of transformed variables."
    },
    {
      "id": "LMS with Features",
      "type": "major",
      "parent": null,
      "description": "Derivation and explanation of the LMS algorithm for fitting models using feature maps."
    },
    {
      "id": "Gradient Descent Update Rule",
      "type": "subnode",
      "parent": "LMS with Features",
      "description": "Update rule for gradient descent in the context of high-dimensional feature maps."
    },
    {
      "id": "High-Dimensional Feature Maps",
      "type": "subnode",
      "parent": "Gradient Descent Update Rule",
      "description": "Feature maps in high-dimensional spaces leading to computational challenges."
    },
    {
      "id": "Kernel Trick",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Technique used to handle high-dimensional feature spaces efficiently without explicitly computing them."
    },
    {
      "id": "Feature Mapping",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Alternative method of testing if a given function is a valid kernel through feature mapping."
    },
    {
      "id": "Iterative Update Process",
      "type": "subnode",
      "parent": "Feature Mapping",
      "description": "Process of updating coefficients iteratively based on input vectors."
    },
    {
      "id": "Theta Representation",
      "type": "subnode",
      "parent": "Iterative Update Process",
      "description": "Representation of theta as a linear combination of transformed inputs."
    },
    {
      "id": "Update Rule for Beta",
      "type": "subnode",
      "parent": "Iterative Update Process",
      "description": "Rule defining how coefficients beta are updated in each iteration."
    },
    {
      "id": "Beta Update Equation",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Equation describing how \u03b2 is updated in each iteration."
    },
    {
      "id": "Feature Map Phi",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Mapping from input space to feature space."
    },
    {
      "id": "Inner Product Computation",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Efficient computation of inner products in high-dimensional spaces."
    },
    {
      "id": "Pre-Computation Strategy",
      "type": "subnode",
      "parent": "Batch Gradient Descent",
      "description": "Technique to pre-compute pairwise inner products before iterations."
    },
    {
      "id": "Efficient Inner Product Calculation",
      "type": "subnode",
      "parent": "Inner Product Computation",
      "description": "Method to compute \u03c6(x),\u03c6(z) efficiently without explicit feature map computation."
    },
    {
      "id": "Feature Maps and Kernels",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on feature maps and their corresponding kernel functions."
    },
    {
      "id": "Kernel Function Definition",
      "type": "subnode",
      "parent": "Feature Maps and Kernels",
      "description": "Definition of the kernel function as an inner product in a transformed space."
    },
    {
      "id": "Efficient Computation with Kernels",
      "type": "subnode",
      "parent": "Feature Maps and Kernels",
      "description": "Algorithm for efficient computation using kernels to update model parameters."
    },
    {
      "id": "Properties of Kernels",
      "type": "major",
      "parent": null,
      "description": "Exploration of fundamental properties and characteristics of kernel functions in machine learning."
    },
    {
      "id": "Kernel Functions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Functions used to measure similarity between data points in a high-dimensional space."
    },
    {
      "id": "Explicit Definition of Feature Map",
      "type": "subnode",
      "parent": "Feature Maps",
      "description": "Definition of feature map in relation to kernels."
    },
    {
      "id": "Intrinsic Nature of Kernel Function",
      "type": "subnode",
      "parent": "Properties of Kernels",
      "description": "Kernel function's independence from explicit feature maps."
    },
    {
      "id": "Characterization of Valid Kernels",
      "type": "subnode",
      "parent": "Properties of Kernels",
      "description": "Criteria for determining if a kernel corresponds to a valid feature map."
    },
    {
      "id": "Polynomial Kernel Example",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Example of a polynomial kernel function with degree 2."
    },
    {
      "id": "Efficiency Considerations",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Discussion on computational efficiency in backpropagation through module compositions."
    },
    {
      "id": "Parameter c in Kernel Function",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Explanation of the role and impact of parameter c in polynomial kernels."
    },
    {
      "id": "Kernels as Similarity Metrics",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Using kernels to measure similarity between data points."
    },
    {
      "id": "Gaussian Kernel",
      "type": "subnode",
      "parent": "Kernels as Similarity Metrics",
      "description": "A specific kernel function used for measuring similarity in machine learning."
    },
    {
      "id": "Necessary Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Criteria that a function must meet to be considered a valid kernel."
    },
    {
      "id": "Symmetry Property",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "A valid kernel matrix is symmetric."
    },
    {
      "id": "Positive Semi-Definiteness",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "A valid kernel matrix must be positive semi-definite."
    },
    {
      "id": "Kernel Matrix",
      "type": "subnode",
      "parent": "Necessary Conditions for Valid Kernels",
      "description": "Matrix representation of a kernel function over a set of points."
    },
    {
      "id": "Sufficient Conditions for Valid Kernels",
      "type": "subnode",
      "parent": "Kernel Functions",
      "description": "Conditions that are both necessary and sufficient for a matrix to be a valid kernel matrix."
    },
    {
      "id": "Feature Construction for Strings",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Constructing features from strings using substring counts."
    },
    {
      "id": "Support Vector Machines (SVM)",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Classification algorithm that uses kernels for efficient computation in high dimensions."
    },
    {
      "id": "Kernel Matrix Properties",
      "type": "major",
      "parent": null,
      "description": "Properties of the kernel matrix in machine learning."
    },
    {
      "id": "Sufficient Conditions for Kernels",
      "type": "subnode",
      "parent": "Kernel Matrix Properties",
      "description": "Conditions under which a function is considered a valid Mercer kernel."
    },
    {
      "id": "Mercer's Theorem",
      "type": "subnode",
      "parent": "Sufficient Conditions for Kernels",
      "description": "Theorem stating necessary and sufficient conditions for a function to be a valid kernel."
    },
    {
      "id": "Kernel Examples",
      "type": "major",
      "parent": null,
      "description": "Examples of kernels used in machine learning problems."
    },
    {
      "id": "Digit Recognition Problem",
      "type": "subnode",
      "parent": "Kernel Examples",
      "description": "Application of polynomial and Gaussian kernels to digit recognition using SVMs."
    },
    {
      "id": "String Classification Example",
      "type": "subnode",
      "parent": "Kernel Examples",
      "description": "Example involving classification of strings, such as amino acid sequences."
    },
    {
      "id": "Margins",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Concept of margins in SVMs to separate data with a large gap."
    },
    {
      "id": "Optimal Margin Classifier",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Classifier that maximizes the geometric margin for linearly separable data sets."
    },
    {
      "id": "Lagrange Duality",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Mathematical concept used in deriving SVM optimization problems."
    },
    {
      "id": "Kernels",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Technique to apply SVMs efficiently in high-dimensional spaces."
    },
    {
      "id": "SMO Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Sequential Minimal Optimization algorithm for solving SVM problems."
    },
    {
      "id": "Functional Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A measure of confidence for classifications based on the distance from the decision boundary."
    },
    {
      "id": "Geometric Margins",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Another measure of confidence that considers the actual physical distance from the decision boundary."
    },
    {
      "id": "Training Data Classification",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "The process of assigning labels to training data based on learned parameters."
    },
    {
      "id": "Support Vector Machines (SVMs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Binary classification algorithm that maximizes the margin between classes."
    },
    {
      "id": "Notation for SVMs",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Introduction of notation using w, b parameters for linear classifiers."
    },
    {
      "id": "Functional Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Margin that ensures each training example has a margin at least as large as gamma."
    },
    {
      "id": "Geometric Margin",
      "type": "subnode",
      "parent": "Support Vector Machines (SVMs)",
      "description": "Measure of distance from a data point to the decision boundary considering label y."
    },
    {
      "id": "Confidence and Correct Prediction",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "A large functional margin indicates high confidence in correct predictions."
    },
    {
      "id": "Scaling w and b",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "Replacing (w,b) with scaled versions does not change the classifier's output but increases the margin."
    },
    {
      "id": "Normalization Condition",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "Imposing a condition like ||w||_2=1 to normalize w and b for meaningful margin comparison."
    },
    {
      "id": "Function Margin of Training Set",
      "type": "subnode",
      "parent": "Functional Margin",
      "description": "Smallest functional margin across all training examples in the set S."
    },
    {
      "id": "Vector w",
      "type": "subnode",
      "parent": "Decision Boundary",
      "description": "Orthogonal to the decision boundary and points towards positive class."
    },
    {
      "id": "Distance to Decision Boundary",
      "type": "subnode",
      "parent": "Decision Boundary",
      "description": "Calculated using unit vector of w and training example x."
    },
    {
      "id": "Optimization Constraints",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Constraints used to optimize model parameters such as scaling constraints on w."
    },
    {
      "id": "Linear Separability",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Property of a dataset where positive and negative examples can be separated by a hyperplane."
    },
    {
      "id": "Lagrangian",
      "type": "subnode",
      "parent": "Lagrange Duality",
      "description": "Function combining objective and constraints using Lagrange multipliers."
    },
    {
      "id": "Constrained Optimization",
      "type": "major",
      "parent": null,
      "description": "Optimization problems with equality and inequality constraints."
    },
    {
      "id": "Lagrange Multipliers",
      "type": "subnode",
      "parent": "Constrained Optimization",
      "description": "Method for finding local maxima and minima of functions subject to equality constraints."
    },
    {
      "id": "Generalized Lagrangian",
      "type": "subnode",
      "parent": "Constrained Optimization",
      "description": "Combines objective function and constraints using Lagrange multipliers."
    },
    {
      "id": "Primal Problem",
      "type": "subnode",
      "parent": "Constrained Optimization",
      "description": "Optimization problem defined by maximizing over alpha and beta while minimizing over w."
    },
    {
      "id": "Lagrange Duality Theory",
      "type": "subnode",
      "parent": "Constrained Optimization",
      "description": "Theory explaining relationships between primal and dual problems (not covered in depth)."
    },
    {
      "id": "Optimization Problem in SVM",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Formulation of the problem to maximize geometric margins."
    },
    {
      "id": "Non-Convex Constraint Issue",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Constraint ||w||=1 complicates optimization."
    },
    {
      "id": "Transformation to Convex Form",
      "type": "subnode",
      "parent": "Optimization Problem in SVM",
      "description": "Transforming problem to maximize \u0107/||w|| without non-convex constraints."
    },
    {
      "id": "Scaling Constraint",
      "type": "subnode",
      "parent": "Transformation to Convex Form",
      "description": "Constraint that functional margin must be 1, simplifying optimization."
    },
    {
      "id": "Final Optimization Problem",
      "type": "subnode",
      "parent": "Transformation to Convex Form",
      "description": "Minimizing ||w||^2 with constraints y(wTx+b) >= 1."
    },
    {
      "id": "Dual Problem",
      "type": "major",
      "parent": null,
      "description": "Optimization problem that inverts the order of maximization and minimization compared to the primal problem."
    },
    {
      "id": "Objective Function Primal",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Function \u03b8\u211aP(w) representing the objective value for the primal problem."
    },
    {
      "id": "Objective Function Dual",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Function \u03b8\u211aD(\u03b1,\u03b2) representing the objective value for the dual problem."
    },
    {
      "id": "Value of Primal Problem",
      "type": "subnode",
      "parent": "Primal Problem",
      "description": "Optimal value p* defined as minimum \u03b8\u211aP(w)."
    },
    {
      "id": "Value of Dual Problem",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Optimal value d* defined as maximum \u03b8\u211aD(\u03b1,\u03b2)."
    },
    {
      "id": "Primal-Dual Relationship",
      "type": "major",
      "parent": null,
      "description": "Relationship between the optimal values of primal and dual problems (d* <= p*)."
    },
    {
      "id": "Support Vectors",
      "type": "major",
      "parent": null,
      "description": "Points that lie on the decision boundary and affect the optimal solution."
    },
    {
      "id": "Dual Form of Problem",
      "type": "major",
      "parent": null,
      "description": "Formulation of optimization problem in terms of Lagrange multipliers."
    },
    {
      "id": "Lagrangian Function",
      "type": "subnode",
      "parent": "Dual Form of Problem",
      "description": "Function used to find the dual form by minimizing with respect to w and b."
    },
    {
      "id": "Inner Product",
      "type": "subnode",
      "parent": "Dual Form of Problem",
      "description": "Key concept in expressing algorithm terms without explicit feature space dimensions."
    },
    {
      "id": "Machine Learning Theory",
      "type": "major",
      "parent": null,
      "description": "Theoretical foundations of machine learning including generalization bounds and hypothesis classes."
    },
    {
      "id": "Optimization Problems",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Formulation and solution methods for optimization problems in ML."
    },
    {
      "id": "Duality Gap",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Difference between optimal values of primal and dual problems."
    },
    {
      "id": "Convex Functions",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Functions where the line segment between any two points on the graph of the function lies above or on the graph."
    },
    {
      "id": "KKT Conditions",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Conditions that must be satisfied for optimality in constrained optimization problems."
    },
    {
      "id": "Feasibility Constraints",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Constraints ensuring the existence of feasible solutions in optimization problems."
    },
    {
      "id": "Dual Complementarity Condition",
      "type": "subnode",
      "parent": "KKT Conditions",
      "description": "Condition specifying when constraints are active in the dual problem."
    },
    {
      "id": "SVM Optimization",
      "type": "subnode",
      "parent": "Optimization Problems",
      "description": "Specific optimization formulation for Support Vector Machines (SVM)."
    },
    {
      "id": "Lagrangian Optimization in SVMs",
      "type": "major",
      "parent": null,
      "description": "Optimization problem formulation for Support Vector Machines using Lagrangian multipliers."
    },
    {
      "id": "Solving Dual Problem",
      "type": "subnode",
      "parent": "Dual Problem",
      "description": "Algorithmic approach to solving the dual problem to find optimal alpha values."
    },
    {
      "id": "Finding w and b from Alpha",
      "type": "subnode",
      "parent": "Lagrangian Optimization in SVMs",
      "description": "Using optimal alpha values to determine the optimal weight vector w and intercept term b."
    },
    {
      "id": "Optimal Parameters Calculation",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Process of finding optimal values for parameters like w and b."
    },
    {
      "id": "Dual Formulation",
      "type": "subnode",
      "parent": "Optimal Parameters Calculation",
      "description": "Alternative method focusing on inner products between feature vectors."
    },
    {
      "id": "Inner Products and Support Vectors",
      "type": "subnode",
      "parent": "Dual Formulation",
      "description": "Use of support vectors to simplify prediction calculations."
    },
    {
      "id": "Dual Formulation of SVM",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Optimization problem in terms of Lagrange multipliers."
    },
    {
      "id": "Support Vector Machines",
      "type": "major",
      "parent": null,
      "description": "Algorithm for classification in high-dimensional spaces."
    },
    {
      "id": "Non-Separable Case",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Handling datasets where data points cannot be perfectly separated."
    },
    {
      "id": "\\(\\ell_{1}\\) Regularization",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Penalizes the absolute value of coefficients to handle outliers and non-separable cases."
    },
    {
      "id": "Parameter C",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Controls the trade-off between maximizing margin and minimizing classification errors."
    },
    {
      "id": "Lagrangian Formulation",
      "type": "subnode",
      "parent": "Optimization Problem",
      "description": "Dual formulation involving Lagrange multipliers to solve constrained optimization problems."
    },
    {
      "id": "Sequential Minimal Optimization (SMO) Algorithm",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Efficient method to solve the dual problem of SVMs."
    },
    {
      "id": "Coordinate Ascent Algorithm",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO) Algorithm",
      "description": "Optimization technique used in SMO for solving unconstrained problems."
    },
    {
      "id": "Unconstrained Optimization Problem",
      "type": "subnode",
      "parent": "Coordinate Ascent Algorithm",
      "description": "Problem formulation to maximize a function W with respect to parameters \u03b1i."
    },
    {
      "id": "Machine Learning Optimization Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques used in machine learning for optimization problems."
    },
    {
      "id": "Coordinate Ascent",
      "type": "subnode",
      "parent": "Machine Learning Optimization Techniques",
      "description": "Optimization method that updates one variable at a time."
    },
    {
      "id": "SVMs (Support Vector Machines)",
      "type": "subnode",
      "parent": "Machine Learning Optimization Techniques",
      "description": "Models used for classification and regression analysis."
    },
    {
      "id": "Dual Formulation of Optimization Problem",
      "type": "subnode",
      "parent": "SMO Algorithm",
      "description": "Formulation used for solving SVM optimization problems."
    },
    {
      "id": "Constraints in SMO",
      "type": "subnode",
      "parent": "SMO Algorithm",
      "description": "Conditions that must be satisfied during the optimization process."
    },
    {
      "id": "Optimization Techniques",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques used to optimize functions in ML models."
    },
    {
      "id": "Quadratic Functions",
      "type": "subnode",
      "parent": "Optimization Techniques",
      "description": "Functions that are second-degree polynomials in multiple variables."
    },
    {
      "id": "Box Constraints",
      "type": "subnode",
      "parent": "Optimization Techniques",
      "description": "Constraints limiting the range of values for optimization parameters."
    },
    {
      "id": "Convergence Check",
      "type": "subnode",
      "parent": "SMO Algorithm",
      "description": "Checking KKT conditions to determine if the SMO process has converged."
    },
    {
      "id": "Efficient Update",
      "type": "subnode",
      "parent": "SMO Algorithm",
      "description": "Deriving efficient updates for \u03b1_i and \u03b1_j based on fixed constraints."
    },
    {
      "id": "Tolerance Parameter",
      "type": "subnode",
      "parent": "Convergence Check",
      "description": "Parameter defining acceptable deviation from KKT conditions for convergence."
    },
    {
      "id": "Constraints on \u03b1_i and \u03b1_j",
      "type": "subnode",
      "parent": "Efficient Update",
      "description": "Box constraints [0,C]x[0,C] and line constraint \u03b1_1y^(1)+\u03b1_2y^(2)=\u03b7."
    },
    {
      "id": "Sequential Minimal Optimization (SMO)",
      "type": "subnode",
      "parent": "Support Vector Machines (SVM)",
      "description": "Algorithm used to solve the optimization problem in SVM efficiently."
    },
    {
      "id": "Alpha Values Update",
      "type": "subnode",
      "parent": "Sequential Minimal Optimization (SMO)",
      "description": "Process of updating alpha values within SMO algorithm constraints."
    },
    {
      "id": "Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Subfield focusing on neural networks to learn representations from data."
    },
    {
      "id": "Supervised Learning with Non-Linear Models",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Introduction to non-linear models in supervised learning context."
    },
    {
      "id": "Non-linear Model h_\u03b8(x)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Abstract non-linear model used in machine learning problems."
    },
    {
      "id": "Training Examples",
      "type": "subnode",
      "parent": "Non-linear Model h_\u03b8(x)",
      "description": "Parallel processing of forward and backward passes for multiple examples."
    },
    {
      "id": "Regression Problems",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Problems where output is a real number, using least square and mean-square loss functions."
    },
    {
      "id": "Least Square Cost Function",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Cost function for individual training examples in regression problems."
    },
    {
      "id": "Mean-Square Cost Function",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Average cost over all training examples, used to evaluate model performance."
    },
    {
      "id": "Loss Function",
      "type": "major",
      "parent": null,
      "description": "Function used to measure the performance of a model and optimize its parameters."
    },
    {
      "id": "Average Loss",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Overall loss calculated as average over individual training examples."
    },
    {
      "id": "Conditional Probabilistic Model",
      "type": "subnode",
      "parent": "Negative Log-Likelihood",
      "description": "Model where output distribution depends on input features."
    },
    {
      "id": "Optimizers",
      "type": "major",
      "parent": null,
      "description": "Algorithms used to minimize loss functions during training, e.g., SGD or ADAM."
    },
    {
      "id": "Gradient Descent (GD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Algorithm for finding local minima of a function by moving iteratively in direction of steepest descent."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Optimizers",
      "description": "Optimization algorithm that updates parameters using a single example at each iteration."
    },
    {
      "id": "Logit",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Output of linear regression before applying the logistic function."
    },
    {
      "id": "Probability Prediction",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Conversion of logit to probability using logistic function."
    },
    {
      "id": "Negative Likelihood Loss",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Loss function derived from negative likelihood for binary classification."
    },
    {
      "id": "Total Loss Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Average of individual loss functions over all training examples."
    },
    {
      "id": "Logits in Multi-class",
      "type": "subnode",
      "parent": "Multi-class Classification",
      "description": "Output of the model before applying softmax function, representing predictions for each class."
    },
    {
      "id": "Mini-batch Stochastic Gradient Descent",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Variant of SGD where gradients are computed over small batches of examples for efficiency."
    },
    {
      "id": "Hyperparameters",
      "type": "subnode",
      "parent": "Stochastic Gradient Descent (SGD)",
      "description": "Parameters like learning rate and number of iterations that control the optimization process."
    },
    {
      "id": "Initialization",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Random initialization of parameters before starting the training process."
    },
    {
      "id": "Deep Learning Models",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Models like neural networks that are trained using SGD and its variants."
    },
    {
      "id": "Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Non-linear models involving matrix multiplications and non-linear operations for various machine learning tasks."
    },
    {
      "id": "Single Neuron Network",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A simple network with one neuron for basic function approximation."
    },
    {
      "id": "ReLU Function",
      "type": "subnode",
      "parent": "Single Neuron Network",
      "description": "Rectified Linear Unit, defined as max(t, 0), introduces non-linearity in neural networks."
    },
    {
      "id": "Housing Price Prediction",
      "type": "subnode",
      "parent": "Single Neuron Network",
      "description": "Example of using a single neuron network for predicting housing prices."
    },
    {
      "id": "Activation Functions",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Functions applied to the output of a neuron to introduce non-linearity."
    },
    {
      "id": "Single Neuron Model",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A model with a single neuron using ReLU activation function and weight vector w."
    },
    {
      "id": "Bias Term",
      "type": "subnode",
      "parent": "Single Neuron Model",
      "description": "The term b in the equation, representing an additional parameter for flexibility."
    },
    {
      "id": "Weight Vector",
      "type": "subnode",
      "parent": "Single Neuron Model",
      "description": "Vector w that represents the weights of inputs to a neuron."
    },
    {
      "id": "Stacking Neurons",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Process of combining multiple neurons to form more complex neural networks."
    },
    {
      "id": "Complex Neural Network Example",
      "type": "subnode",
      "parent": "Stacking Neurons",
      "description": "Example using features like house size, number of bedrooms, zip code, and neighborhood wealth."
    },
    {
      "id": "Housing Prices Model",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Modeling housing prices based on derived features such as family size, walkability, and school quality."
    },
    {
      "id": "Derived Features",
      "type": "subnode",
      "parent": "Housing Prices Model",
      "description": "Features like family size, walkable neighborhood, and school quality derived from input data."
    },
    {
      "id": "Family Size",
      "type": "subnode",
      "parent": "Derived Features",
      "description": "Intermediate variable representing the maximum family size a house can accommodate."
    },
    {
      "id": "Walkability",
      "type": "subnode",
      "parent": "Derived Features",
      "description": "Indicator of how walkable a neighborhood is based on zip code and other factors."
    },
    {
      "id": "School Quality",
      "type": "subnode",
      "parent": "Derived Features",
      "description": "Quality of the local elementary school derived from neighborhood wealth and zip code."
    },
    {
      "id": "Neural Network Architecture",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Architecture including hidden units and output layer for predicting housing prices."
    },
    {
      "id": "Hidden Units",
      "type": "subnode",
      "parent": "Neural Network Architecture",
      "description": "Intermediate variables (a1, a2, a3) representing derived features in the neural network."
    },
    {
      "id": "Output Layer",
      "type": "subnode",
      "parent": "Neural Network Architecture",
      "description": "Final layer that combines hidden units to predict housing prices without applying ReLU."
    },
    {
      "id": "Parameters (\u03b8)",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Set of parameters that define the model's structure and function."
    },
    {
      "id": "Biological Inspiration",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Artificial neural networks are inspired by biological neural networks, but their similarity is debated."
    },
    {
      "id": "Two-layer Fully-Connected Neural Network",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A simple model where each node in one layer connects to every node in the next layer."
    },
    {
      "id": "Prior Knowledge",
      "type": "subnode",
      "parent": "Two-layer Fully-Connected Neural Network",
      "description": "Incorporating prior knowledge about input relationships can guide network construction."
    },
    {
      "id": "Generic Parameterization",
      "type": "subnode",
      "parent": "Two-layer Fully-Connected Neural Network",
      "description": "A flexible approach to parameterize the model without relying on specific domain knowledge."
    },
    {
      "id": "Fully-Connected Neural Network",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A type of neural network where each neuron is connected to every neuron in the next layer."
    },
    {
      "id": "Intermediate Variables (a_i)",
      "type": "subnode",
      "parent": "Fully-Connected Neural Network",
      "description": "Variables that depend on all inputs and are used for computation within layers."
    },
    {
      "id": "Two-Layer Fully-Connected NN",
      "type": "subnode",
      "parent": "Fully-Connected Neural Network",
      "description": "A neural network with two layers, one hidden layer with m units and a d-dimensional input."
    },
    {
      "id": "Vectorization",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Simplification of expressions using matrix and vector notations for efficiency."
    },
    {
      "id": "Vectorization in Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Process of converting loops into matrix operations for efficiency."
    },
    {
      "id": "Matrix Algebra",
      "type": "subnode",
      "parent": "Vectorization in Neural Networks",
      "description": "Use of matrix operations to replace loops in neural network computations."
    },
    {
      "id": "BLAS Libraries",
      "type": "subnode",
      "parent": "Vectorization in Neural Networks",
      "description": "Highly optimized libraries for numerical linear algebra used in vectorization."
    },
    {
      "id": "Two-Layer Fully-Connected Network",
      "type": "subnode",
      "parent": "Vectorization in Neural Networks",
      "description": "Example of a neural network structure that benefits from vectorization."
    },
    {
      "id": "Multi-layer Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Stacked layers of fully-connected neural networks with ReLU activation functions."
    },
    {
      "id": "Weight Matrices and Biases",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "Dimensions and compatibility requirements for weight matrices and biases in multi-layer networks."
    },
    {
      "id": "Total Neurons and Parameters",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "Calculation of total neurons and parameters based on layer dimensions."
    },
    {
      "id": "Notational Consistency",
      "type": "subnode",
      "parent": "Multi-layer Neural Networks",
      "description": "Introduction of notations for consistency across layers, including input and output definitions."
    },
    {
      "id": "Other Activation Functions",
      "type": "major",
      "parent": null,
      "description": "Alternative non-linear functions that can replace ReLU in neural networks."
    },
    {
      "id": "Two-Layer Neural Network",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "A simple neural network with one hidden layer for learning non-linear relationships."
    },
    {
      "id": "Weight Matrices",
      "type": "subnode",
      "parent": "Two-Layer Neural Network",
      "description": "Matrices containing weights that the model learns during training to make predictions."
    },
    {
      "id": "Biases",
      "type": "subnode",
      "parent": "Two-Layer Neural Network",
      "description": "Bias terms added to linear combinations of inputs in each layer."
    },
    {
      "id": "Hidden Layer",
      "type": "subnode",
      "parent": "Two-Layer Neural Network",
      "description": "Layer between input and output where non-linear transformations occur."
    },
    {
      "id": "Multi-layer Networks",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Networks with multiple hidden layers for learning complex patterns in data."
    },
    {
      "id": "Feature Engineering",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Process of selecting and transforming raw data into features for model training."
    },
    {
      "id": "Neural Network Parameters",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Parameters in neural networks excluding those in the last layer."
    },
    {
      "id": "Learned Features",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Features automatically discovered by deep learning models in the penultimate layer."
    },
    {
      "id": "Tanh Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Similar to sigmoid but ranges from -1 to 1."
    },
    {
      "id": "Leaky ReLU",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "A variant of ReLU with a small slope for negative values."
    },
    {
      "id": "GELU Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Smoothly approximates the ReLU function, used in NLP models."
    },
    {
      "id": "Softplus Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "A smooth approximation of the ReLU function with a proper second derivative."
    },
    {
      "id": "Identity Function",
      "type": "subnode",
      "parent": "Activation Functions",
      "description": "Function that returns input as output, not commonly used in neural networks."
    },
    {
      "id": "Deep Learning Representations",
      "type": "major",
      "parent": null,
      "description": "Discusses feature representations in deep learning models."
    },
    {
      "id": "House Price Prediction Example",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Illustrates use of neural networks for predicting house prices without specifying intermediate features."
    },
    {
      "id": "Feature Maps and Representation Transferability",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Explains how feature maps from one dataset can be useful in other datasets."
    },
    {
      "id": "Black Box Nature of Neural Networks",
      "type": "subnode",
      "parent": "Deep Learning Representations",
      "description": "Highlights the difficulty in understanding features discovered by neural networks."
    },
    {
      "id": "Matrix Multiplication as a Building Block",
      "type": "subnode",
      "parent": "Modules in Modern Neural Networks",
      "description": "Explains matrix multiplication operation with parameters W and b."
    },
    {
      "id": "MLP Composition of Modules",
      "type": "subnode",
      "parent": "Modules in Modern Neural Networks",
      "description": "Describes MLP as a composition of multiple matrix multiplications and nonlinear activations."
    },
    {
      "id": "MLP (Multilayer Perceptron)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A neural network model composed of multiple layers of matrix multiplication and nonlinear activation functions."
    },
    {
      "id": "Matrix Multiplication Module",
      "type": "subnode",
      "parent": "MLP (Multilayer Perceptron)",
      "description": "Building block for MLP, involves linear transformation using weights and biases."
    },
    {
      "id": "Nonlinear Activation Module",
      "type": "subnode",
      "parent": "MLP (Multilayer Perceptron)",
      "description": "Applies a nonlinear function to the output of matrix multiplication modules."
    },
    {
      "id": "ResNet (Residual Network)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Neural network architecture that uses residual connections for better training of deep networks."
    },
    {
      "id": "Residual Block",
      "type": "subnode",
      "parent": "ResNet (Residual Network)",
      "description": "Basic unit in ResNet, adds input to output of a series of matrix multiplications and activations."
    },
    {
      "id": "Layer Normalization",
      "type": "major",
      "parent": null,
      "description": "Normalization technique in neural networks."
    },
    {
      "id": "LN-S(z)",
      "type": "subnode",
      "parent": "Layer Normalization",
      "description": "Standardized version of the input vector z."
    },
    {
      "id": "Affine Transformation",
      "type": "subnode",
      "parent": "Layer Normalization",
      "description": "Transformation using learnable parameters beta and gamma."
    },
    {
      "id": "Learnable Parameters (beta, gamma)",
      "type": "subnode",
      "parent": "Layer Normalization",
      "description": "Scalars used to adjust the mean and standard deviation of LN-S(z)."
    },
    {
      "id": "Scaling-Invariant Property",
      "type": "subnode",
      "parent": "Layer Normalization",
      "description": "Property ensuring model invariance under parameter scaling."
    },
    {
      "id": "MM_{W,b}",
      "type": "subnode",
      "parent": "Scaling-Invariant Property",
      "description": "Linear transformation module with parameters W and b."
    },
    {
      "id": "Machine Learning Architectures",
      "type": "major",
      "parent": null,
      "description": "Overview of different architectures used in machine learning."
    },
    {
      "id": "ResNet Architecture",
      "type": "subnode",
      "parent": "Machine Learning Architectures",
      "description": "Deep residual network architecture using convolution layers and batch normalization."
    },
    {
      "id": "Convolutional Layers",
      "type": "subnode",
      "parent": "ResNet Architecture",
      "description": "Specialized layers in neural networks for processing grid-like topology data."
    },
    {
      "id": "Batch Normalization Variants",
      "type": "subnode",
      "parent": "ResNet Architecture",
      "description": "Different types of batch normalization techniques used in neural networks."
    },
    {
      "id": "Transformer Architecture",
      "type": "subnode",
      "parent": "Machine Learning Architectures",
      "description": "Architecture widely used in modern large language models."
    },
    {
      "id": "Layer Normalization (LN)",
      "type": "subnode",
      "parent": "Batch Normalization Variants",
      "description": "Normalization technique applied to each training example independently."
    },
    {
      "id": "LN-S Module",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Sub-module of layer normalization that normalizes vector to zero mean and unit variance."
    },
    {
      "id": "Affine Transformation in LN",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Transformation using learnable parameters beta and gamma for desired mean and standard deviation."
    },
    {
      "id": "Convolutional Neural Networks (CNN)",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Type of neural network commonly used for image and signal processing."
    },
    {
      "id": "1-D Convolution",
      "type": "subnode",
      "parent": "Convolutional Neural Networks (CNN)",
      "description": "Simplified version of convolution layer used in text and sequence data analysis."
    },
    {
      "id": "Filter Vector",
      "type": "subnode",
      "parent": "1-D Convolution",
      "description": "Vector of weights applied to input sequences for feature extraction."
    },
    {
      "id": "Bias Scalar",
      "type": "subnode",
      "parent": "1-D Convolution",
      "description": "Scalar value added to the output of each convolution operation."
    },
    {
      "id": "Matrix Multiplication",
      "type": "subnode",
      "parent": "1-D Convolution",
      "description": "Operation representing convolution as a matrix multiplication with shared parameters."
    },
    {
      "id": "Scale-Invariant Property",
      "type": "subnode",
      "parent": "Layer Normalization (LN)",
      "description": "Property of modern DL architectures regarding weight scaling in layers before the last layer."
    },
    {
      "id": "Other Normalization Layers",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Alternative normalization techniques used in neural networks."
    },
    {
      "id": "Batch Normalization",
      "type": "subnode",
      "parent": "Other Normalization Layers",
      "description": "Normalization technique applied to mini-batches of data, commonly used in computer vision."
    },
    {
      "id": "Group Normalization",
      "type": "subnode",
      "parent": "Other Normalization Layers",
      "description": "Normalization method that divides channels into groups and normalizes over each group."
    },
    {
      "id": "Convolutional Neural Networks (CNNs)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Neural networks designed for processing structured grid data such as images."
    },
    {
      "id": "Parameter Sharing",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Mechanism where the same filter is used across all positions of an input."
    },
    {
      "id": "Efficiency",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Reduced computational complexity compared to generic matrix multiplication."
    },
    {
      "id": "1D Convolution",
      "type": "subnode",
      "parent": "Convolutional Layers",
      "description": "Type of convolution applied on one-dimensional data."
    },
    {
      "id": "Channels",
      "type": "subnode",
      "parent": "1D Convolution",
      "description": "Multiple input and output dimensions in a convolution operation."
    },
    {
      "id": "Differentiable Circuit",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Composition of arithmetic operations and elementary functions that can compute real-valued functions."
    },
    {
      "id": "Gradient Computation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Iterative computation of gradients with respect to hidden activations and parameters using chain rule."
    },
    {
      "id": "Conv1D-S Module",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A Convolutional 1D module with specific parameters."
    },
    {
      "id": "Total Parameters in Conv1D",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Calculation of total parameters for a Conv1D layer."
    },
    {
      "id": "2-D Convolution (Conv2D-S)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Description and formula for 2-dimensional convolutional module."
    },
    {
      "id": "Total Parameters in Conv2D",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Calculation of total parameters for a Conv2D layer."
    },
    {
      "id": "Partial Derivatives",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Introduction to partial derivatives and their role in machine learning computations."
    },
    {
      "id": "Multi-Variable Functions",
      "type": "subnode",
      "parent": "Partial Derivatives",
      "description": "Handling partial derivatives in multi-variable scenarios."
    },
    {
      "id": "Chain Rule",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Application of the chain rule for composite functions in machine learning."
    },
    {
      "id": "Auto-Differentiation",
      "type": "subnode",
      "parent": "Chain Rule",
      "description": "Techniques for automatic computation of derivatives in complex models."
    },
    {
      "id": "Loss Function Composition",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Representation of loss functions as compositions of modules."
    },
    {
      "id": "Machine Learning Fundamentals",
      "type": "major",
      "parent": null,
      "description": "Core concepts and principles of machine learning."
    },
    {
      "id": "Backward Function in Machine Learning",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Explains the backward function as a linear map from \u2202J/\u2202u to \u2202J/\u2202z."
    },
    {
      "id": "Chain Rule in Machine Learning",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Describes the chain rule as a method for computing gradients through intermediate variables."
    },
    {
      "id": "Jacobian Matrix",
      "type": "subnode",
      "parent": "Backward Function in Machine Learning",
      "description": "Matrix representation of partial derivatives, often used in understanding transformations between variables."
    },
    {
      "id": "Loss Function J(\u03b8)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Description and computation of the loss function for a given example."
    },
    {
      "id": "Backpropagation Overview",
      "type": "major",
      "parent": null,
      "description": "Overview and implementation details of backpropagation in deep learning packages."
    },
    {
      "id": "Chain Rule Perspective",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "Reviewing the basic Chain rule for understanding backpropagation."
    },
    {
      "id": "General Backprop Strategy",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "Introduction to general strategy for implementing backpropagation."
    },
    {
      "id": "Backward Function Computation",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "Discussion on computing backward functions for basic modules in neural networks."
    },
    {
      "id": "Concrete Backprop Algorithm",
      "type": "subnode",
      "parent": "Backpropagation Overview",
      "description": "Putting together a concrete backprop algorithm for MLPs."
    },
    {
      "id": "Backpropagation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Algorithm for computing gradients of loss function with respect to weights in a neural network."
    },
    {
      "id": "Matrix Multiplication Module (MM)",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Detailed computation of backward function for matrix multiplication module."
    },
    {
      "id": "Basic Modules",
      "type": "subnode",
      "parent": "Neural Networks",
      "description": "Computation and implementation of basic neural network modules."
    },
    {
      "id": "Backward Function for Parameters",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Derivation and computation of backward functions for parameters W and b."
    },
    {
      "id": "Vectorized Notation",
      "type": "subnode",
      "parent": "Backward Function for Parameters",
      "description": "Expression in vector form for the backward function."
    },
    {
      "id": "Backward Function for Activations",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Derivation and computation of backward functions for activation functions."
    },
    {
      "id": "Binary Classification Problem",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A specific type of classification task with two classes."
    },
    {
      "id": "MLP Model",
      "type": "subnode",
      "parent": "Binary Classification Problem",
      "description": "Multi-layer perceptron model used for binary classification."
    },
    {
      "id": "Modules in MLP",
      "type": "subnode",
      "parent": "MLP Model",
      "description": "Components of an MLP, including parameters and fixed operations."
    },
    {
      "id": "Intermediate Variables",
      "type": "subnode",
      "parent": "Loss Function",
      "description": "Variables used during the computation process in a loss function."
    },
    {
      "id": "Forward Pass",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of computing intermediate variables sequentially from input to output."
    },
    {
      "id": "Backward Pass",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Process of computing derivatives in reverse order for parameter updates."
    },
    {
      "id": "Derivatives Computation",
      "type": "subnode",
      "parent": "Backward Pass",
      "description": "Calculation of gradients with respect to intermediate variables and parameters."
    },
    {
      "id": "Backward Function for Loss Functions",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Explains the backward function for different loss functions."
    },
    {
      "id": "Squared Loss (MSE)",
      "type": "subnode",
      "parent": "Backward Function for Loss Functions",
      "description": "Derivation of the backward pass for squared loss."
    },
    {
      "id": "Logistic Loss",
      "type": "subnode",
      "parent": "Backward Function for Loss Functions",
      "description": "Derivation of the backward pass for logistic loss."
    },
    {
      "id": "Parameter Gradients",
      "type": "subnode",
      "parent": "Gradient Calculation",
      "description": "Calculation of gradients for weights and biases."
    },
    {
      "id": "Machine Learning Loss Functions",
      "type": "major",
      "parent": null,
      "description": "Overview of loss functions used in machine learning."
    },
    {
      "id": "Forward Pass in MLP",
      "type": "subnode",
      "parent": "Back-propagation for MLPs",
      "description": "Sequence of operations to compute the loss function in a multi-layer perceptron."
    },
    {
      "id": "Matrix Notation in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of using matrix notation for machine learning operations."
    },
    {
      "id": "Training Examples Representation",
      "type": "subnode",
      "parent": "Matrix Notation in Machine Learning",
      "description": "Representation of multiple training examples in matrix form."
    },
    {
      "id": "First-Layer Activations",
      "type": "subnode",
      "parent": "Training Examples Representation",
      "description": "Calculation of first-layer activations for each example."
    },
    {
      "id": "Vectorization of Operations",
      "type": "subnode",
      "parent": "Training Examples Representation",
      "description": "Combining operations into a single unified matrix formulation."
    },
    {
      "id": "Broadcasting in Matrix Operations",
      "type": "subnode",
      "parent": "Vectorization of Operations",
      "description": "Explanation of broadcasting technique for adding bias to matrices."
    },
    {
      "id": "Generalizing to Multiple Layers",
      "type": "subnode",
      "parent": "Matrix Notation in Machine Learning",
      "description": "Discussion on extending matrix notation to multiple layers with implementation subtleties."
    },
    {
      "id": "Matricization Approach",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Method to generalize the matricization approach for multiple layers."
    },
    {
      "id": "Implementation Subtleties",
      "type": "subnode",
      "parent": "Matricization Approach",
      "description": "Details on how deep learning packages handle data points and matrix operations."
    },
    {
      "id": "Data Matrix Representation",
      "type": "subnode",
      "parent": "Implementation Subtleties",
      "description": "Discussion on the row-major convention used in implementations."
    },
    {
      "id": "Generalization and Regularization",
      "type": "major",
      "parent": null,
      "description": "Tools to analyze and understand model generalization."
    },
    {
      "id": "Training Loss Function",
      "type": "subnode",
      "parent": "Generalization and Regularization",
      "description": "Description of the training loss function used in supervised learning problems."
    },
    {
      "id": "Training Loss",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "Measure of model performance on training data, often referred to as empirical loss."
    },
    {
      "id": "Test Error",
      "type": "subnode",
      "parent": "Loss Functions",
      "description": "Evaluation metric for a model's predictive accuracy on unseen test examples."
    },
    {
      "id": "Training vs Test Datasets",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Difference between datasets used for training and evaluating models, with test data being unseen by the learning process."
    },
    {
      "id": "Overfitting and Underfitting",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Concepts of overfitting when model performs well on training data but poorly on test data, underfitting when both errors are high."
    },
    {
      "id": "Bias-Variance Tradeoff",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Balancing model complexity to minimize both bias and variance for optimal performance."
    },
    {
      "id": "Training Dataset Example",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Example illustrating training data generation and noise assumptions."
    },
    {
      "id": "Linear Regression Models",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Discussion on fitting linear models to the dataset."
    },
    {
      "id": "Underfitting Example",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Illustration of underfitting with a linear model that cannot capture data structure."
    },
    {
      "id": "Training vs Test Distributions",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Difference between distributions of training and test data sets."
    },
    {
      "id": "Domain Shift",
      "type": "subnode",
      "parent": "Training vs Test Distributions",
      "description": "Scenario where training and testing datasets come from different distributions."
    },
    {
      "id": "Generalization Gap",
      "type": "subnode",
      "parent": "Overfitting and Underfitting",
      "description": "Difference between training error and test error indicating poor generalization."
    },
    {
      "id": "Double Descent Phenomenon",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Phenomenon discussed in Section 8.2 involving complex models performing better than simple ones after a certain point."
    },
    {
      "id": "Linear Model Limitations",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Explains the limitations and high bias of linear models."
    },
    {
      "id": "Bias Definition",
      "type": "subnode",
      "parent": "Linear Model Limitations",
      "description": "Definition of model bias in terms of test error."
    },
    {
      "id": "5th Degree Polynomial Models",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Discussion on the overfitting issue with 5th degree polynomials."
    },
    {
      "id": "Generalization Failure",
      "type": "subnode",
      "parent": "5th Degree Polynomial Models",
      "description": "Explanation of why high-degree polynomial models fail to generalize well."
    },
    {
      "id": "Low Bias, High Variance",
      "type": "subnode",
      "parent": "5th Degree Polynomial Models",
      "description": "Discussion on the low bias and high variance characteristics of complex models."
    },
    {
      "id": "Polynomial Fitting",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on fitting polynomials to data sets."
    },
    {
      "id": "Variance in Model",
      "type": "subnode",
      "parent": "Polynomial Fitting",
      "description": "Description of variance and its impact on model fitting."
    },
    {
      "id": "Model Complexity",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Measure of the complexity of a machine learning model, often related to the number or type of parameters."
    },
    {
      "id": "Test Error Decomposition",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Breakdown of test error into bias and variance components."
    },
    {
      "id": "Variance Term",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Error due to model's sensitivity to training data randomness."
    },
    {
      "id": "Model-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Peak in test error as model complexity increases, followed by a decrease."
    },
    {
      "id": "Training Dataset",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Dataset used for training a model in regression tasks."
    },
    {
      "id": "Test Example",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Example used to test the performance of a trained model."
    },
    {
      "id": "Expected Test Error",
      "type": "subnode",
      "parent": "Regression Problems",
      "description": "Average error expected on new data after training."
    },
    {
      "id": "Mean Squared Error (MSE)",
      "type": "subnode",
      "parent": "Expected Test Error",
      "description": "Measure of the average squared difference between predicted and actual values."
    },
    {
      "id": "Claim 8.1.1",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Mathematical tool for decomposing MSE into bias and variance terms."
    },
    {
      "id": "Independence of Random Variables",
      "type": "subnode",
      "parent": "Claim 8.1.1",
      "description": "Property used in the proof of Claim 8.1.1 to simplify calculations."
    },
    {
      "id": "Model Evaluation",
      "type": "subnode",
      "parent": "Machine Learning Fundamentals",
      "description": "Assessment of model performance using various metrics."
    },
    {
      "id": "Average Model (h_avg)",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Hypothetical model representing the average prediction over an infinite number of datasets."
    },
    {
      "id": "True Function (h_star)",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "The optimal function that a model aims to approximate."
    },
    {
      "id": "Training Model (h_S)",
      "type": "subnode",
      "parent": "Bias-Variance Tradeoff",
      "description": "Model trained on a specific dataset S."
    },
    {
      "id": "Sample-wise Double Descent",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Test error pattern in relation to the sample size relative to model complexity."
    },
    {
      "id": "Overparameterized Models",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Models with more parameters than necessary, showing a decrease in test error after an initial increase."
    },
    {
      "id": "Historical Context",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Discovery and popularization of the double descent phenomenon by various researchers over time."
    },
    {
      "id": "Optimal Regularization",
      "type": "subnode",
      "parent": "Sample-wise Double Descent",
      "description": "Improvement of test error with optimal regularization tuning."
    },
    {
      "id": "Implicit Regularization",
      "type": "subnode",
      "parent": "Model-wise Double Descent",
      "description": "Regularization effect provided by optimizers like gradient descent in overparameterized models."
    },
    {
      "id": "Sample Complexity Bounds",
      "type": "major",
      "parent": null,
      "description": "Theoretical bounds on the number of samples needed for learning algorithms to perform well."
    },
    {
      "id": "Model Selection",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Process of choosing the best model from a set of candidate models based on training data performance."
    },
    {
      "id": "Generalization Error",
      "type": "subnode",
      "parent": "Sample Complexity Bounds",
      "description": "Error rate of the hypothesis function on unseen data drawn from the same distribution."
    },
    {
      "id": "Gradient Descent Optimizer",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Optimization algorithm used to minimize loss functions."
    },
    {
      "id": "Minimum Norm Solution",
      "type": "subnode",
      "parent": "Gradient Descent Optimizer",
      "description": "Solution with the smallest Euclidean norm in overparameterized models."
    },
    {
      "id": "Model Complexity Measures",
      "type": "subnode",
      "parent": "Double Descent Phenomenon",
      "description": "Different ways to measure the complexity of a machine learning model."
    },
    {
      "id": "Number of Parameters",
      "type": "subnode",
      "parent": "Model Complexity Measures",
      "description": "Common measure for model complexity, often leading to double descent."
    },
    {
      "id": "Norm of Learned Model",
      "type": "subnode",
      "parent": "Model Complexity Measures",
      "description": "Alternative measure that can avoid the double descent phenomenon."
    },
    {
      "id": "Regularization Techniques",
      "type": "major",
      "parent": null,
      "description": "Methods to prevent overfitting in machine learning models."
    },
    {
      "id": "Learning Guarantees",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Conditions and proofs for the success of learning algorithms."
    },
    {
      "id": "Union Bound Lemma",
      "type": "subnode",
      "parent": "Learning Guarantees",
      "description": "Probability bound on union of events, not requiring independence."
    },
    {
      "id": "Hoeffding Inequality (Chernoff Bound)",
      "type": "subnode",
      "parent": "Learning Guarantees",
      "description": "Bound on deviation between sample mean and true probability for Bernoulli variables."
    },
    {
      "id": "Hypothesis Function",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Function representing a learner's prediction based on input data."
    },
    {
      "id": "Training Error",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Error rate of the hypothesis function on the training set."
    },
    {
      "id": "Empirical Risk Minimization (ERM)",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Algorithm that minimizes the empirical risk over training data."
    },
    {
      "id": "PAC Assumptions",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Framework and assumptions for learning theory, including training and testing on the same distribution."
    },
    {
      "id": "Hypothesis Class",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "Set of all classifiers considered by a learning algorithm."
    },
    {
      "id": "Finite Hypothesis Classes",
      "type": "subnode",
      "parent": "Empirical Risk Minimization (ERM)",
      "description": "Consideration of ERM in scenarios with finite hypothesis classes."
    },
    {
      "id": "Empirical Risk Minimization",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Process of selecting a hypothesis with the smallest training error."
    },
    {
      "id": "Generalization Error Guarantees",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Strategies to ensure that training error closely approximates generalization error."
    },
    {
      "id": "Bernoulli Random Variable Z",
      "type": "subnode",
      "parent": "Generalization Error Guarantees",
      "description": "Random variable indicating whether a hypothesis misclassifies an example drawn from the distribution."
    },
    {
      "id": "Training Set Sampling",
      "type": "subnode",
      "parent": "Generalization Error Guarantees",
      "description": "Process of sampling training examples independently and identically distributed (i.i.d.)."
    },
    {
      "id": "Hoeffding Inequality",
      "type": "subnode",
      "parent": "Generalization Error Guarantees",
      "description": "Inequality used to bound the probability that the empirical error deviates from the true error by more than a certain amount."
    },
    {
      "id": "Simultaneous Generalization Guarantee",
      "type": "subnode",
      "parent": "Generalization Error Guarantees",
      "description": "Ensuring high-probability guarantees for all hypotheses in the hypothesis space simultaneously."
    },
    {
      "id": "Sample Complexity",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Number of samples required for a learning algorithm to achieve certain performance with high probability."
    },
    {
      "id": "Uniform Convergence",
      "type": "subnode",
      "parent": "Generalization Error",
      "description": "Property ensuring that training error approximates generalization error for all hypotheses in the hypothesis space."
    },
    {
      "id": "Hypothesis Space (k)",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Set of all possible hypotheses considered by a learning algorithm, denoted as k."
    },
    {
      "id": "Empirical Error",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "The observed error rate on training data."
    },
    {
      "id": "True Error",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "The actual error rate of a hypothesis when applied to unseen data."
    },
    {
      "id": "Union Bound",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "Probability bound used to combine individual event probabilities into an overall probability for multiple events."
    },
    {
      "id": "Hypothesis Space H",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "Set of all possible hypotheses under consideration."
    },
    {
      "id": "n (Sample Size)",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "Number of training examples used to estimate empirical error."
    },
    {
      "id": "gamma (Margin)",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "Threshold value determining the acceptable difference between true and empirical errors."
    },
    {
      "id": "Probability Bound",
      "type": "subnode",
      "parent": "Uniform Convergence",
      "description": "Upper limit on the probability that a hypothesis's empirical error deviates from its true error by more than gamma."
    },
    {
      "id": "Generalization Error Bound",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Mathematical bound on the difference between training error and true error for a given hypothesis class."
    },
    {
      "id": "Hypothesis Class Size",
      "type": "subnode",
      "parent": "Generalization Error Bound",
      "description": "Impact of the size of hypothesis class on generalization error bounds."
    },
    {
      "id": "Hypothesis Space Parameterization",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Different ways to parameterize the hypothesis space."
    },
    {
      "id": "Linear Classifiers",
      "type": "subnode",
      "parent": "Hypothesis Space Parameterization",
      "description": "Examples of linear classifiers with different parameterizations."
    },
    {
      "id": "VC Dimension",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Definition and importance of VC dimension in machine learning theory."
    },
    {
      "id": "Shattering Sets",
      "type": "subnode",
      "parent": "VC Dimension",
      "description": "A set is shattered if every possible labeling can be achieved by some hypothesis in H"
    },
    {
      "id": "Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "Theorem establishing the relationship between VC dimension and generalization error"
    },
    {
      "id": "Corollary to Vapnik's Theorem",
      "type": "major",
      "parent": null,
      "description": "Number of training examples needed for learning well is linear in VC dimension"
    },
    {
      "id": "Floating Point Representation",
      "type": "subnode",
      "parent": "Sample Complexity",
      "description": "Use of 64-bit floating point numbers in learning algorithms."
    },
    {
      "id": "Linear Dependence",
      "type": "subnode",
      "parent": "Sample Complexity",
      "description": "Relationship between sample size and number of model parameters."
    },
    {
      "id": "Hypothesis Classes",
      "type": "subnode",
      "parent": "Machine Learning Theory",
      "description": "Set of hypotheses or models considered during the learning process."
    },
    {
      "id": "Infinite Hypothesis Classes",
      "type": "subnode",
      "parent": "Hypothesis Classes",
      "description": "Hypothesis classes parameterized by real numbers, often infinite in size."
    },
    {
      "id": "Regularization in Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Overview of regularization techniques and concepts in deep learning."
    },
    {
      "id": "Explicit Regularization Techniques",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "Techniques such as weight decay, dropout, data augmentation, spectral norm regularization, and Lipschitzness regularization."
    },
    {
      "id": "Implicit Regularization Effect",
      "type": "subnode",
      "parent": "Regularization in Deep Learning",
      "description": "The impact of optimizers on model generalization through implicit bias."
    },
    {
      "id": "Regularization in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting by adding a penalty for complexity."
    },
    {
      "id": "Regularizer",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizes complexity of the model to avoid overfitting."
    },
    {
      "id": "Regularized Loss",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Combination of loss function and regularizer, controlled by lambda."
    },
    {
      "id": "Lambda Parameter",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Balances the trade-off between fitting data and model complexity."
    },
    {
      "id": "L2 Regularization",
      "type": "subnode",
      "parent": "Regularizer",
      "description": "Encourages small L2 norm of parameters, often called weight decay."
    },
    {
      "id": "Weight Decay",
      "type": "subnode",
      "parent": "L2 Regularization",
      "description": "Adjusts weights during gradient descent to prevent overfitting."
    },
    {
      "id": "Sparsity Inducing Regularization",
      "type": "subnode",
      "parent": "Regularizer",
      "description": "Promotes sparsity in model parameters by penalizing non-zero elements."
    },
    {
      "id": "Optimizers and Generalization",
      "type": "major",
      "parent": null,
      "description": "Discussion on how optimizers affect model generalization."
    },
    {
      "id": "Global Minima",
      "type": "subnode",
      "parent": "Optimizers and Generalization",
      "description": "Exploration of how optimizers bias towards certain types of global minima."
    },
    {
      "id": "Cross Validation",
      "type": "major",
      "parent": null,
      "description": "Technique to estimate model generalization error using validation sets."
    },
    {
      "id": "Chapter 9 Regularization and Model Selection",
      "type": "major",
      "parent": "Machine Learning Concepts",
      "description": "Focuses on regularization techniques to prevent overfitting by controlling model complexity."
    },
    {
      "id": "Training Loss/Cost Function",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Function used in training models that is modified by regularization to include penalties for complexity."
    },
    {
      "id": "Regularizer R(\u03b8)",
      "type": "subnode",
      "parent": "Regularization",
      "description": "Additional term added to the loss function to penalize overly complex models and control model complexity."
    },
    {
      "id": "Regularized Loss J\u03bb(\u03b8)",
      "type": "subnode",
      "parent": "Training Loss/Cost Function",
      "description": "Loss function that includes a regularization term to balance fitting training data with avoiding overfitting."
    },
    {
      "id": "Sparsity Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizing the number of non-zero parameters to reduce model complexity."
    },
    {
      "id": "L1 Norm (LASSO)",
      "type": "subnode",
      "parent": "Sparsity Regularization",
      "description": "A continuous relaxation of sparsity using L1 norm as a regularizer."
    },
    {
      "id": "L2 Norm Regularization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Penalizing the squared magnitude of parameters to prevent overfitting."
    },
    {
      "id": "Gradient Descent Optimization",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Optimization technique that cannot directly optimize non-continuous functions like L0 norm."
    },
    {
      "id": "Deep Learning Regularizers",
      "type": "subnode",
      "parent": "Regularization in Machine Learning",
      "description": "Techniques specific to deep learning models, including weight decay and dropout."
    },
    {
      "id": "Kernel Methods Compatibility",
      "type": "subnode",
      "parent": "L1 Norm (LASSO)",
      "description": "L2 norm is preferred over L1 for kernel methods due to compatibility issues with the kernel trick."
    },
    {
      "id": "Model Selection via Cross Validation",
      "type": "major",
      "parent": null,
      "description": "Process of choosing the best model from a set of candidates."
    },
    {
      "id": "Polynomial Regression Model",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Regression model using polynomial functions to fit data."
    },
    {
      "id": "Bias and Variance Tradeoff",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Balancing underfitting (bias) and overfitting (variance)."
    },
    {
      "id": "Cross Validation Technique",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Method to evaluate model performance by splitting data into subsets."
    },
    {
      "id": "Finite Set of Models",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "A predefined set of models for selection among."
    },
    {
      "id": "Infinite Model Classes",
      "type": "subnode",
      "parent": "Model Selection via Cross Validation",
      "description": "Models that can be chosen from an infinite parameter space."
    },
    {
      "id": "Machine Learning Techniques",
      "type": "major",
      "parent": null,
      "description": "Overview of machine learning techniques including models and adaptation methods."
    },
    {
      "id": "Validation Set",
      "type": "subnode",
      "parent": "Model Selection",
      "description": "Subset of data used to evaluate the generalization error of models during training."
    },
    {
      "id": "Hold Out Cross Validation",
      "type": "subnode",
      "parent": "Validation Set",
      "description": "Method where a portion of the dataset is held out for validation purposes."
    },
    {
      "id": "k-fold Cross Validation",
      "type": "subnode",
      "parent": "Validation Set",
      "description": "Technique that divides data into k subsets and uses each as a validation set in turn."
    },
    {
      "id": "Leave-One-Out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Uses each data point as a test set once while training on the rest of the data."
    },
    {
      "id": "Data Scarcity",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Situation where there is insufficient data to train models effectively."
    },
    {
      "id": "Hold-Out Cross Validation",
      "type": "subnode",
      "parent": "Cross Validation",
      "description": "Splitting data into training and validation sets for better error estimation."
    },
    {
      "id": "Training Set S",
      "type": "subnode",
      "parent": "Empirical Risk Minimization",
      "description": "Dataset used to train models in empirical risk minimization."
    },
    {
      "id": "Hypotheses Training Error",
      "type": "subnode",
      "parent": "Empirical Risk Minimization",
      "description": "Error of hypotheses on training set, often overestimates true error."
    },
    {
      "id": "Validation Set S_cv",
      "type": "subnode",
      "parent": "Hold-Out Cross Validation",
      "description": "Subset of data used to validate models and estimate generalization error."
    },
    {
      "id": "Bayesian Statistics",
      "type": "major",
      "parent": null,
      "description": "Approach to parameter estimation considering parameters as random variables."
    },
    {
      "id": "Frequentist View",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "View of parameters as constant-valued but unknown values."
    },
    {
      "id": "Prior Distribution",
      "type": "subnode",
      "parent": "Bayesian Statistics",
      "description": "Distribution expressing prior beliefs about the parameters before seeing data."
    },
    {
      "id": "Bayesian Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Predictions made using posterior distribution on parameters."
    },
    {
      "id": "Bayesian Logistic Regression",
      "type": "subnode",
      "parent": "Training Set",
      "description": "Model using logistic regression with a Bayesian approach."
    },
    {
      "id": "Prediction on New Example",
      "type": "subnode",
      "parent": "Posterior Distribution",
      "description": "Making predictions by integrating over posterior distribution of parameters."
    },
    {
      "id": "k-means Algorithm",
      "type": "major",
      "parent": null,
      "description": "Clustering algorithm that partitions data into k clusters."
    },
    {
      "id": "Convergence",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Guaranteed to converge in a certain sense, minimizing distortion function iteratively."
    },
    {
      "id": "Distortion Function",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Measures sum of squared distances between examples and their assigned cluster centroids."
    },
    {
      "id": "Inner-loop Steps",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Assigns each example to closest centroid, then moves centroids to mean of assigned points."
    },
    {
      "id": "Posterior Approximation",
      "type": "subnode",
      "parent": "Bayesian Inference",
      "description": "Approximates the posterior with a point estimate or MAP."
    },
    {
      "id": "MAP Estimate",
      "type": "subnode",
      "parent": "Posterior Approximation",
      "description": "Estimate that maximizes the posterior probability of parameters."
    },
    {
      "id": "MLE vs. MAP",
      "type": "subnode",
      "parent": "MAP Estimate",
      "description": "Comparison between maximum likelihood and Bayesian estimates."
    },
    {
      "id": "Clustering",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups."
    },
    {
      "id": "K-means Algorithm",
      "type": "subnode",
      "parent": "Clustering",
      "description": "Iterative algorithm to partition data into k clusters."
    },
    {
      "id": "Distortion Function J",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Function measuring the quality of clustering in k-means."
    },
    {
      "id": "Convergence Properties",
      "type": "subnode",
      "parent": "k-means Algorithm",
      "description": "Properties related to convergence and local optima in k-means."
    },
    {
      "id": "EM Algorithms",
      "type": "major",
      "parent": null,
      "description": "Expectation-Maximization algorithms for density estimation."
    },
    {
      "id": "EM for Mixture of Gaussians",
      "type": "subnode",
      "parent": "EM Algorithms",
      "description": "Application of EM algorithm to model data with Gaussian distributions."
    },
    {
      "id": "Mixture of Gaussians Model",
      "type": "subnode",
      "parent": "Unsupervised Learning",
      "description": "Model using multiple Gaussian distributions with latent variables."
    },
    {
      "id": "Latent Variables",
      "type": "subnode",
      "parent": "Mixture of Gaussians Model",
      "description": "Hidden random variables that influence the observed data."
    },
    {
      "id": "Joint Distribution",
      "type": "subnode",
      "parent": "Mixture of Gaussians Model",
      "description": "Distribution modeling both latent and observable variables."
    },
    {
      "id": "Likelihood Estimation",
      "type": "subnode",
      "parent": "Model Parameters",
      "description": "Process of estimating parameters based on observed data likelihood."
    },
    {
      "id": "Closed Form Solution",
      "type": "subnode",
      "parent": "Likelihood Estimation",
      "description": "Infeasibility of finding exact solutions for model parameters."
    },
    {
      "id": "Gaussian Mixture Model (GMM)",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Model using a mixture of Gaussian distributions to represent data clusters."
    },
    {
      "id": "Expectation-Maximization (EM) Algorithm",
      "type": "subnode",
      "parent": "Parameter Estimation",
      "description": "An iterative method for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on unobserved latent variables."
    },
    {
      "id": "E-step",
      "type": "subnode",
      "parent": "Expectation-Maximization (EM) Algorithm",
      "description": "Estimation step where posterior distribution of latent variables is calculated given observed data and current parameters."
    },
    {
      "id": "M-step",
      "type": "subnode",
      "parent": "Expectation-Maximization (EM) Algorithm",
      "description": "Maximization step where model parameters are updated to maximize the expected log-likelihood found in E-step."
    },
    {
      "id": "EM Algorithm",
      "type": "major",
      "parent": null,
      "description": "Iterative method for finding maximum likelihood estimates in probabilistic models with latent variables."
    },
    {
      "id": "Gaussian Mixture Model",
      "type": "subnode",
      "parent": "E-step",
      "description": "Model used for clustering data into multiple Gaussian distributions with different means and covariances."
    },
    {
      "id": "Soft Assignments",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Assigns probabilities to each cluster rather than hard assignments, allowing for probabilistic membership in clusters."
    },
    {
      "id": "K-means Clustering",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Clustering algorithm that assigns data points to the nearest cluster center, contrasting with EM's soft assignments."
    },
    {
      "id": "Jensen's Inequality",
      "type": "major",
      "parent": null,
      "description": "A fundamental inequality in probability theory and statistics used for proving convergence properties of algorithms like EM."
    },
    {
      "id": "Concave Functions",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Negative of convex functions, with reversed inequalities in Jensen's inequality."
    },
    {
      "id": "Latent Variable Models",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Models containing unobserved variables that are not directly measured but are inferred from other variables."
    },
    {
      "id": "Expectation-Maximization Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Algorithm used for finding maximum likelihood or maximum a posteriori estimates of parameters in statistical models where the model depends on unobserved latent variables."
    },
    {
      "id": "Convergence Guarantees",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Conditions under which EM algorithm is guaranteed to converge."
    },
    {
      "id": "Strict Convexity",
      "type": "subnode",
      "parent": "Convex Functions",
      "description": "A property of convex functions where the inequality is strict unless the input values are identical."
    },
    {
      "id": "Random Variables and Expectations",
      "type": "subnode",
      "parent": "Jensen's Inequality",
      "description": "Discussion on random variables, their expectations, and how Jensen's inequality applies to them."
    },
    {
      "id": "Machine_Learning_Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of key concepts in machine learning."
    },
    {
      "id": "Probability_Distributions",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Discussion on probability distributions used in ML models."
    },
    {
      "id": "Jensen's_Inequality",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Explanation of Jensen's inequality and its application."
    },
    {
      "id": "Log_Probability_Bound",
      "type": "subnode",
      "parent": "Machine_Learning_Concepts",
      "description": "Derivation of a lower bound on log probability using distributions Q."
    },
    {
      "id": "Optimization Challenges",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Challenges faced when optimizing parameters in non-convex problems."
    },
    {
      "id": "EM Algorithm Overview",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Overview of the Expectation-Maximization algorithm for maximum likelihood estimation."
    },
    {
      "id": "E-step and M-step",
      "type": "subnode",
      "parent": "EM Algorithm Overview",
      "description": "Description of the E-step (expectation) and M-step (maximization) in EM algorithm."
    },
    {
      "id": "Single Example Optimization",
      "type": "subnode",
      "parent": "EM Algorithm Overview",
      "description": "Optimizing likelihood for a single example before extending to multiple examples."
    },
    {
      "id": "Evidence Lower Bound (ELBO)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Objective function used in variational inference to approximate complex probability distributions."
    },
    {
      "id": "Tight Bound Conditions",
      "type": "subnode",
      "parent": "Evidence Lower Bound (ELBO)",
      "description": "Conditions for making the bound tight at a particular value of \u03b8."
    },
    {
      "id": "Posterior Distribution Q(z)",
      "type": "subnode",
      "parent": "Tight Bound Conditions",
      "description": "Setting Q(z) to be the posterior p(z|x;\u03b8) ensures equality in ELBO derivation."
    },
    {
      "id": "Log-Likelihood Maximization",
      "type": "major",
      "parent": null,
      "description": "Objective of EM algorithm is to iteratively improve the log-likelihood function until convergence."
    },
    {
      "id": "Convergence Proof",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Proof showing that each iteration of EM increases or maintains the log-likelihood value."
    },
    {
      "id": "ELBO Interpretation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Different interpretations and forms of Evidence Lower Bound (ELBO)."
    },
    {
      "id": "KL Divergence",
      "type": "subnode",
      "parent": "ELBO Interpretation",
      "description": "Measure of difference between two probability distributions."
    },
    {
      "id": "Conditional Likelihood Maximization",
      "type": "subnode",
      "parent": "ELBO Interpretation",
      "description": "Maximizing likelihood under conditional distribution for simpler inference."
    },
    {
      "id": "Mixture of Gaussians",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Modeling data with a mixture of Gaussian distributions."
    },
    {
      "id": "Log-Likelihood Optimization",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Optimizing the log-likelihood function under single example assumption."
    },
    {
      "id": "Multiple Examples Extension",
      "type": "subnode",
      "parent": "EM Algorithm",
      "description": "Extending the EM algorithm to handle multiple training examples by summing over all examples."
    },
    {
      "id": "Single Example ELBO",
      "type": "subnode",
      "parent": "Evidence Lower Bound (ELBO)",
      "description": "Building the evidence lower bound for a single training example."
    },
    {
      "id": "Multiple Examples ELBO",
      "type": "subnode",
      "parent": "Evidence Lower Bound (ELBO)",
      "description": "Summing over all examples to build the evidence lower bound for multiple training examples."
    },
    {
      "id": "E-step Calculation",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Calculates the expected value of the log-likelihood, given current parameter estimates."
    },
    {
      "id": "M-step Maximization",
      "type": "subnode",
      "parent": "Expectation-Maximization Algorithm",
      "description": "Maximizes the expected log-likelihood found in the E step as a function of the parameters."
    },
    {
      "id": "Parameter Updates",
      "type": "subnode",
      "parent": "M-step Maximization",
      "description": "Updates for \u03c6, \u03bc, and \u03a3 based on maximizing expected log-likelihood."
    },
    {
      "id": "\u03b8 Update Rule",
      "type": "subnode",
      "parent": "Parameter Updates",
      "description": "Rule for updating parameters in the M-step of EM algorithm."
    },
    {
      "id": "Variational Inference",
      "type": "major",
      "parent": null,
      "description": "Technique for approximating probability distributions in Bayesian statistics using optimization methods."
    },
    {
      "id": "ELBO (Evidence Lower Bound)",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Objective function used to approximate the true posterior distribution."
    },
    {
      "id": "Mean Field Assumption",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Assumption that coordinates of latent variables are independent, simplifying optimization."
    },
    {
      "id": "Optimization Objective",
      "type": "subnode",
      "parent": "ELBO (Evidence Lower Bound)",
      "description": "Objective to maximize ELBO over Q and \u03b8."
    },
    {
      "id": "M-step Update Rule",
      "type": "subnode",
      "parent": "Expectation-Maximization (EM) Algorithm",
      "description": "Rule for updating parameters during the maximization step of the EM algorithm."
    },
    {
      "id": "Lagrangian Method",
      "type": "subnode",
      "parent": "M-step Update Rule",
      "description": "Mathematical technique used to find local maxima and minima of a function subject to equality constraints."
    },
    {
      "id": "Variational Auto-Encoder (VAE)",
      "type": "subnode",
      "parent": "Variational Inference",
      "description": "Model used to learn latent variable models with stochastic backpropagation through reparametrization."
    },
    {
      "id": "Re-parametrization Trick",
      "type": "subnode",
      "parent": "Variational Auto-Encoder (VAE)",
      "description": "Method to optimize the evidence lower bound in VAEs by transforming random variables."
    },
    {
      "id": "Neural Network Parameterization",
      "type": "subnode",
      "parent": "Variational Auto-Encoder (VAE)",
      "description": "Parameterizing distributions with neural networks for complex models."
    },
    {
      "id": "Gaussian Mixture Models",
      "type": "subnode",
      "parent": "Machine Learning",
      "description": "Model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters."
    },
    {
      "id": "Gaussian Distributions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "A type of continuous probability distribution used in machine learning for modeling data."
    },
    {
      "id": "Continuous Latent Variables",
      "type": "subnode",
      "parent": "Latent Variable Models",
      "description": "Focus on continuous latent variables in models."
    },
    {
      "id": "Gaussian Distribution Assumption",
      "type": "subnode",
      "parent": "Continuous Latent Variables",
      "description": "Assuming Gaussian distribution for simplifying calculations."
    },
    {
      "id": "Succinct Representation of Means",
      "type": "subnode",
      "parent": "Continuous Latent Variables",
      "description": "Using functions to represent means succinctly."
    },
    {
      "id": "ELBO Optimization",
      "type": "subnode",
      "parent": "Continuous Latent Variables",
      "description": "Optimizing Evidence Lower Bound (ELBO) in the context of continuous latent variables."
    },
    {
      "id": "Reparameterization Trick",
      "type": "subnode",
      "parent": "Gradient Calculation",
      "description": "A technique used to compute gradients of expectations with respect to random variable parameters by introducing a differentiable transformation."
    },
    {
      "id": "Gradient Estimation",
      "type": "subnode",
      "parent": "Reparameterization Trick",
      "description": "Estimating the gradient of the expected value of a function with respect to its parameters using samples from a distribution."
    },
    {
      "id": "Principal Components Analysis (PCA)",
      "type": "major",
      "parent": null,
      "description": "A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components."
    },
    {
      "id": "RedundancyDetection",
      "type": "major",
      "parent": null,
      "description": "Detecting and removing redundant attributes in data sets."
    },
    {
      "id": "PCAAlgorithm",
      "type": "subnode",
      "parent": "DataPreprocessingAndAnalysis",
      "description": "Principal Component Analysis for dimensionality reduction."
    },
    {
      "id": "DataNormalization",
      "type": "subnode",
      "parent": "DataPreprocessingAndAnalysis",
      "description": "Normalizing data features to have mean 0 and variance 1."
    },
    {
      "id": "LinearDependency",
      "type": "subnode",
      "parent": "RedundancyDetection",
      "description": "Identifying linearly dependent attributes in datasets."
    },
    {
      "id": "SurveyExample",
      "type": "subnode",
      "parent": "RedundancyDetection",
      "description": "Illustrative example using a survey of RC helicopter pilots."
    },
    {
      "id": "DataPreprocessingAndAnalysis",
      "type": "major",
      "parent": null,
      "description": "Techniques for preprocessing and analyzing data before applying PCA."
    },
    {
      "id": "Data Normalization",
      "type": "major",
      "parent": null,
      "description": "Process of standardizing data attributes to have zero mean and unit variance."
    },
    {
      "id": "Mean Subtraction",
      "type": "subnode",
      "parent": "Data Normalization",
      "description": "Step in normalization where the mean is subtracted from each feature."
    },
    {
      "id": "Variance Scaling",
      "type": "subnode",
      "parent": "Data Normalization",
      "description": "Process of dividing by standard deviation to ensure unit variance."
    },
    {
      "id": "Comparability Across Attributes",
      "type": "subnode",
      "parent": "Variance Scaling",
      "description": "Ensures different attributes are treated on the same scale after normalization."
    },
    {
      "id": "Major Axis of Variation",
      "type": "major",
      "parent": null,
      "description": "Direction in which data varies most significantly, maximizing variance of projected data."
    },
    {
      "id": "Projection onto Direction u",
      "type": "subnode",
      "parent": "Major Axis of Variation",
      "description": "Process of projecting data to find the direction with maximum variance."
    },
    {
      "id": "Principal Component Analysis (PCA)",
      "type": "major",
      "parent": null,
      "description": "Statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components."
    },
    {
      "id": "Projection of Data Points",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Process of projecting data points onto a unit vector to reduce dimensions"
    },
    {
      "id": "Variance Maximization",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Objective to maximize variance in projections for optimal dimensionality reduction"
    },
    {
      "id": "Empirical Covariance Matrix",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Matrix representing the covariance of data points used in PCA calculations"
    },
    {
      "id": "Eigenvectors and Eigenvalues",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Key components for finding optimal projections in PCA"
    },
    {
      "id": "PCA",
      "type": "major",
      "parent": null,
      "description": "Principal Component Analysis for dimensionality reduction and data visualization."
    },
    {
      "id": "Dimensionality Reduction",
      "type": "subnode",
      "parent": "PCA",
      "description": "Reduces high-dimensional data to a lower-dimensional representation while preserving variability."
    },
    {
      "id": "Eigenvectors of Sigma",
      "type": "subnode",
      "parent": "PCA",
      "description": "Top k eigenvectors form an orthogonal basis for the data."
    },
    {
      "id": "Principal Components",
      "type": "subnode",
      "parent": "Dimensionality Reduction",
      "description": "First k principal components are used to represent data in a lower-dimensional space."
    },
    {
      "id": "Approximation Error Minimization",
      "type": "subnode",
      "parent": "PCA",
      "description": "Derivation of PCA by minimizing the error from projecting data onto a k-dimensional subspace."
    },
    {
      "id": "Data Visualization",
      "type": "subnode",
      "parent": "Dimensionality Reduction",
      "description": "Reduces high dimensional data to 2 or 3 dimensions for visualization purposes."
    },
    {
      "id": "Compression",
      "type": "subnode",
      "parent": "Dimensionality Reduction",
      "description": "Represents high-dimensional data with lower-dimensional vectors for efficient storage and processing."
    },
    {
      "id": "Independent Component Analysis (ICA)",
      "type": "major",
      "parent": null,
      "description": "Statistical and computational method for revealing hidden factors that underlie sets of random variables, measurements, or signals."
    },
    {
      "id": "Cocktail Party Problem",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Example scenario where ICA is applied to separate overlapping audio signals."
    },
    {
      "id": "Mixing Matrix A",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Matrix representing the mixing process of original sources into observed data."
    },
    {
      "id": "Unmixing Matrix W",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Inverse matrix used to recover original source signals from mixed observations."
    },
    {
      "id": "ICA Ambiguities",
      "type": "major",
      "parent": null,
      "description": "Discussion on the limitations and uncertainties in recovering the unmixing matrix W without prior knowledge."
    },
    {
      "id": "Permutation Matrix",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Describes the role of permutation matrices in ICA ambiguities."
    },
    {
      "id": "Scaling Ambiguity",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Explains how scaling factors do not affect observed data in ICA."
    },
    {
      "id": "Sign Change Irrelevance",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Discusses the irrelevance of sign changes in source recovery."
    },
    {
      "id": "Applications of PCA",
      "type": "subnode",
      "parent": "Principal Component Analysis (PCA)",
      "description": "Various applications including clustering, noise reduction, and face recognition."
    },
    {
      "id": "Noise Reduction",
      "type": "subnode",
      "parent": "Applications of PCA",
      "description": "Estimating intrinsic features from noisy data measurements."
    },
    {
      "id": "Eigenfaces Method",
      "type": "subnode",
      "parent": "Applications of PCA",
      "description": "Face recognition technique using principal components analysis on face images."
    },
    {
      "id": "Scaling Factor",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Impact of scaling a signal by a positive factor or changing its sign."
    },
    {
      "id": "Non-Gaussian Sources",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Sources are non-Gaussian, which resolves certain ambiguities."
    },
    {
      "id": "Gaussian Data Example",
      "type": "subnode",
      "parent": "ICA Ambiguities",
      "description": "Example showing issues with Gaussian data in ICA."
    },
    {
      "id": "Rotation Matrix R",
      "type": "subnode",
      "parent": "Gaussian Data Example",
      "description": "Introduction to an orthogonal rotation matrix and its effect on data."
    },
    {
      "id": "Density Transformation",
      "type": "major",
      "parent": null,
      "description": "Transformation of density functions under linear mappings."
    },
    {
      "id": "1D Example",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Example in one dimension illustrating the transformation formula."
    },
    {
      "id": "General Case",
      "type": "subnode",
      "parent": "Density Transformation",
      "description": "Extension of density transformation to vector-valued distributions."
    },
    {
      "id": "Volume Calculation",
      "type": "subnode",
      "parent": "General Case",
      "description": "Calculation of volume changes under linear transformations."
    },
    {
      "id": "ICA Algorithm",
      "type": "major",
      "parent": null,
      "description": "Derivation and interpretation of an ICA algorithm based on maximum likelihood estimation."
    },
    {
      "id": "Mixing Matrix Ambiguity",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Explains the ambiguity in determining the mixing matrix due to rotational symmetry."
    },
    {
      "id": "Non-Gaussian Data Recovery",
      "type": "subnode",
      "parent": "Independent Component Analysis (ICA)",
      "description": "Discusses the possibility of recovering sources from non-Gaussian data."
    },
    {
      "id": "Linear Transformations and Densities",
      "type": "major",
      "parent": null,
      "description": "Exploration of how linear transformations affect probability densities in ICA context."
    },
    {
      "id": "ICA Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to Independent Component Analysis concepts and principles."
    },
    {
      "id": "Distribution of Sources",
      "type": "subnode",
      "parent": "ICA Overview",
      "description": "Assumption that source distributions are independent and modeled as product densities."
    },
    {
      "id": "Joint Distribution Model",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation",
      "description": "Modeling the joint distribution of sources as a product of marginal densities."
    },
    {
      "id": "Density on x",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation",
      "description": "Deriving density function for observed data based on source distributions and mixing matrix."
    },
    {
      "id": "Cumulative Distribution Function (CDF)",
      "type": "subnode",
      "parent": "Distribution of Sources",
      "description": "Definition and properties of CDF in the context of ICA source distribution modeling."
    },
    {
      "id": "Sigmoid Function as Default Density",
      "type": "subnode",
      "parent": "Cumulative Distribution Function (CDF)",
      "description": "Using sigmoid function to model source densities due to its desirable properties for non-Gaussian data."
    },
    {
      "id": "Training Methods",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Various techniques used to train models such as stochastic gradient ascent."
    },
    {
      "id": "Self-supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning paradigm that uses unlabeled data and self-generated labels for training."
    },
    {
      "id": "Foundation Models",
      "type": "subnode",
      "parent": "Self-supervised Learning",
      "description": "Large models pre-trained on vast amounts of data and adaptable to various tasks with minimal labeled data."
    },
    {
      "id": "Pretraining Phase",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Initial training phase using unlabeled data to learn general representations."
    },
    {
      "id": "Adaptation Phase",
      "type": "subnode",
      "parent": "Foundation Models",
      "description": "Phase where the pretrained model is fine-tuned for specific tasks with limited labeled data."
    },
    {
      "id": "Transfer Learning",
      "type": "major",
      "parent": "Machine Learning Overview",
      "description": "Technique where a pre-trained model is adapted for specific tasks."
    },
    {
      "id": "Unlabeled Data",
      "type": "subnode",
      "parent": "Pretraining Phase",
      "description": "Dataset used in the initial training phase without labels."
    },
    {
      "id": "Labeled Data",
      "type": "subnode",
      "parent": "Adaptation Phase",
      "description": "Dataset with labeled examples for fine-tuning the model."
    },
    {
      "id": "Self-Supervised Learning",
      "type": "subnode",
      "parent": "Pretraining Phase",
      "description": "Learning from data where supervision is derived from the data itself."
    },
    {
      "id": "Data Preprocessing",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques to prepare data for modeling, including mean normalization."
    },
    {
      "id": "Stochastic Gradient Ascent",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Optimization technique for updating model parameters in machine learning."
    },
    {
      "id": "Parameter Matrix W",
      "type": "subnode",
      "parent": "Likelihood Estimation",
      "description": "Square matrix used as a parameter in the likelihood estimation process."
    },
    {
      "id": "Learning Rate Alpha",
      "type": "subnode",
      "parent": "Stochastic Gradient Ascent",
      "description": "Hyperparameter controlling the step size in each iteration of the update rule."
    },
    {
      "id": "Machine Learning Adaptation Methods",
      "type": "major",
      "parent": null,
      "description": "Methods for adapting machine learning models to new tasks."
    },
    {
      "id": "Labeled Dataset",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Dataset used in downstream tasks with labeled examples."
    },
    {
      "id": "Zero-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Scenario where no labeled data is available for the task."
    },
    {
      "id": "Few-Shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Demonstration that language models can perform well on few-shot learning tasks."
    },
    {
      "id": "Adaptation Algorithm",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Algorithm that takes in downstream dataset and pretrained model to output adapted model."
    },
    {
      "id": "Linear Probe Approach",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Uses a linear head on top of the representation for prediction without finetuning the pretrained model."
    },
    {
      "id": "Finetuning Algorithm",
      "type": "subnode",
      "parent": "Adaptation Algorithm",
      "description": "Further trains both the downstream prediction model and the pretrained model."
    },
    {
      "id": "Finetuning Pretrained Models",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Methods",
      "description": "Process of adjusting pretrained model parameters for specific tasks."
    },
    {
      "id": "Prediction Model Structure",
      "type": "subnode",
      "parent": "Finetuning Pretrained Models",
      "description": "Structure involving both fixed and trainable parts of the model."
    },
    {
      "id": "Optimization Process",
      "type": "subnode",
      "parent": "Finetuning Pretrained Models",
      "description": "Minimizing loss function with respect to parameters w and theta."
    },
    {
      "id": "Pretraining Methods in CV",
      "type": "major",
      "parent": null,
      "description": "Techniques for pretraining models specifically in computer vision tasks."
    },
    {
      "id": "Supervised Pretraining",
      "type": "subnode",
      "parent": "Pretraining Methods in CV",
      "description": "Training with large labeled datasets to learn general representations."
    },
    {
      "id": "Contrastive Learning",
      "type": "subnode",
      "parent": "Pretraining Methods in CV",
      "description": "Technique for learning representations by contrasting positive and negative pairs."
    },
    {
      "id": "SIMCLR",
      "type": "subnode",
      "parent": "Contrastive Learning",
      "description": "Algorithm based on contrastive learning principle introduced in 2020."
    },
    {
      "id": "Augmentation Techniques",
      "type": "subnode",
      "parent": "SIMCLR",
      "description": "Techniques applied to data to increase variability and improve learning."
    },
    {
      "id": "Representation Function",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Maps semantically similar images to similar representations."
    },
    {
      "id": "Supervised Contrastive Algorithms",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Works well with labeled pretraining datasets."
    },
    {
      "id": "Data Augmentation",
      "type": "subnode",
      "parent": "Self-Supervised Learning",
      "description": "Generates pairs of augmented images from the same original image."
    },
    {
      "id": "Positive Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Augmented versions of the same image, semantically related."
    },
    {
      "id": "Negative Pair",
      "type": "subnode",
      "parent": "Data Augmentation",
      "description": "Randomly selected images and their augmentations, not necessarily semantically related."
    },
    {
      "id": "Loss Function Analysis",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Analysis of loss functions in machine learning models."
    },
    {
      "id": "Pretrained Large Language Models",
      "type": "major",
      "parent": null,
      "description": "Overview of pretraining large language models in NLP."
    },
    {
      "id": "Natural Language Processing",
      "type": "subnode",
      "parent": "Pretrained Large Language Models",
      "description": "Introduction to natural language processing and its applications."
    },
    {
      "id": "Language Model Probability Distribution",
      "type": "subnode",
      "parent": "Pretrained Large Language Models",
      "description": "Explanation of the probability distribution in language models."
    },
    {
      "id": "Conditional Probability Modeling",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Modeling the probability of an event given prior events."
    },
    {
      "id": "Parameterized Model",
      "type": "subnode",
      "parent": "Conditional Probability Modeling",
      "description": "A model parameterized by \u03b8 to predict conditional probabilities."
    },
    {
      "id": "Embeddings",
      "type": "subnode",
      "parent": "Parameterized Model",
      "description": "Numerical representation of words used as input for models."
    },
    {
      "id": "Transformer Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A model widely used in natural language processing tasks."
    },
    {
      "id": "Input-Output Interface",
      "type": "subnode",
      "parent": "Transformer Model",
      "description": "Interface describing how data is processed by the Transformer."
    },
    {
      "id": "Language Models",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Models that generate text based on learned patterns from large datasets."
    },
    {
      "id": "Temperature Parameter",
      "type": "subnode",
      "parent": "Language Models",
      "description": "Parameter used to adjust the randomness of generated text in language models."
    },
    {
      "id": "Finetuning",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Process of adapting a pretrained model to specific tasks by further training on task-specific data."
    },
    {
      "id": "Zero-shot Learning",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Adapting models to new tasks without additional training data."
    },
    {
      "id": "In-context Learning",
      "type": "subnode",
      "parent": "Machine Learning Techniques",
      "description": "Learning from a small amount of context or examples provided during inference time."
    },
    {
      "id": "Machine Learning Adaptation Techniques",
      "type": "major",
      "parent": null,
      "description": "Techniques for adapting machine learning models to new tasks."
    },
    {
      "id": "Zero-shot Adaptation",
      "type": "subnode",
      "parent": "Machine Learning Adaptation Techniques",
      "description": "Adapting models without input-output pairs from downstream tasks."
    },
    {
      "id": "Language Model Utilization",
      "type": "subnode",
      "parent": "Zero-shot Adaptation",
      "description": "Methods for decoding answers from language models."
    },
    {
      "id": "Prompt Construction",
      "type": "subnode",
      "parent": "In-context Learning",
      "description": "Creating prompts by concatenating labeled examples and test data."
    },
    {
      "id": "Transformers",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "A type of deep learning model used for sequence data tasks."
    },
    {
      "id": "Training Process",
      "type": "subnode",
      "parent": "Transformers",
      "description": "Process of training a transformer model using loss functions."
    },
    {
      "id": "Autoregressive Decoding",
      "type": "subnode",
      "parent": "Transformers",
      "description": "Process of generating text sequentially using a trained transformer model."
    },
    {
      "id": "Reinforcement Learning",
      "type": "major",
      "parent": null,
      "description": "Type of machine learning where an agent learns to behave in an environment by performing actions and receiving rewards."
    },
    {
      "id": "Sequential Decision Making",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Decision making in sequential processes without explicit supervision."
    },
    {
      "id": "Reward Function",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Function that determines the reward given to an agent based on its actions and state transitions."
    },
    {
      "id": "Policy Execution",
      "type": "major",
      "parent": null,
      "description": "The process of selecting actions based on a policy in given states."
    },
    {
      "id": "Value Function",
      "type": "subnode",
      "parent": "Policy Execution",
      "description": "Function that gives the expected utility starting from each state under a specific policy."
    },
    {
      "id": "Bellman Equations",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Set of equations used to solve for the value function in a finite-state MDP."
    },
    {
      "id": "Immediate Reward",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Reward received immediately upon entering state s."
    },
    {
      "id": "Expected Future Rewards",
      "type": "subnode",
      "parent": "Bellman Equations",
      "description": "Summation of expected discounted rewards after the first action in an MDP."
    },
    {
      "id": "Optimal Value Function",
      "type": "subnode",
      "parent": "Value Function",
      "description": "The best possible value function attainable by any policy"
    },
    {
      "id": "Bellman's Equation",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Equation used in dynamic programming to find the optimal policy and value function in MDPs."
    },
    {
      "id": "Optimal Policy",
      "type": "subnode",
      "parent": "Value Function",
      "description": "Policy that maximizes the expected sum of discounted rewards for all states"
    },
    {
      "id": "Markov Decision Processes (MDP)",
      "type": "subnode",
      "parent": "Reinforcement Learning",
      "description": "Formal framework for modeling decision-making situations in which outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "States",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Set of all possible conditions or configurations an agent can be in."
    },
    {
      "id": "Actions",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Possible actions that the decision maker can take in each state."
    },
    {
      "id": "State Transition Probabilities",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Probabilities of moving from one state to another given an action."
    },
    {
      "id": "Discount Factor",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Parameter that determines the present value of future rewards in reinforcement learning problems."
    },
    {
      "id": "Reinforcement Learning Overview",
      "type": "major",
      "parent": null,
      "description": "Overview of reinforcement learning concepts and processes."
    },
    {
      "id": "Markov Decision Process (MDP)",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "A framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker."
    },
    {
      "id": "State Transition",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "The process by which an agent moves from one state to another based on actions taken."
    },
    {
      "id": "Action Selection",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Choosing the next action in a sequence of states and transitions."
    },
    {
      "id": "Total Payoff Calculation",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "Calculating the total reward over time, considering discount factors."
    },
    {
      "id": "Discount Factor (\u03b3)",
      "type": "subnode",
      "parent": "Total Payoff Calculation",
      "description": "A factor that discounts future rewards to reflect their reduced value compared to immediate rewards."
    },
    {
      "id": "Policy Definition",
      "type": "subnode",
      "parent": "Reinforcement Learning Overview",
      "description": "A strategy or rule for choosing actions in different states of the environment."
    },
    {
      "id": "Value Iteration",
      "type": "subnode",
      "parent": "Algorithm",
      "description": "A dynamic programming algorithm used for solving Markov decision processes, iteratively updating state values until convergence."
    },
    {
      "id": "Policy Iteration",
      "type": "subnode",
      "parent": "Algorithm",
      "description": "An iterative method to find the optimal policy in reinforcement learning by alternating between policy evaluation and improvement."
    },
    {
      "id": "Synchronous Update",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Updating all state values simultaneously before overwriting old values."
    },
    {
      "id": "Asynchronous Update",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Updating one state value at a time in sequence."
    },
    {
      "id": "Continuous State MDPs",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Markov Decision Processes with infinite state spaces."
    },
    {
      "id": "Markov Decision Processes (MDPs)",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Models for decision-making problems under uncertainty."
    },
    {
      "id": "Learning Model Parameters",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDPs)",
      "description": "Estimating transition probabilities and rewards from observed data in MDPs."
    },
    {
      "id": "Inverted Pendulum Problem",
      "type": "subnode",
      "parent": "Learning Model Parameters",
      "description": "Example problem used to illustrate learning model parameters in MDPs."
    },
    {
      "id": "Expected Immediate Reward",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Average reward observed in a particular state."
    },
    {
      "id": "Model Learning",
      "type": "subnode",
      "parent": "Markov Decision Process (MDP)",
      "description": "Process of learning the transition probabilities and rewards from experience."
    },
    {
      "id": "Continuous State Space",
      "type": "major",
      "parent": null,
      "description": "Extension of MDPs where states are continuous rather than discrete."
    },
    {
      "id": "Convergence of Value Iteration",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "The process by which the state values converge to their optimal values in value iteration."
    },
    {
      "id": "Optimal Policy from Value Iteration",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Using converged state values to determine the best policy for an MDP."
    },
    {
      "id": "Policy Evaluation",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Step in policy iteration where the value function is computed given a fixed policy."
    },
    {
      "id": "Policy Improvement",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Step in policy iteration where the policy is updated based on the current value function."
    },
    {
      "id": "Greedy Policy with Respect to V",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "A policy that chooses actions maximizing expected rewards given a state-value function."
    },
    {
      "id": "Bellman's Equations",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "Set of equations used to determine the optimal value function in MDPs."
    },
    {
      "id": "Comparison of Value Iteration and Policy Iteration",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Discussion on when each algorithm is preferred based on problem size and characteristics."
    },
    {
      "id": "Model Creation Methods",
      "type": "major",
      "parent": null,
      "description": "Different methods to create a model for state transitions in machine learning."
    },
    {
      "id": "Physics Simulation",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Using physical laws or software packages to simulate system behavior."
    },
    {
      "id": "Open Dynamics Engine",
      "type": "subnode",
      "parent": "Physics Simulation",
      "description": "Free/open-source physics simulator used for simulating mechanical systems."
    },
    {
      "id": "Learning from Data",
      "type": "subnode",
      "parent": "Model Creation Methods",
      "description": "Deriving a model based on data collected through repeated trials in an MDP."
    },
    {
      "id": "Discretization Methods",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for converting continuous state spaces into discrete ones."
    },
    {
      "id": "Curse of Dimensionality",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Problem where the number of possible states grows exponentially with dimension, making discretization impractical for high-dimensional spaces."
    },
    {
      "id": "Continuous-State MDPs",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Discussion on challenges and solutions for continuous-state Markov Decision Processes (MDPs)."
    },
    {
      "id": "Discretization Method",
      "type": "subnode",
      "parent": "Continuous-State MDPs",
      "description": "Approach to solving problems by converting continuous states into discrete ones."
    },
    {
      "id": "Value Function Approximation",
      "type": "subnode",
      "parent": "Continuous-State MDPs",
      "description": "Alternative method to find policies by approximating value functions directly without discretizing state space."
    },
    {
      "id": "Model or Simulator",
      "type": "subnode",
      "parent": "Value Function Approximation",
      "description": "Black-box tool that simulates transitions from a given state and action in an MDP."
    },
    {
      "id": "Linear Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Predicts next state as a linear function of the current state and action."
    },
    {
      "id": "Deterministic Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Outputs are exactly determined given inputs."
    },
    {
      "id": "Stochastic Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Models output as a random function of the input with noise."
    },
    {
      "id": "Non-linear Functions",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Uses non-linear feature mappings for state and action predictions."
    },
    {
      "id": "Model Representation",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Representation of models using state-action mappings."
    },
    {
      "id": "Non-linear Feature Mappings",
      "type": "subnode",
      "parent": "Model Representation",
      "description": "Use of non-linear functions to map states and actions."
    },
    {
      "id": "Fitted Value Iteration",
      "type": "major",
      "parent": null,
      "description": "Algorithm for approximating the value function using a finite sample of states and supervised learning."
    },
    {
      "id": "Discrete Action Space",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Assumption of a small and discrete action space."
    },
    {
      "id": "Supervised Learning Algorithm",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Use of linear regression to approximate the value function based on state samples."
    },
    {
      "id": "State Sampling",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Random sampling of states to approximate the value function iteratively."
    },
    {
      "id": "Convergence Issues",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Discussion on the convergence properties of fitted value iteration."
    },
    {
      "id": "Deterministic Simulators",
      "type": "subnode",
      "parent": "Fitted Value Iteration",
      "description": "Simplification when using deterministic models in MDPs."
    },
    {
      "id": "Expectation Approximation",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Methods for approximating expectations in value iteration."
    },
    {
      "id": "Gaussian Noise Models",
      "type": "subnode",
      "parent": "Expectation Approximation",
      "description": "Models where noise is Gaussian and can be approximated by setting it to zero."
    },
    {
      "id": "Computational Complexity",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Discussion on the computational cost of sampling states in value iteration."
    },
    {
      "id": "VE Procedure",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Procedure to evaluate the value function under a given policy in reinforcement learning algorithms."
    },
    {
      "id": "Algorithm 6",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "A variant of policy iteration that uses VE procedure with parameter k for intermediate updates between policy evaluation and improvement."
    },
    {
      "id": "k Parameter",
      "type": "subnode",
      "parent": "VE Procedure",
      "description": "Hyperparameter controlling the number of iterations in the value function update process within VE procedure."
    },
    {
      "id": "Optimal Bellman Equation",
      "type": "major",
      "parent": null,
      "description": "Equation defining the optimal value function for MDPs."
    },
    {
      "id": "k Steps Update",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Number of steps in an update cycle for value iteration or policy iteration."
    },
    {
      "id": "Procedure VE",
      "type": "subnode",
      "parent": "Policy Iteration",
      "description": "Algorithm used to compute the optimal value function over multiple iterations."
    },
    {
      "id": "Linear System Solver",
      "type": "subnode",
      "parent": "Procedure VE",
      "description": "Tool that can solve linear systems faster than iterative methods for large k."
    },
    {
      "id": "Optimal Policy Recovery",
      "type": "subnode",
      "parent": "Optimal Bellman Equation",
      "description": "Method to derive the optimal policy from the optimal value function."
    },
    {
      "id": "Expectation in MDPs",
      "type": "major",
      "parent": null,
      "description": "Rewriting expectation as sum or integral based on state space."
    },
    {
      "id": "Rewards Dependence",
      "type": "subnode",
      "parent": "Expectation in MDPs",
      "description": "Rewards depend on both states and actions."
    },
    {
      "id": "Optimal Action Computation",
      "type": "subnode",
      "parent": "Rewards Dependence",
      "description": "Computing optimal action considering state-action rewards and expected future value."
    },
    {
      "id": "Finite Horizon MDPs",
      "type": "major",
      "parent": null,
      "description": "MDPs with a fixed time horizon T."
    },
    {
      "id": "Time Horizon Definition",
      "type": "subnode",
      "parent": "Finite Horizon MDPs",
      "description": "Definition of finite horizon MDP as tuple (S,A,Psa,T,R)."
    },
    {
      "id": "Payoff Calculation",
      "type": "subnode",
      "parent": "Finite Horizon MDPs",
      "description": "Calculation of payoff in finite vs infinite horizon scenarios."
    },
    {
      "id": "Discount Factor Role",
      "type": "subnode",
      "parent": "Finite Horizon MDPs",
      "description": "Explanation of discount factor's role and necessity in different horizons."
    },
    {
      "id": "Non-Stationary Optimal Policy",
      "type": "major",
      "parent": null,
      "description": "Discussion on why optimal policy is non-stationary in a finite-horizon setting."
    },
    {
      "id": "Policy Dynamics",
      "type": "subnode",
      "parent": "Non-Stationary Optimal Policy",
      "description": "Dynamics of optimal policies at different time steps."
    },
    {
      "id": "Policy in MDPs",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Description of how policy changes over time in Markov Decision Processes (MDPs)."
    },
    {
      "id": "Finite Horizon MDP",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Explanation of finite horizon setting and its implications on policies."
    },
    {
      "id": "Time Dependent Dynamics",
      "type": "subnode",
      "parent": "Finite Horizon MDP",
      "description": "Explanation of how dynamics and rewards change over time in an MDP."
    },
    {
      "id": "Value Function Definition",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Definition of value function at a given time for a policy in the context of finite horizon MDPs."
    },
    {
      "id": "Dynamic Programming",
      "type": "subnode",
      "parent": "Markov Decision Processes (MDP)",
      "description": "A method for solving complex problems by breaking them down into simpler subproblems."
    },
    {
      "id": "Value Iteration Algorithm",
      "type": "subnode",
      "parent": "Dynamic Programming",
      "description": "An iterative algorithm for solving Bellman's equation to find the optimal policy in an MDP."
    },
    {
      "id": "Quadratic Assumption",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Assumes that the value functions are quadratic for simplification."
    },
    {
      "id": "Dynamics Model",
      "type": "subnode",
      "parent": "Optimal Value Function",
      "description": "Incorporates dynamics of the system to predict future states."
    },
    {
      "id": "Optimal Policy Derivation",
      "type": "major",
      "parent": null,
      "description": "Derives the optimal policy as a linear function of state."
    },
    {
      "id": "Linear Optimal Action",
      "type": "subnode",
      "parent": "Optimal Policy Derivation",
      "description": "Determines that the optimal action is linear in state."
    },
    {
      "id": "LQR Model Assumptions",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Assumptions made for the Linear Quadratic Regulator model."
    },
    {
      "id": "LQR Algorithm Steps",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Steps involved in implementing the LQR algorithm."
    },
    {
      "id": "Step 1: Estimate Matrices",
      "type": "subnode",
      "parent": "LQR Algorithm Steps",
      "description": "Collect data and use linear regression to estimate matrices A, B, and Sigma."
    },
    {
      "id": "Value Approximation",
      "type": "subnode",
      "parent": "Step 1: Estimate Matrices",
      "description": "Use value approximation techniques to improve matrix estimation accuracy."
    },
    {
      "id": "Step 2: Derive Optimal Policy",
      "type": "subnode",
      "parent": "LQR Algorithm Steps",
      "description": "Derive the optimal policy using dynamic programming given known model parameters."
    },
    {
      "id": "Dynamic Programming Application",
      "type": "subnode",
      "parent": "Step 2: Derive Optimal Policy",
      "description": "Apply dynamic programming to compute V_t* for each time step."
    },
    {
      "id": "Bellman Update",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Operator used to update value functions in reinforcement learning problems."
    },
    {
      "id": "Geometric Convergence",
      "type": "subnode",
      "parent": "Value Iteration",
      "description": "Rate of convergence for the approximation error in value iteration."
    },
    {
      "id": "Theorem on Bellman Operator",
      "type": "subnode",
      "parent": "Bellman Update",
      "description": "Mathematical proof showing that the Bellman operator is a contracting operator."
    },
    {
      "id": "Continuous Setting Assumptions",
      "type": "subnode",
      "parent": "Linear Quadratic Regulation (LQR)",
      "description": "Assumptions about the state space, action space, and transition dynamics in LQR problems."
    },
    {
      "id": "Linear Transitions",
      "type": "subnode",
      "parent": "Continuous Setting Assumptions",
      "description": "Model of system transitions with linear equations and Gaussian noise."
    },
    {
      "id": "Quadratic Rewards",
      "type": "subnode",
      "parent": "Continuous Setting Assumptions",
      "description": "Reward function that is quadratic in state and action variables, ensuring negative rewards for non-zero values."
    },
    {
      "id": "Linear Quadratic Regulator (LQR)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Optimal control algorithm for linear systems with quadratic cost functions."
    },
    {
      "id": "Discrete Ricatti Equations",
      "type": "subnode",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "Set of equations used to find the optimal policy in LQR problems."
    },
    {
      "id": "Optimal Policy Independence from Noise",
      "type": "subnode",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "The optimal control policy does not depend on noise parameters."
    },
    {
      "id": "Algorithm Steps for LQR",
      "type": "subnode",
      "parent": "Linear Quadratic Regulator (LQR)",
      "description": "Steps to solve an LQR problem, including parameter estimation and iterative updates."
    },
    {
      "id": "Efficiency Improvements in LQR",
      "type": "subnode",
      "parent": "Algorithm Steps for LQR",
      "description": "Optimization techniques that reduce computational complexity by updating only necessary parameters."
    },
    {
      "id": "Non-linear Dynamics to LQR",
      "type": "major",
      "parent": null,
      "description": "Extension of linear control theory to non-linear systems through approximation methods."
    },
    {
      "id": "Inverted Pendulum Example",
      "type": "subnode",
      "parent": "Non-linear Dynamics to LQR",
      "description": "Illustration of how non-linear dynamics can be approximated for use with LQR algorithms."
    },
    {
      "id": "Optimization in RL",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for optimizing policies in reinforcement learning."
    },
    {
      "id": "LQR Framework",
      "type": "subnode",
      "parent": "Optimization in RL",
      "description": "Linear Quadratic Regulator framework used to find optimal control policies."
    },
    {
      "id": "LQG",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Gaussian framework for handling state uncertainty in control systems."
    },
    {
      "id": "State Transition Equation",
      "type": "subnode",
      "parent": "Inverted Pendulum Example",
      "description": "Equation describing state transitions in the inverted pendulum system."
    },
    {
      "id": "Linearization of Dynamics",
      "type": "major",
      "parent": null,
      "description": "Approximating system dynamics around each point in the trajectory with linear equations."
    },
    {
      "id": "Taylor Expansion",
      "type": "subnode",
      "parent": "Linearization of dynamics",
      "description": "Use of Taylor expansion to approximate nonlinear functions with linear ones."
    },
    {
      "id": "LQR Assumptions",
      "type": "subnode",
      "parent": "Linearization of dynamics",
      "description": "Connection between linearized system and assumptions in Linear Quadratic Regulator (LQR)."
    },
    {
      "id": "Constant Term Absorption",
      "type": "subnode",
      "parent": "Linearization of dynamics",
      "description": "Technique to incorporate constant terms into the system by increasing dimensionality."
    },
    {
      "id": "Nominal Trajectory Generation",
      "type": "subnode",
      "parent": "Differential Dynamic Programming (DDP)",
      "description": "Creating an initial approximate trajectory using a naive controller."
    },
    {
      "id": "Rewriting System Dynamics",
      "type": "subnode",
      "parent": "Linearization of Dynamics",
      "description": "Expressing state transitions using matrices A and B for linearized dynamics."
    },
    {
      "id": "Reward Function Approximation",
      "type": "subnode",
      "parent": "Differential Dynamic Programming (DDP)",
      "description": "Approximating the reward function with a second-order Taylor expansion around each point."
    },
    {
      "id": "LQR Updates",
      "type": "subnode",
      "parent": "Machine Learning Algorithms",
      "description": "Backward pass computations in LQR updates."
    },
    {
      "id": "Randomized Policy",
      "type": "subnode",
      "parent": "Policy Gradient (REINFORCE)",
      "description": "Learning a policy that outputs actions probabilistically based on state."
    },
    {
      "id": "Expected Total Payoff",
      "type": "subnode",
      "parent": "Policy Gradient (REINFORCE)",
      "description": "Objective function for optimizing the policy parameters over trajectories."
    },
    {
      "id": "Partially Observable MDPs (POMDP)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "MDPs extended to handle situations where the full state is not observable."
    },
    {
      "id": "Observation Layer",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Additional layer in POMDP that introduces observations based on current state."
    },
    {
      "id": "Belief State",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Distribution over states maintained based on past observations."
    },
    {
      "id": "Policy Mapping",
      "type": "subnode",
      "parent": "Partially Observable MDPs (POMDP)",
      "description": "Mapping of belief states to actions in POMDP framework."
    },
    {
      "id": "LQR Extension",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Extension of Linear Quadratic Regulator to partially observable settings."
    },
    {
      "id": "Kalman Filter",
      "type": "subnode",
      "parent": "LQR Extension",
      "description": "Algorithm used for efficient computation and updating belief states over time."
    },
    {
      "id": "Step 2",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Use mean as best approximation for state at time t."
    },
    {
      "id": "Step 3",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Set action based on linear transformation of estimated state."
    },
    {
      "id": "Predict Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Estimates the next state based on current state distribution."
    },
    {
      "id": "Update Step",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Refines the estimate using new observation data."
    },
    {
      "id": "LQR Algorithm",
      "type": "major",
      "parent": null,
      "description": "Linear Quadratic Regulator for optimal control problems."
    },
    {
      "id": "Efficient Computation",
      "type": "major",
      "parent": null,
      "description": "Techniques for reducing computational complexity over time steps."
    },
    {
      "id": "Belief States Update",
      "type": "subnode",
      "parent": "Kalman Filter",
      "description": "Updates belief states based on predict and update steps."
    },
    {
      "id": "Kalman Gain",
      "type": "subnode",
      "parent": "Update Step",
      "description": "Matrix used to update the state estimate based on new observations."
    },
    {
      "id": "Policy Gradient Theorem",
      "type": "major",
      "parent": null,
      "description": "The theorem that connects the gradient of expected reward to policy parameters."
    },
    {
      "id": "Log Probability Derivative",
      "type": "subnode",
      "parent": "Policy Gradient Theorem",
      "description": "Derivation of the gradient of log probability with respect to parameters."
    },
    {
      "id": "Vanilla REINFORCE Algorithm",
      "type": "subnode",
      "parent": "Policy Gradient Theorem",
      "description": "Basic policy gradient method using estimated gradients from sample trajectories."
    },
    {
      "id": "Trajectory Probability Change",
      "type": "subnode",
      "parent": "Log Probability Derivative",
      "description": "Direction of change in parameters to increase trajectory likelihood."
    },
    {
      "id": "Total Payoff Function",
      "type": "subnode",
      "parent": "Policy Gradient Theorem",
      "description": "Function representing the cumulative reward over a trajectory."
    },
    {
      "id": "Expectation Estimation",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Estimating expectations using samples from a distribution."
    },
    {
      "id": "Sample-Based Gradient Estimation",
      "type": "subnode",
      "parent": "Policy Gradient Theorem",
      "description": "Using empirical samples to estimate the gradient of expected rewards."
    },
    {
      "id": "Log Probability Calculation",
      "type": "subnode",
      "parent": "Expectation Estimation",
      "description": "Calculating log probabilities for trajectories in a policy distribution."
    },
    {
      "id": "Policy Gradient Methods",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for optimizing policies in reinforcement learning environments."
    },
    {
      "id": "Expected Reward Function",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Function representing the expected cumulative reward under a policy."
    },
    {
      "id": "Reward Function Estimation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Challenges in estimating gradients of the reward function without knowing its form."
    },
    {
      "id": "Reparametrization Techniques",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Methods used to compute gradients through stochastic variables."
    },
    {
      "id": "REINFORCE Algorithm",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Algorithm for estimating the gradient of the expected reward function in reinforcement learning."
    },
    {
      "id": "Policy Gradients",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Techniques for optimizing policies in reinforcement learning."
    },
    {
      "id": "Trajectory Probability",
      "type": "subnode",
      "parent": "Policy Gradients",
      "description": "Probability of a sequence of states and actions under a policy."
    },
    {
      "id": "Expectation Equations",
      "type": "subnode",
      "parent": "Policy Gradients",
      "description": "Mathematical expectations used to derive gradients in reinforcement learning algorithms."
    },
    {
      "id": "Simplification of Formula (17.8)",
      "type": "subnode",
      "parent": "Expectation Equations",
      "description": "Derivation and simplification process for formula (17.9) based on constant reward function."
    },
    {
      "id": "Law of Total Expectation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Mathematical principle used to simplify expectations over random variables."
    },
    {
      "id": "Estimator Simplification",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Simplifying the estimator based on certain conditions and laws of expectation."
    },
    {
      "id": "Value Functions in RL",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Discussion on value functions and their role in reinforcement learning equations."
    },
    {
      "id": "Baseline Estimation",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Use of a baseline to reduce variance in policy gradient estimators."
    },
    {
      "id": "Trajectory Collection",
      "type": "subnode",
      "parent": "Policy Gradient Methods",
      "description": "Process of collecting data by executing policies in an environment."
    },
    {
      "id": "Gradient Estimator Update",
      "type": "subnode",
      "parent": "Baseline Estimation",
      "description": "Update rule for policy parameters using gradient estimates with a baseline."
    },
    {
      "id": "Statistical Mechanics of Learning",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Application of statistical mechanics principles to understand learning processes."
    },
    {
      "id": "Generalization Theory",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Theoretical framework for understanding how models generalize from training data to unseen data."
    },
    {
      "id": "Machine Learning Literature",
      "type": "major",
      "parent": null,
      "description": "Collection of key papers and articles in machine learning."
    },
    {
      "id": "Bias-Variance Trade-off",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Discussion on reconciling modern ML practice with classical bias-variance trade-off."
    },
    {
      "id": "BERT Model",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Introduction of BERT, a deep bidirectional transformer model for language understanding."
    },
    {
      "id": "Implicit Bias",
      "type": "subnode",
      "parent": "Machine Learning Literature",
      "description": "Study on the implicit bias introduced by noise covariance in machine learning models."
    },
    {
      "id": "Machine Learning Papers",
      "type": "major",
      "parent": null,
      "description": "Collection of research papers in machine learning and statistical learning theory."
    },
    {
      "id": "Implicit Bias Studies",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Studies focusing on the implicit bias in high-dimensional models."
    },
    {
      "id": "Deep Residual Learning",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on deep residual learning for image recognition."
    },
    {
      "id": "Variational Bayes",
      "type": "subnode",
      "parent": "Machine Learning Papers",
      "description": "Research on auto-encoding variational bayes methods."
    }
  ],
  "edges": [
    {
      "from": "Likelihood Function",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Average Model (h_avg)",
      "relationship": "has_subcomponent"
    },
    {
      "from": "Variational Inference",
      "to": "Mean Field Assumption",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "Classification Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Training Examples Representation",
      "to": "First-Layer Activations",
      "relationship": "subtopic_of"
    },
    {
      "from": "Logistic Regression",
      "to": "Parameter Fitting",
      "relationship": "involves"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "Convergence Guarantees",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Few-Shot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Linear regression",
      "to": "LMS algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Lagrange Duality",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Parameter c in Kernel Function",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Jensen's inequality",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Negative Log-Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "Bellman Update",
      "to": "Theorem on Bellman Operator",
      "relationship": "related_to"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Adaptation Phase",
      "to": "Foundation Models",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Applications of PCA",
      "to": "Clustering",
      "relationship": "related_to"
    },
    {
      "from": "Regularization Techniques",
      "to": "Minimum Norm Solution",
      "relationship": "depends_on"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Data Visualization",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Continuous State MDPs",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "related_to"
    },
    {
      "from": "RedundancyDetection",
      "to": "DataPreprocessingAndAnalysis",
      "relationship": "depends_on"
    },
    {
      "from": "Sigmoid Function",
      "to": "Derivative of Sigmoid Function",
      "relationship": "has_property"
    },
    {
      "from": "Logistic Regression",
      "to": "Logistic Loss Function",
      "relationship": "defines"
    },
    {
      "from": "EM Algorithm",
      "to": "E-step",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Value Function",
      "to": "Optimal Policy Derivation",
      "relationship": "subtopic"
    },
    {
      "from": "Test Error Decomposition",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Finding w and b from Alpha",
      "to": "Dual Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning",
      "to": "Convolutional Layers",
      "relationship": "related_to"
    },
    {
      "from": "Support vector machines",
      "to": "Lagrange duality (optional reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Binary Classification Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Gradients",
      "relationship": "has_subtopic"
    },
    {
      "from": "Naive Bayes Classifier",
      "to": "Bernoulli Event Model",
      "relationship": "related_to"
    },
    {
      "from": "SIMCLR",
      "to": "Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "LQR Algorithm Steps",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Functional Margin",
      "to": "Function Margin of Training Set",
      "relationship": "subtopic"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear regression",
      "to": "Probabilistic interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Principal components analysis",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Trade-off",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "Multinomial Random Variable",
      "to": "Maximum Likelihood Estimates",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Finetuning",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Differentiable Circuit",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization",
      "to": "Bias-variance tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Probability Estimation",
      "to": "Laplace Smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Inverted Pendulum Example",
      "to": "State Transition Equation",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Overview",
      "to": "General Backprop Strategy",
      "relationship": "subtopic"
    },
    {
      "from": "Deep learning",
      "to": "Modules in modern neural networks",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "E-step Calculation",
      "relationship": "has_subtopic"
    },
    {
      "from": "LQR Algorithm",
      "to": "Step 3",
      "relationship": "related_to"
    },
    {
      "from": "Posterior Distribution",
      "to": "Bayesian Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Conv1D-S Module",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Model Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep learning",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Regularizer R(\u03b8)",
      "to": "Training Loss/Cost Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Cross Validation",
      "relationship": "depends_on"
    },
    {
      "from": "Major Axis of Variation",
      "to": "Projection onto Direction u",
      "relationship": "subtopic"
    },
    {
      "from": "Non-Stationary Optimal Policy",
      "to": "Policy Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "State Sampling",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Gradient Methods",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Descent Update Rule",
      "to": "LMS with Features",
      "relationship": "subtopic"
    },
    {
      "from": "Training vs Test Distributions",
      "to": "Domain Shift",
      "relationship": "related_to"
    },
    {
      "from": "Total Payoff Function",
      "to": "Policy Gradient Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Language Models",
      "to": "Temperature Parameter",
      "relationship": "has_subtopic"
    },
    {
      "from": "Laplace Smoothing",
      "to": "Spam Classification Example",
      "relationship": "depends_on"
    },
    {
      "from": "Binary Classification Problem",
      "to": "Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Mixture Model",
      "to": "E-step",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Baseline Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Sample Complexity",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Decision Boundaries",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Properties of Kernels",
      "relationship": "related_to"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Empirical Risk Minimization (ERM)",
      "relationship": "related_to"
    },
    {
      "from": "Regression Problems",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Text_Classification",
      "to": "Spam_Filtering",
      "relationship": "subtopic_of"
    },
    {
      "from": "Finite Set of Models",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Supervised Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient (REINFORCE)",
      "to": "Randomized Policy",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Logistic Regression Derivation",
      "relationship": "subtopic"
    },
    {
      "from": "Labeled Data",
      "to": "Adaptation Phase",
      "relationship": "related_to"
    },
    {
      "from": "Generative Learning",
      "to": "Bayes Rule",
      "relationship": "related_to"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Implicit Regularization",
      "relationship": "related_to"
    },
    {
      "from": "Validation Set",
      "to": "k-fold Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Word Generation Process",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Layers",
      "to": "Efficiency",
      "relationship": "subtopic"
    },
    {
      "from": "Baseline Estimation",
      "to": "Gradient Estimator Update",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Regression Problems",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cross-Entropy Loss",
      "to": "Gradient Computation",
      "relationship": "related_to"
    },
    {
      "from": "Likelihood Estimation",
      "to": "Closed Form Solution",
      "relationship": "related_to"
    },
    {
      "from": "Dynamic Programming",
      "to": "Bellman's Equation",
      "relationship": "depends_on"
    },
    {
      "from": "Density Transformation",
      "to": "1D Example",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Back-propagation for MLPs",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Expectation-Maximization Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Markov Decision Processes (MDPs)",
      "relationship": "contains"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Machine Learning Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Model Parameters",
      "to": "Likelihood Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "NeurIPS Conference",
      "relationship": "related_to"
    },
    {
      "from": "Reward Function",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "REINFORCE Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-class Classification",
      "to": "Softmax Function",
      "relationship": "uses"
    },
    {
      "from": "1D Convolution",
      "to": "Channels",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Reinforcement Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Optimal Margin Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Single Neuron Network",
      "to": "Housing Price Prediction",
      "relationship": "example_of"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Deep Learning Regularizers",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Covariance Matrix",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "k-fold Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Mixture of Gaussians revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Training Dataset Example",
      "relationship": "depends_on"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "Scale-Invariant Property",
      "relationship": "depends_on"
    },
    {
      "from": "Probability Estimation",
      "to": "Multinomial Random Variable",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimization Challenges",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-class Classification",
      "to": "Multinomial Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries on partial derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Discrete Ricatti Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian discriminant analysis",
      "to": "Discussion: GDA and logistic regression",
      "relationship": "subtopic"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Finite Horizon MDPs",
      "to": "Time Horizon Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Cross-Entropy Loss",
      "to": "Softmax Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Transformers",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Exponential Family Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Activation Functions",
      "to": "ReLU Function",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining Methods in CV",
      "to": "Contrastive Learning",
      "relationship": "contains"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Optimal Policy Independence from Noise",
      "relationship": "subtopic"
    },
    {
      "from": "Finetuning Pretrained Models",
      "to": "Prediction Model Structure",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Double Descent Phenomenon",
      "relationship": "related_to"
    },
    {
      "from": "Function Representation",
      "to": "Linear Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "E-step",
      "relationship": "has_part"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Claim 8.1.1",
      "relationship": "uses"
    },
    {
      "from": "Machine Learning Models",
      "to": "Conditional Probability Modeling",
      "relationship": "contains"
    },
    {
      "from": "Kernel Examples",
      "to": "String Classification Example",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization (EM) Algorithm",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Parameter Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Overview",
      "to": "Chain Rule Perspective",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "In-context Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised Learning",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Derived Features",
      "to": "Family Size",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining methods in computer vision",
      "relationship": "subtopic"
    },
    {
      "from": "Zero-shot Adaptation",
      "to": "Language Model Utilization",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM Algorithm Overview",
      "to": "Single Example Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Generalized Linear Model (GLM)",
      "relationship": "contains"
    },
    {
      "from": "Expectation-Maximization Algorithm",
      "to": "M-step Maximization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Data Preprocessing",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Linear Regression Models",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimization Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Feature Maps",
      "to": "Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "PCA",
      "to": "Dimensionality Reduction",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Chain Rule in Machine Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Quadratic Regulator (LQR)",
      "relationship": "related_to"
    },
    {
      "from": "EM Algorithm",
      "to": "Multiple Examples Extension",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Chain Rule Application",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Rate Alpha",
      "to": "Stochastic Gradient Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Geometric Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Gradient Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Learning Rate (\u03b1)",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Quadratic Regulation (LQR)",
      "to": "Continuous Setting Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Constrained Optimization",
      "to": "Generalized Lagrangian",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "Convex Functions",
      "relationship": "related_to"
    },
    {
      "from": "Data Normalization",
      "to": "Variance Scaling",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "LQG",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problems",
      "to": "Mean-Square Cost Function",
      "relationship": "related_to"
    },
    {
      "from": "VE Procedure",
      "to": "k Parameter",
      "relationship": "related_to"
    },
    {
      "from": "Log Probability Calculation",
      "to": "Gradient Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Other Normalization Layers",
      "to": "Batch Normalization",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Fisher Scoring",
      "relationship": "related_to"
    },
    {
      "from": "Uniform Convergence",
      "to": "Generalization Error",
      "relationship": "related_to"
    },
    {
      "from": "Maximum Likelihood Estimates",
      "to": "Laplace Smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Variance Maximization",
      "to": "Lagrange Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Training Error",
      "to": "Empirical Risk Minimization (ERM)",
      "relationship": "related_to"
    },
    {
      "from": "Neural Network Parameters",
      "to": "Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Observation Layer",
      "relationship": "subtopic"
    },
    {
      "from": "Least Squares Regression",
      "to": "Cost Function J(\u03b8)",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradients",
      "to": "Expectation Equations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Hypothesis Class",
      "relationship": "related_to"
    },
    {
      "from": "Optimizers",
      "to": "Pretraining Phase",
      "relationship": "depends_on"
    },
    {
      "from": "Expected Future Rewards",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "LQR Extension",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Algorithm 6",
      "relationship": "depends_on"
    },
    {
      "from": "K-means Clustering",
      "to": "EM Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Expectation Equations",
      "to": "Simplification of Formula (17.8)",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Robustness of Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Mean Vector",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised learning",
      "to": "Support vector machines",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Gradient Ascent Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Chapter 9 Regularization and Model Selection",
      "to": "Regularization",
      "relationship": "contains"
    },
    {
      "from": "Multiple Examples Extension",
      "to": "Multiple Examples ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimal Parameters Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "SMO Algorithm",
      "to": "Dual Formulation of Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Vapnik's Theorem",
      "to": "Uniform Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Initialization",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Total Parameters in Conv2D",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm",
      "to": "Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "5th Degree Polynomial Models",
      "relationship": "contains"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "ELBO Optimization",
      "relationship": "leads_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimization in RL",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "contains"
    },
    {
      "from": "Deep learning",
      "to": "Supervised learning with non-linear models",
      "relationship": "subtopic"
    },
    {
      "from": "Implementation Subtleties",
      "to": "Data Matrix Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "related_to"
    },
    {
      "from": "Adaptation Algorithm",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Layer Normalization",
      "to": "Scaling-Invariant Property",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning",
      "to": "Policy Gradient Methods",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Optimization Techniques",
      "to": "Coordinate Ascent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Sample-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Concave Functions",
      "to": "Jensen's Inequality",
      "relationship": "related_to"
    },
    {
      "from": "Generalization Error Bound",
      "to": "Uniform Convergence",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "More data can hurt for linear regression: Sample-wise double descent",
      "relationship": "subtopic"
    },
    {
      "from": "Sample-Based Gradient Estimation",
      "to": "Policy Gradient Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Value Function Approximation",
      "relationship": "contains"
    },
    {
      "from": "Kernel Methods",
      "to": "Machine Learning Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Probabilistic Interpretation",
      "to": "Linear Regression",
      "relationship": "related_to"
    },
    {
      "from": "gamma (Margin)",
      "to": "Uniform Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Mean Vector Influence",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Convergence Issues",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Multiplication as a Building Block",
      "to": "Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Calculation",
      "to": "Bandwidth Parameter (\u03c4)",
      "relationship": "related_to"
    },
    {
      "from": "Conditional Distribution Assumptions",
      "to": "Natural Parameter and Inputs Relationship",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Models",
      "to": "Regression Problems",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization (EM) Algorithm",
      "to": "E-step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement Learning Overview",
      "to": "Markov Decision Process (MDP)",
      "relationship": "depends_on"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Unmixing Matrix W",
      "relationship": "related_to"
    },
    {
      "from": "Generative Models",
      "to": "Naive Bayes Assumption",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Methods",
      "to": "Feature Maps",
      "relationship": "subtopic"
    },
    {
      "from": "Likelihood Estimation",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-class Classification",
      "to": "Logits in Multi-class",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Feature Mapping",
      "relationship": "has_subtopic"
    },
    {
      "from": "Layer Normalization",
      "to": "Learnable Parameters (beta, gamma)",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Model Limitations",
      "to": "Bias Definition",
      "relationship": "defines"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Optimal Margin Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Clustering",
      "relationship": "subtopic"
    },
    {
      "from": "Implicit Bias Studies",
      "to": "Shape Matters: Understanding the implicit bias of the noise covariance",
      "relationship": "related_to"
    },
    {
      "from": "Conditional Distribution Modeling",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Soft Assignments",
      "to": "EM Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "SMO Algorithm",
      "to": "Efficient Update",
      "relationship": "subtopic"
    },
    {
      "from": "Matricization Approach",
      "to": "Implementation Subtleties",
      "relationship": "depends_on"
    },
    {
      "from": "Underfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "L2 Norm Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Evidence Lower Bound (ELBO)",
      "to": "Gradient Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Distance to Decision Boundary",
      "to": "Decision Boundary",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problems",
      "to": "Test Example",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Hypothesis Classes",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Techniques",
      "to": "Quadratic Functions",
      "relationship": "related_to"
    },
    {
      "from": "Generalized linear models",
      "to": "Constructing GLMs",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bernoulli Distribution",
      "to": "Logistic Function",
      "relationship": "defines"
    },
    {
      "from": "Evidence Lower Bound (ELBO)",
      "to": "Jensen's Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Descent",
      "to": "Optimization Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Assumptions/Design Choices",
      "relationship": "subtopic"
    },
    {
      "from": "Support vector machines",
      "to": "Dual form of optimal margin classifiers",
      "relationship": "subtopic"
    },
    {
      "from": "SMO algorithm (optional reading)",
      "to": "Coordinate ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Selection",
      "relationship": "related_to"
    },
    {
      "from": "Regression Problems",
      "to": "Expected Test Error",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problem",
      "to": "Parameter C",
      "relationship": "related_to"
    },
    {
      "from": "Policy Iteration",
      "to": "Policy Evaluation",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Markov decision processes",
      "relationship": "subtopic"
    },
    {
      "from": "Training Methods",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Hypothesis Representation",
      "relationship": "defines"
    },
    {
      "from": "General Case",
      "to": "Volume Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Class Priors",
      "to": "Bayesian Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Newton's Method",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Vectorization of Operations",
      "to": "Broadcasting in Matrix Operations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "True Function (h_star)",
      "relationship": "related_to"
    },
    {
      "from": "Multi-layer Neural Networks",
      "to": "Weight Matrices and Biases",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman's Equation",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Binary Features",
      "relationship": "subtopic"
    },
    {
      "from": "Contrastive Learning",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "5th Degree Polynomial Models",
      "to": "Generalization Failure",
      "relationship": "results_in"
    },
    {
      "from": "Parameter Estimation",
      "to": "Naive Bayes Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Coordinate Ascent Algorithm",
      "to": "Unconstrained Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic Regression",
      "to": "Sigmoid Function",
      "relationship": "uses"
    },
    {
      "from": "Cocktail Party Problem",
      "to": "Mixing Matrix A",
      "relationship": "depends_on"
    },
    {
      "from": "Value of Primal Problem",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Spam Filtering",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradients",
      "to": "Reward Function",
      "relationship": "depends_on"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Statistical Mechanics of Learning",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Generalized Linear Models (GLMs)",
      "relationship": "contains"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "States",
      "relationship": "component_of"
    },
    {
      "from": "General EM algorithms",
      "to": "Other interpretation of ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Logistic Regression",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backpropagation Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression (LWR)",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining Phase",
      "to": "Foundation Models",
      "relationship": "subtopic"
    },
    {
      "from": "GELU Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Sample-wise Double Descent",
      "to": "Optimal Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "Variational Inference",
      "relationship": "extends"
    },
    {
      "from": "Dual Formulation of SVM",
      "to": "Lagrange Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "ResNet Architecture",
      "to": "Batch Normalization Variants",
      "relationship": "subtopic"
    },
    {
      "from": "Hoeffding Inequality",
      "to": "Generalization Error Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Matrix Operations",
      "relationship": "subtopic"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Continuous state MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generative learning algorithms",
      "to": "Naive bayes (Option Reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Support Vector Machines (SVM)",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity",
      "to": "Floating Point Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Machine Learning Theory",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Frequentist View",
      "relationship": "related_to"
    },
    {
      "from": "Linear Regression",
      "to": "Housing Example",
      "relationship": "example_of"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Backpropagation",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Bayesian Classification",
      "relationship": "related_to"
    },
    {
      "from": "SVMs (Support Vector Machines)",
      "to": "SMO Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Negative Log-Likelihood",
      "relationship": "depends_on"
    },
    {
      "from": "Generative Learning",
      "to": "Class Priors",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "SMO Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Pretrained large language models",
      "to": "Open up the blackbox of Transformers",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Deterministic Simulators",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "ResNet (Residual Network)",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Supervised Learning Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "n (Sample Size)",
      "to": "Uniform Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Loss Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Layer Normalization",
      "to": "LN-S(z)",
      "relationship": "depends_on"
    },
    {
      "from": "Conditional Probability",
      "to": "Bayesian Classification",
      "relationship": "depends_on"
    },
    {
      "from": "Linear regression",
      "to": "Locally weighted linear regression",
      "relationship": "subtopic"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Predict Step",
      "to": "Gaussian Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Gradient Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Probability Calculation",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Kernel Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Optimal Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized linear models",
      "to": "Exponential family",
      "relationship": "subtopic"
    },
    {
      "from": "Classification and logistic regression",
      "to": "Perceptron learning algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "DataPreprocessingAndAnalysis",
      "to": "DataNormalization",
      "relationship": "subtopic"
    },
    {
      "from": "Initialization",
      "to": "k-means Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous Setting Assumptions",
      "to": "Linear Transitions",
      "relationship": "related_to"
    },
    {
      "from": "Infinite Model Classes",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Fully-Connected Network",
      "to": "Vectorization in Neural Networks",
      "relationship": "example_of"
    },
    {
      "from": "Machine Learning Adaptation Methods",
      "to": "Finetuning Pretrained Models",
      "relationship": "contains"
    },
    {
      "from": "Baseline Estimation",
      "to": "Value Function",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Mean",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation Approximation",
      "to": "Deterministic Simulators",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Gradient Ascent",
      "relationship": "depends_on"
    },
    {
      "from": "Vectorization",
      "to": "Training Examples",
      "relationship": "subtopic_of"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Newton's Method in ML",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Support vector machines",
      "to": "Regularization and non-separable case",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Mixing Matrix A",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Expectation Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation Estimation",
      "to": "Log Probability Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Optimization Techniques",
      "to": "SVMs (Support Vector Machines)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Hypothesis Classes",
      "to": "Infinite Hypothesis Classes",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Deep Learning Models",
      "relationship": "related_to"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Convex Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Data Normalization",
      "to": "Mean Subtraction",
      "relationship": "depends_on"
    },
    {
      "from": "Deep learning",
      "to": "Neural networks",
      "relationship": "subtopic"
    },
    {
      "from": "Sparsity Inducing Regularization",
      "to": "Regularizer",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Selection",
      "relationship": "subtopic"
    },
    {
      "from": "In-context Learning",
      "to": "Prompt Construction",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Non-linear Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov Decision Processes (MDPs)",
      "to": "Value Iteration",
      "relationship": "contains"
    },
    {
      "from": "Parameter Updates",
      "to": "\u03b8 Update Rule",
      "relationship": "has_subtopic"
    },
    {
      "from": "Implicit Regularization Effect",
      "to": "Regularization in Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Functional Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Risk Minimization",
      "to": "Training Set S",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Multi-class Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Gaussian Mixture Model (GMM)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Geometric Margins",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Properties of Kernels",
      "to": "Intrinsic Nature of Kernel Function",
      "relationship": "related_to"
    },
    {
      "from": "Evidence Lower Bound (ELBO)",
      "to": "Variational Inference",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDPs)",
      "to": "Learning Model Parameters",
      "relationship": "contains"
    },
    {
      "from": "Least Squares Revisited",
      "to": "Design Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Problems",
      "to": "KKT Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Parametric Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Negative Likelihood Loss",
      "relationship": "related_to"
    },
    {
      "from": "Natural Parameter (\u03b7)",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Data Scarcity",
      "to": "k-fold Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Optimal Policy from Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Geometric Margin",
      "to": "Decision Boundary",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Generalized Linear Models (GLMs)",
      "relationship": "depends_on"
    },
    {
      "from": "Density Transformation",
      "to": "General Case",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Implicit regularization effect (optional reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Partial Derivatives",
      "to": "Multi-Variable Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Reward Function",
      "relationship": "component_of"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Applications of PCA",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Factor",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Synchronous Update",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Margin Classifier",
      "to": "Functional Margin",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Distributions",
      "to": "Variational Inference",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Non-Convex Constraint Issue",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Linear Model Limitations",
      "relationship": "contains"
    },
    {
      "from": "Gradient Descent Methods",
      "to": "Stochastic Gradient Descent",
      "relationship": "contains"
    },
    {
      "from": "Discretization Methods",
      "to": "Curse of Dimensionality",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Policy in MDPs",
      "to": "Non-Stationary Optimal Policy",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Calculation",
      "to": "Parameter Gradients",
      "relationship": "subtopic_of"
    },
    {
      "from": "Kernel Examples",
      "to": "Digit Recognition Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Loss Function Composition",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Positive Semi-Definiteness",
      "relationship": "depends_on"
    },
    {
      "from": "Constrained Optimization",
      "to": "Lagrange Duality Theory",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy in MDPs",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Chapter 9 Regularization and Model Selection",
      "relationship": "contains"
    },
    {
      "from": "Log-Likelihood Maximization",
      "to": "Jensen's Inequality",
      "relationship": "uses"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Variational Inference",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Generalization Error",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic Regression",
      "to": "Total Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "MLP Model",
      "to": "Modules in MLP",
      "relationship": "has_subtopic"
    },
    {
      "from": "Tight Bound Conditions",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Probability Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Poisson Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Least-Squares Cost Function",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Bellman's Equation",
      "to": "Optimal Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linearization of dynamics",
      "relationship": "related_to"
    },
    {
      "from": "Claim 8.1.1",
      "to": "Independence of Random Variables",
      "relationship": "depends_on"
    },
    {
      "from": "Sparsity Regularization",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Binary Classification",
      "to": "Logistic Function",
      "relationship": "subtopic"
    },
    {
      "from": "Cost Function",
      "to": "Normal Equations",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradients",
      "to": "Trajectory Probability",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problems",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Kernels as Similarity Metrics",
      "to": "Gaussian Kernel",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Mapping",
      "to": "Kernels as Similarity Metrics",
      "relationship": "related_to"
    },
    {
      "from": "Parameterized Model",
      "to": "Embeddings",
      "relationship": "depends_on"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Model Selection",
      "relationship": "related_to"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Efficient Computation",
      "to": "Kalman Filter",
      "relationship": "related_to"
    },
    {
      "from": "Finite-horizon MDPs",
      "to": "Optimal Bellman Equation",
      "relationship": "depends_on"
    },
    {
      "from": "Functional Margins",
      "to": "Geometric Margins",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Linear Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Algorithm Steps for LQR",
      "to": "Efficiency Improvements in LQR",
      "relationship": "related_to"
    },
    {
      "from": "MLP (Multilayer Perceptron)",
      "to": "Nonlinear Activation Module",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Model Assumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Chain Rule in Machine Learning",
      "to": "Gradient Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Backward Function in Machine Learning",
      "to": "Jacobian Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVMs)",
      "relationship": "depends_on"
    },
    {
      "from": "Sufficient Conditions for Kernels",
      "to": "Feature Mapping",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Action Selection",
      "relationship": "subtopic"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Conditional Probabilistic Model",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement Learning and Control",
      "to": "Reinforcement learning",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization (EM) Algorithm",
      "to": "M-step Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Calculation",
      "to": "Reparameterization Trick",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Trick",
      "to": "Kernel Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Loss Functions",
      "to": "Logistic Loss Function",
      "relationship": "subtopic"
    },
    {
      "from": "Bernoulli Distribution",
      "to": "Exponential Family Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Necessary Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Mixture Model (GMM)",
      "to": "Parameter Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "Functional Margin",
      "to": "Geometric Margin",
      "relationship": "depends_on"
    },
    {
      "from": "Posterior Distribution",
      "to": "Prediction on New Example",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Bellman Equation",
      "to": "Optimal Policy Recovery",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Assumptions and Data Efficiency",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization Error",
      "to": "Sample Complexity",
      "relationship": "depends_on"
    },
    {
      "from": "Data Augmentation",
      "to": "Positive Pair",
      "relationship": "subtopic"
    },
    {
      "from": "MLP Composition of Modules",
      "to": "Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Variance Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Multivariate Normal Distribution",
      "relationship": "contains"
    },
    {
      "from": "Expected Test Error",
      "to": "Mean Squared Error (MSE)",
      "relationship": "defines"
    },
    {
      "from": "Layer Normalization",
      "to": "Affine Transformation",
      "relationship": "subtopic"
    },
    {
      "from": "k-means Algorithm",
      "to": "Distortion Function J",
      "relationship": "depends_on"
    },
    {
      "from": "BLAS Libraries",
      "to": "Vectorization in Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Discriminative Learning",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "Feature Mapping",
      "to": "Iterative Update Process",
      "relationship": "depends_on"
    },
    {
      "from": "Adaptation Phase",
      "to": "Transfer Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function Gradient Calculation",
      "to": "Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Variational Auto-Encoder (VAE)",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Polynomial Kernel Example",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Sign Change Irrelevance",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation Approximation",
      "to": "Gaussian Noise Models",
      "relationship": "subtopic"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Pre-Computation Strategy",
      "relationship": "depends_on"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Multinomial Features",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Kernel Matrix",
      "relationship": "related_to"
    },
    {
      "from": "Conditional Probability",
      "to": "Naive Bayes Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Polynomial Regression Model",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Non-parametric Algorithms",
      "to": "Locally Weighted Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Algorithm",
      "to": "Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "PCA",
      "to": "Approximation Error Minimization",
      "relationship": "subtopic"
    },
    {
      "from": "Probability Distributions",
      "to": "Gaussian Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining and adaptation",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Function",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization Methods",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Learning Guarantees",
      "to": "Union Bound Lemma",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithm",
      "to": "Evidence Lower Bound (ELBO)",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised Learning",
      "to": "EM algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Maps and Kernels",
      "relationship": "depends_on"
    },
    {
      "from": "Jensen's_Inequality",
      "to": "Log_Probability_Bound",
      "relationship": "depends_on"
    },
    {
      "from": "Rewards Dependence",
      "to": "Optimal Action Computation",
      "relationship": "depends_on"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Spam/Non-Spam Classification",
      "to": "Multinomial Event Model",
      "relationship": "depends_on"
    },
    {
      "from": "Dual Problem",
      "to": "Primal Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Non-linear Dynamics to LQR",
      "to": "Inverted Pendulum Example",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "Weight Matrices",
      "relationship": "depends_on"
    },
    {
      "from": "Loss Function",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Housing Prices Model",
      "to": "Derived Features",
      "relationship": "subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Canonical Response Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Naive Bayes Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "SMO algorithm (optional reading)",
      "to": "SMO",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Selection",
      "relationship": "depends_on"
    },
    {
      "from": "Transformation to Convex Form",
      "to": "Scaling Constraint",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Vector Selection",
      "relationship": "depends_on"
    },
    {
      "from": "5th Degree Polynomial Models",
      "to": "Low Bias, High Variance",
      "relationship": "describes"
    },
    {
      "from": "Gradient Descent",
      "to": "J(\u03b8)",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Overview",
      "to": "Fitted Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "Single Neuron Model",
      "relationship": "contains"
    },
    {
      "from": "Optimal Parameters Calculation",
      "to": "Dual Formulation",
      "relationship": "related_to"
    },
    {
      "from": "Function Representation",
      "to": "Cost Function",
      "relationship": "defines"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Non-Gaussian Data Recovery",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression Derivation",
      "to": "Perceptron Learning Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Parameter Matrix W",
      "to": "Likelihood Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Backward functions for basic modules",
      "relationship": "subtopic"
    },
    {
      "from": "Model Parameters",
      "to": "Log-Likelihood Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Overfitting and Underfitting",
      "relationship": "subtopic"
    },
    {
      "from": "Necessary Conditions for Valid Kernels",
      "to": "Symmetry Property",
      "relationship": "depends_on"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Permutation Matrix",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Comparison of Value Iteration and Policy Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Least Squares Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Inner Product",
      "to": "Dual Form of Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Support vector machines",
      "to": "Margins: intuition",
      "relationship": "subtopic"
    },
    {
      "from": "Sample Complexity",
      "to": "Hypothesis Class Size",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization Error",
      "to": "PAC Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Back-propagation for MLPs",
      "to": "Gradient Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous-State MDPs",
      "to": "Value Function Approximation",
      "relationship": "related_to"
    },
    {
      "from": "Inner Product Computation",
      "to": "Efficient Inner Product Calculation",
      "relationship": "related_to"
    },
    {
      "from": "Two-layer Fully-Connected Neural Network",
      "to": "Prior Knowledge",
      "relationship": "depends_on"
    },
    {
      "from": "Neural Networks",
      "to": "Biological Inspiration",
      "relationship": "related_to"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Unmixing Matrix W",
      "relationship": "subtopic"
    },
    {
      "from": "Immediate Reward",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Hyperparameters",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Notation in Machine Learning",
      "to": "Generalizing to Multiple Layers",
      "relationship": "has_subtopic"
    },
    {
      "from": "VC Dimension",
      "to": "Vapnik's Theorem",
      "relationship": "related_to"
    },
    {
      "from": "Model Parameters",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Global Minima",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Belief States Update",
      "to": "Update Step",
      "relationship": "follows"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Gradient Descent",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithm",
      "to": "Log-Likelihood Optimization",
      "relationship": "subtopic"
    },
    {
      "from": "RedundancyDetection",
      "to": "LinearDependency",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization (EM) Algorithm",
      "to": "Reparameterization Trick",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Linear Regression",
      "relationship": "related_to"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Compression",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Transformer Model",
      "relationship": "contains"
    },
    {
      "from": "Probability Estimation",
      "to": "Naive Bayes Classifier",
      "relationship": "related_to"
    },
    {
      "from": "Trajectory Probability Change",
      "to": "Log Probability Derivative",
      "relationship": "related_to"
    },
    {
      "from": "Non-Separable Case",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis Space Parameterization",
      "to": "Linear Classifiers",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Gaussian Distribution",
      "relationship": "depends_on"
    },
    {
      "from": "Model Creation Methods",
      "to": "Physics Simulation",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Asymptotic Efficiency",
      "relationship": "depends_on"
    },
    {
      "from": "RedundancyDetection",
      "to": "SurveyExample",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Convolutional Neural Networks (CNN)",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning",
      "to": "Gaussian Mixture Models",
      "relationship": "related_to"
    },
    {
      "from": "M-step Maximization",
      "to": "Parameter Updates",
      "relationship": "has_subtopic"
    },
    {
      "from": "Parameter Estimation",
      "to": "Expectation-Maximization (EM) Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problems",
      "to": "Training Dataset",
      "relationship": "depends_on"
    },
    {
      "from": "Quadratic Assumption",
      "to": "Dynamics Model",
      "relationship": "depends_on"
    },
    {
      "from": "Variance Scaling",
      "to": "Comparability Across Attributes",
      "relationship": "related_to"
    },
    {
      "from": "Supervised learning",
      "to": "Kernel methods",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "State Transition",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Predict Step",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Risk Minimization",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Ordinary Least Squares (OLS)",
      "to": "Response Variable",
      "relationship": "defines"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Optimization Problems",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Policy",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Sample Complexity",
      "relationship": "subtopic"
    },
    {
      "from": "Training Set",
      "to": "Multinomial Event Model",
      "relationship": "subtopic"
    },
    {
      "from": "Stochastic Gradient Ascent",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Single Neuron Network",
      "to": "ReLU Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "k-means Algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Bayesian Inference",
      "to": "Naive Bayes Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Finding Roots",
      "relationship": "depends_on"
    },
    {
      "from": "Step 1: Estimate Matrices",
      "to": "Value Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Dynamic Programming",
      "relationship": "related_to"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Posterior Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Learning Model Parameters",
      "to": "Maximum Likelihood Estimation",
      "relationship": "uses"
    },
    {
      "from": "Naive bayes (Option Reading)",
      "to": "Event models for text classification",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Variational inference and variational auto-encoder (optional reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Efficiency Considerations",
      "to": "Vectorization in Neural Networks",
      "relationship": "depends_on"
    },
    {
      "from": "Labeled Dataset",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "depends_on"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Latent Variables",
      "relationship": "depends_on"
    },
    {
      "from": "Training Set",
      "to": "Hypothesis Function",
      "relationship": "depends_on"
    },
    {
      "from": "Backward Function for Loss Functions",
      "to": "Squared Loss (MSE)",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Model (GLM)",
      "to": "Conditional Distribution Assumptions",
      "relationship": "contains"
    },
    {
      "from": "Model Complexity",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "VC Dimension",
      "to": "Shattering Sets",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Matricization Approach",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised learning",
      "to": "Linear regression",
      "relationship": "subtopic"
    },
    {
      "from": "Non-linear Model h_\u03b8(x)",
      "to": "Training Examples",
      "relationship": "depends_on"
    },
    {
      "from": "Log Partition Function for Bernoulli (a(\u03b7))",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Union Bound",
      "to": "Uniform Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Likelihood Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Mapping",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting",
      "to": "Bias-Variance Tradeoff",
      "relationship": "depends_on"
    },
    {
      "from": "Training Set Sampling",
      "to": "Generalization Error Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "LMS Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Batch Normalization Variants",
      "to": "Layer Normalization (LN)",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "contains"
    },
    {
      "from": "Regularization",
      "to": "Bias-Variance Tradeoff",
      "relationship": "related_to"
    },
    {
      "from": "Posterior Approximation",
      "to": "MAP Estimate",
      "relationship": "subtopic"
    },
    {
      "from": "Single Neuron Model",
      "to": "Weight Vector",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement learning",
      "to": "Continuous state MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Density Function",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "M-step",
      "relationship": "depends_on"
    },
    {
      "from": "Classification",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Least Squares Revisited",
      "to": "Vector y",
      "relationship": "depends_on"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Pretraining Phase",
      "relationship": "subtopic"
    },
    {
      "from": "Functional Margin",
      "to": "Normalization Condition",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning",
      "to": "Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Foundation Models",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Model Evaluation",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Model selection via cross validation",
      "relationship": "subtopic"
    },
    {
      "from": "Implicit Bias",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "2-D Convolution (Conv2D-S)",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Forward Pass",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Gradient Descent Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Probability Prediction",
      "relationship": "subtopic"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "Constant Term Absorption",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Non-linear Model h_\u03b8(x)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backward Function for Activations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Latent Variable Models",
      "to": "EM Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Expected Reward Function",
      "to": "Reward Function Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Inference",
      "to": "Posterior Approximation",
      "relationship": "depends_on"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Non-Gaussian Sources",
      "relationship": "subtopic"
    },
    {
      "from": "Discretization Methods",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Convex Functions",
      "to": "Jensen's Inequality",
      "relationship": "depends_on"
    },
    {
      "from": "EM Algorithms",
      "to": "EM for Mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "ReLU Function",
      "to": "Leaky ReLU",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Techniques",
      "to": "Box Constraints",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Bayes",
      "to": "Auto-encoding variational bayes",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Empirical Covariance Matrix",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Decay",
      "to": "L2 Regularization",
      "relationship": "related_to"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Projection of Data Points",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Logistic Regression",
      "relationship": "contains"
    },
    {
      "from": "Likelihood Function",
      "to": "Log-Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "Linear regression",
      "to": "The normal equations",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "subtopic"
    },
    {
      "from": "Normal Equations Method",
      "to": "Matrix Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Iteration",
      "to": "Procedure VE",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "LQR Model Assumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Model Representation",
      "to": "Non-linear Feature Mappings",
      "relationship": "subtopic"
    },
    {
      "from": "Regression Problem",
      "to": "Probabilistic Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "ReLU Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Gaussian Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Cost Function",
      "to": "Ordinary Least Squares",
      "relationship": "special_case_of"
    },
    {
      "from": "Covariance Matrix",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Total Payoff Calculation",
      "to": "Discount Factor (\u03b3)",
      "relationship": "related_to"
    },
    {
      "from": "Supervised learning",
      "to": "Generative learning algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Sample Complexity",
      "to": "Linear Dependence",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "related_to"
    },
    {
      "from": "MLP (Multilayer Perceptron)",
      "to": "Matrix Multiplication Module",
      "relationship": "depends_on"
    },
    {
      "from": "Hold-Out Cross Validation",
      "to": "Validation Set S_cv",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Maps and Representation Transferability",
      "to": "Deep Learning Representations",
      "relationship": "subtopic"
    },
    {
      "from": "Polynomial Fitting",
      "to": "Variance in Model",
      "relationship": "related_to"
    },
    {
      "from": "Generalization Theory",
      "to": "Statistical Mechanics of Learning",
      "relationship": "depends_on"
    },
    {
      "from": "LMS Update Rule",
      "to": "Error Term",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Deterministic Model",
      "relationship": "related_to"
    },
    {
      "from": "Supervised Learning",
      "to": "Hypothesis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "Hidden Layer",
      "relationship": "subtopic"
    },
    {
      "from": "Reparameterization Trick",
      "to": "Gradient Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Iterative Update Process",
      "to": "Update Rule for Beta",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Kernel Trick",
      "relationship": "related_to"
    },
    {
      "from": "Softmax Function",
      "to": "Probability Vector",
      "relationship": "subtopic"
    },
    {
      "from": "Pretrained Large Language Models",
      "to": "Language Model Probability Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Foundation Models",
      "to": "Self-supervised Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Map Definition",
      "to": "Linear Function Over Features",
      "relationship": "subtopic"
    },
    {
      "from": "Generative Learning",
      "to": "p(x|y)",
      "relationship": "subtopic"
    },
    {
      "from": "Convergence",
      "to": "k-means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Iterative Update Process",
      "to": "Theta Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Inference",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Mapping",
      "to": "Kernel Trick",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Policy Definition",
      "to": "Value Function",
      "relationship": "depends_on"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretrained large language models",
      "relationship": "subtopic"
    },
    {
      "from": "Pretrained large language models",
      "to": "Zero-shot learning and in-context learning",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Residual Learning",
      "to": "Deep residual learning for image recognition",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Design Matrix X",
      "relationship": "related_to"
    },
    {
      "from": "Supervised learning",
      "to": "Deep learning",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "Zero-Shot Learning",
      "to": "Machine Learning Adaptation Methods",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Cost Function",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Network Architecture",
      "to": "Hidden Units",
      "relationship": "depends_on"
    },
    {
      "from": "Value Iteration",
      "to": "k Steps Update",
      "relationship": "depends_on"
    },
    {
      "from": "ELBO Interpretation",
      "to": "Conditional Likelihood Maximization",
      "relationship": "related_to"
    },
    {
      "from": "Discriminative Learning",
      "to": "Perceptron Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Supervised Learning",
      "to": "Classification Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Non-linear Dynamics to LQR",
      "relationship": "related_to"
    },
    {
      "from": "Classification and logistic regression",
      "to": "Logistic regression",
      "relationship": "subtopic"
    },
    {
      "from": "Physics Simulation",
      "to": "Open Dynamics Engine",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Transformation to Convex Form",
      "relationship": "related_to"
    },
    {
      "from": "Stacking Neurons",
      "to": "Complex Neural Network Example",
      "relationship": "example_of"
    },
    {
      "from": "Sufficient Statistic (T(y))",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Cross Validation Technique",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Two-layer Fully-Connected Neural Network",
      "to": "Generic Parameterization",
      "relationship": "related_to"
    },
    {
      "from": "Value Iteration",
      "to": "Asynchronous Update",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "EM for mixture of Gaussians",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Methods",
      "to": "Newton's Method",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Activation Functions",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning",
      "to": "Contrastive Learning",
      "relationship": "related_to"
    },
    {
      "from": "EM Algorithm Overview",
      "to": "E-step and M-step",
      "relationship": "subtopic"
    },
    {
      "from": "Tanh Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Double Descent Phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Joint Distribution Model",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Actions",
      "relationship": "component_of"
    },
    {
      "from": "Inner-loop Steps",
      "to": "k-means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning",
      "to": "Markov Decision Processes (MDP)",
      "relationship": "defines_formalism"
    },
    {
      "from": "Step 2: Derive Optimal Policy",
      "to": "Dynamic Programming Application",
      "relationship": "related_to"
    },
    {
      "from": "Mixture of Gaussians",
      "to": "EM Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Vectorization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Hypothesis Space (k)",
      "relationship": "subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Canonical Link Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Training vs Test Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Back-propagation for MLPs",
      "to": "Forward Pass in MLP",
      "relationship": "subtopic"
    },
    {
      "from": "Optimal Value Function",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Neural Networks (CNNs)",
      "to": "1-D Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Stochastic Model",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Softmax Function",
      "relationship": "depends_on"
    },
    {
      "from": "Primal-Dual Relationship",
      "to": "Dual Problem",
      "relationship": "related_to"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Vectorization",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM Algorithm",
      "to": "M-step",
      "relationship": "has_part"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Overfitting and Underfitting",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient (REINFORCE)",
      "to": "Expected Total Payoff",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "MLP (Multilayer Perceptron)",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization in RL",
      "to": "LQR Framework",
      "relationship": "related_to"
    },
    {
      "from": "L1 Norm (LASSO)",
      "to": "Kernel Methods Compatibility",
      "relationship": "related_to"
    },
    {
      "from": "Probability_Distributions",
      "to": "Jensen's_Inequality",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian discriminant analysis",
      "to": "Multivariate normal distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Applications of PCA",
      "to": "Eigenfaces Method",
      "relationship": "subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Vectorization over training examples",
      "relationship": "subtopic"
    },
    {
      "from": "SMO Algorithm",
      "to": "Convergence Check",
      "relationship": "depends_on"
    },
    {
      "from": "Forward Pass",
      "to": "Kalman Gain",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression (LWLR)",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Discount Factor",
      "relationship": "component_of"
    },
    {
      "from": "Machine Learning Models",
      "to": "Conditional Distribution Modeling",
      "relationship": "has_subtopic"
    },
    {
      "from": "Empirical Covariance Matrix",
      "to": "Eigenvectors and Eigenvalues",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problem in SVM",
      "to": "Geometric Margin",
      "relationship": "depends_on"
    },
    {
      "from": "Applications of PCA",
      "to": "Noise Reduction",
      "relationship": "related_to"
    },
    {
      "from": "Update Rule",
      "to": "Stochastic Gradient Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Expected Immediate Reward",
      "to": "Model Learning",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Naive Bayes Classifier",
      "relationship": "related_to"
    },
    {
      "from": "Generalization Error Guarantees",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous-State MDPs",
      "to": "Discretization Method",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Layer Normalization (LN)",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Pass",
      "to": "Derivatives Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting",
      "to": "Variance in Model",
      "relationship": "depends_on"
    },
    {
      "from": "Neural Networks",
      "to": "Parameters (\u03b8)",
      "relationship": "depends_on"
    },
    {
      "from": "Kernel methods",
      "to": "LMS with features",
      "relationship": "subtopic"
    },
    {
      "from": "MAP Estimate",
      "to": "MLE vs. MAP",
      "relationship": "depends_on"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "Bellman's Equations",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Dual Formulation of SVM",
      "relationship": "subtopic"
    },
    {
      "from": "Transformers",
      "to": "Conditional Probability",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Value iteration and policy iteration",
      "relationship": "subtopic"
    },
    {
      "from": "KKT Conditions",
      "to": "Dual Complementarity Condition",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel methods",
      "to": "Feature maps",
      "relationship": "subtopic"
    },
    {
      "from": "Evidence Lower Bound (ELBO)",
      "to": "Single Example ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Log_Probability_Bound",
      "to": "Probability_Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "EM Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "Classification",
      "to": "Multi-class Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Support Vector Machines (SVM)",
      "relationship": "related_to"
    },
    {
      "from": "Transformers",
      "to": "Autoregressive Decoding",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Bellman Update",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Convex Quadratic Function",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Matrix Properties",
      "to": "Sufficient Conditions for Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Theorem",
      "to": "Expectation Estimation",
      "relationship": "related_to"
    },
    {
      "from": "Objective Function Primal",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Functions",
      "to": "Efficiency Considerations",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Probabilistic Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Iteration",
      "to": "Greedy Policy with Respect to V",
      "relationship": "related_to"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model Complexity Measures",
      "relationship": "depends_on"
    },
    {
      "from": "k-means Algorithm",
      "to": "Convergence Properties",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Mini-batch Stochastic Gradient Descent",
      "relationship": "contains"
    },
    {
      "from": "Chain Rule",
      "to": "Auto-Differentiation",
      "relationship": "depends_on"
    },
    {
      "from": "Cross-Entropy Loss",
      "to": "Gradient Calculation",
      "relationship": "related_to"
    },
    {
      "from": "Evidence Lower Bound (ELBO)",
      "to": "Multiple Examples ELBO",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Functions",
      "to": "Feature Maps",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent",
      "to": "Stochastic Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "LMS with Features",
      "to": "Linear Function Over Features",
      "relationship": "depends_on"
    },
    {
      "from": "Multi-layer Neural Networks",
      "to": "Notational Consistency",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Overview",
      "to": "Maximum Likelihood Estimation",
      "relationship": "depends_on"
    },
    {
      "from": "ResNet (Residual Network)",
      "to": "Residual Block",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Backward Function for Parameters",
      "to": "Efficiency Considerations",
      "relationship": "subtopic"
    },
    {
      "from": "Overfitting and Underfitting",
      "to": "Generalization Gap",
      "relationship": "depends_on"
    },
    {
      "from": "Few-Shot Learning",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "DataPreprocessingAndAnalysis",
      "to": "PCAAlgorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "Densities and linear transformations",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Learning Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation",
      "to": "Density on x",
      "relationship": "subtopic"
    },
    {
      "from": "Model-wise Double Descent",
      "to": "Overparameterized Models",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Trajectory Collection",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Ascent Rule",
      "to": "Chain Rule Application",
      "relationship": "depends_on"
    },
    {
      "from": "Normalization Constant",
      "to": "Exponential Family Distributions",
      "relationship": "related_to"
    },
    {
      "from": "Linear Function Approximation",
      "to": "Parameters (Weights)",
      "relationship": "defines"
    },
    {
      "from": "Fully-Connected Neural Network",
      "to": "Intermediate Variables (a_i)",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Neural Networks",
      "relationship": "contains"
    },
    {
      "from": "Sigmoid Function",
      "to": "Tanh Function",
      "relationship": "related_to"
    },
    {
      "from": "M-step Update Rule",
      "to": "Lagrangian Method",
      "relationship": "related_to"
    },
    {
      "from": "Regularization and model selection",
      "to": "Bayesian statistics and regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Identity Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "LQR Algorithm Steps",
      "to": "Step 2: Derive Optimal Policy",
      "relationship": "subtopic"
    },
    {
      "from": "Procedure VE",
      "to": "Linear System Solver",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Fully-Connected Neural Network",
      "relationship": "has_subtopic"
    },
    {
      "from": "Black Box Nature of Neural Networks",
      "to": "Deep Learning Representations",
      "relationship": "subtopic"
    },
    {
      "from": "Training Data Classification",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Model Evaluation",
      "to": "Mean Squared Error (MSE)",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Single Neuron Network",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis Function",
      "to": "Training Error",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Algebra",
      "to": "Vectorization in Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Support vector machines",
      "to": "Notation (option reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Fitted Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Generative Models",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Gaussian Discriminant Analysis (GDA)",
      "relationship": "related_to"
    },
    {
      "from": "Convolutional Neural Networks (CNN)",
      "to": "1-D Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "Regularizer",
      "to": "Regularized Loss",
      "relationship": "depends_on"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "EM Algorithm",
      "relationship": "extends"
    },
    {
      "from": "Log Probability Derivative",
      "to": "Policy Gradient Theorem",
      "relationship": "depends_on"
    },
    {
      "from": "Discretization Method",
      "to": "Curse of Dimensionality",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "ELBO Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Discriminative Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous State MDPs",
      "to": "Discretization",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation-Maximization (EM) Algorithm",
      "to": "M-step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Transformation to Convex Form",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel methods",
      "to": "LMS with the kernel trick",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Ordinary Least Squares (OLS)",
      "relationship": "contains"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Gaussian Data Example",
      "relationship": "subtopic"
    },
    {
      "from": "Cost Function",
      "to": "Vector Calculus",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Geometric Margin",
      "relationship": "has_subtopic"
    },
    {
      "from": "Constructing GLMs",
      "to": "Logistic regression (GLM)",
      "relationship": "subtopic"
    },
    {
      "from": "Log Likelihood Function",
      "to": "Likelihood Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Naive bayes (Option Reading)",
      "to": "Laplace smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Binary Classification",
      "relationship": "defines"
    },
    {
      "from": "Kernel methods",
      "to": "Properties of kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Log-Likelihood Optimization",
      "to": "Single Example ELBO",
      "relationship": "related_to"
    },
    {
      "from": "Neural Networks",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Generative Learning Algorithms",
      "relationship": "contains"
    },
    {
      "from": "Probability Estimation",
      "to": "Maximum Likelihood Estimates",
      "relationship": "subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Gaussian Family",
      "relationship": "subtopic"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Joint Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Distribution of Sources",
      "to": "Cumulative Distribution Function (CDF)",
      "relationship": "depends_on"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Eigenvectors and Eigenvalues",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Overview",
      "to": "Distribution of Sources",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Logistic Regression",
      "relationship": "contains"
    },
    {
      "from": "Pretraining Phase",
      "to": "Transfer Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Problems",
      "to": "SVM Optimization",
      "relationship": "depends_on"
    },
    {
      "from": "Conditional Probability",
      "to": "Softmax Function",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "Gaussian Distribution Assumption",
      "relationship": "depends_on"
    },
    {
      "from": "Optimization Challenges",
      "to": "EM Algorithm Overview",
      "relationship": "related_to"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function Approximation",
      "to": "Model or Simulator",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Maps and Kernels",
      "to": "Efficient Computation with Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Neural Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "Feature_Vector",
      "to": "Vocabulary",
      "relationship": "uses"
    },
    {
      "from": "Value of Dual Problem",
      "to": "Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrangian Optimization in SVMs",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Optimal Bellman Equation",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Functional Margin",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Batch Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Fully-Connected Neural Network",
      "to": "Two-Layer Fully-Connected NN",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Regression Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Historical Context",
      "relationship": "related_to"
    },
    {
      "from": "Posterior Distribution",
      "to": "EM Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Reparametrization Techniques",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Kernel Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Classification Model",
      "to": "Probabilistic Assumptions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Necessary Conditions for Valid Kernels",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation Overview",
      "to": "Concrete Backprop Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Loss Function Analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Training Set",
      "to": "Bayesian Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Binary Classification",
      "relationship": "related_to"
    },
    {
      "from": "Transformer Model",
      "to": "Input-Output Interface",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Decision Boundary",
      "relationship": "related_to"
    },
    {
      "from": "Backward Function for Parameters",
      "to": "Vectorized Notation",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Primal-Dual Relationship",
      "to": "Primal Problem",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "Optimal Parameters Calculation",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Ordinary Least Squares (OLS)",
      "relationship": "special_case_of"
    },
    {
      "from": "Machine Learning Techniques",
      "to": "Zero-shot Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Finetuning Algorithm",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Discrete Action Space",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Multi-layer Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent Update Rule",
      "to": "High-Dimensional Feature Maps",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Neural Network Architecture",
      "relationship": "related_to"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "From non-linear dynamics to LQR",
      "relationship": "has_subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Law of Total Expectation",
      "relationship": "depends_on"
    },
    {
      "from": "State Transition Probabilities",
      "to": "Model Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Posterior Approximation",
      "to": "Prior Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Classification and logistic regression",
      "to": "Multi-class classification",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Intermediate Variables",
      "relationship": "subtopic"
    },
    {
      "from": "Transfer Learning",
      "to": "Machine Learning Overview",
      "relationship": "related_to"
    },
    {
      "from": "Multivariate Normal Distribution",
      "to": "Covariance Matrix Impact",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis Space H",
      "to": "Uniform Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Log Likelihood Function",
      "relationship": "depends_on"
    },
    {
      "from": "Single Neuron Model",
      "to": "Bias Term",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines",
      "to": "Non-Separable Case",
      "relationship": "related_to"
    },
    {
      "from": "Decision Boundary",
      "to": "Machine Learning Concepts",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Hypothesis Space Parameterization",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Standard Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimization Techniques",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Chain Rule",
      "relationship": "related_to"
    },
    {
      "from": "Sequential Minimal Optimization (SMO) Algorithm",
      "to": "Coordinate Ascent Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Convergence Check",
      "to": "Tolerance Parameter",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization",
      "to": "\\(\\ell_{1}\\) Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Projection of Data Points",
      "to": "Variance Maximization",
      "relationship": "depends_on"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Linearization of dynamics",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cumulative Distribution Function (CDF)",
      "to": "Sigmoid Function as Default Density",
      "relationship": "related_to"
    },
    {
      "from": "Linear Quadratic Regulator (LQR)",
      "to": "Algorithm Steps for LQR",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Architectures",
      "to": "Transformer Architecture",
      "relationship": "related_to"
    },
    {
      "from": "Softplus Function",
      "to": "ReLU Function",
      "relationship": "depends_on"
    },
    {
      "from": "Stochastic Gradient Descent",
      "to": "Learning Rate Decay",
      "relationship": "depends_on"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Loss Function Gradient Calculation",
      "to": "Cross Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Training Process",
      "to": "Cross-Entropy Loss",
      "relationship": "uses"
    },
    {
      "from": "Vanilla REINFORCE Algorithm",
      "to": "Policy Gradient Theorem",
      "relationship": "subtopic"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Policy Mapping",
      "relationship": "subtopic"
    },
    {
      "from": "Bias Term",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Contrastive Learning",
      "to": "SIMCLR",
      "relationship": "subtopic"
    },
    {
      "from": "Partially Observable MDPs (POMDP)",
      "to": "Belief State",
      "relationship": "subtopic"
    },
    {
      "from": "Pretraining Methods in CV",
      "to": "Supervised Pretraining",
      "relationship": "contains"
    },
    {
      "from": "Optimization Problems",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Optimal Policy Derivation",
      "to": "Linear Optimal Action",
      "relationship": "subtopic"
    },
    {
      "from": "BERT Model",
      "to": "Machine Learning Literature",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "Re-parametrization Trick",
      "relationship": "uses"
    },
    {
      "from": "1-D Convolution",
      "to": "Matrix Multiplication",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation Overview",
      "to": "Backward Function Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Theory",
      "to": "Generalization Error",
      "relationship": "subtopic"
    },
    {
      "from": "Model Creation Methods",
      "to": "Learning from Data",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Gaussian (LQG)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Convolutional Layers",
      "to": "Parameter Sharing",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Logistic Regression",
      "relationship": "special_case_of"
    },
    {
      "from": "Update Rule",
      "to": "LMS Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Classification and logistic regression",
      "to": "Maximizing ell(theta)",
      "relationship": "subtopic"
    },
    {
      "from": "Convolutional Layers",
      "to": "1D Convolution",
      "relationship": "subtopic"
    },
    {
      "from": "Training Examples Representation",
      "to": "Vectorization of Operations",
      "relationship": "subtopic_of"
    },
    {
      "from": "Latent Variable Models",
      "to": "Continuous Latent Variables",
      "relationship": "subtopic"
    },
    {
      "from": "Properties of Kernels",
      "to": "Characterization of Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Reward Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Variational Inference",
      "to": "ELBO (Evidence Lower Bound)",
      "relationship": "depends_on"
    },
    {
      "from": "Optimal Bellman Equation",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Variational Inference",
      "to": "Variational Auto-Encoder (VAE)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Model Evaluation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Model Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Data Example",
      "to": "Mixing Matrix A",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous Latent Variables",
      "to": "Succinct Representation of Means",
      "relationship": "related_to"
    },
    {
      "from": "Deep Learning",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "General strategy of backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Generative Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Underfitting Example",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian discriminant analysis",
      "to": "The Gaussian discriminant analysis model",
      "relationship": "subtopic"
    },
    {
      "from": "Sufficient Conditions for Kernels",
      "to": "Mercer's Theorem",
      "relationship": "depends_on"
    },
    {
      "from": "Constrained Optimization",
      "to": "Primal Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Policy Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Scaling-Invariant Property",
      "to": "MM_{W,b}",
      "relationship": "depends_on"
    },
    {
      "from": "Algorithm",
      "to": "Bellman Equations",
      "relationship": "related_to"
    },
    {
      "from": "Value Function Approximation",
      "to": "Feature Mapping",
      "relationship": "contains"
    },
    {
      "from": "Bellman Equations",
      "to": "Value Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Partially Observable MDPs (POMDP)",
      "relationship": "depends_on"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Nominal Trajectory Generation",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Iteration",
      "to": "VE Procedure",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Neural Networks",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Classification Model",
      "relationship": "related_to"
    },
    {
      "from": "Optimization in RL",
      "to": "Hessian Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Continuous Setting Assumptions",
      "to": "Quadratic Rewards",
      "relationship": "related_to"
    },
    {
      "from": "Neural Network Architecture",
      "to": "Output Layer",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic Regression",
      "to": "Logit",
      "relationship": "depends_on"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Gradient Descent Optimization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Pretrained Large Language Models",
      "to": "Natural Language Processing",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backward Function for Loss Functions",
      "relationship": "has_subtopic"
    },
    {
      "from": "Newton's Method",
      "to": "Convergence Rate",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Loss Function",
      "to": "Gradient Computation",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Matrix Multiplication Module (MM)",
      "relationship": "related_to"
    },
    {
      "from": "Cross Validation",
      "to": "Hold-Out Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Trick",
      "to": "Dual Form of Problem",
      "relationship": "related_to"
    },
    {
      "from": "Convergence Check",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Spam_Filtering",
      "to": "Feature_Vector",
      "relationship": "depends_on"
    },
    {
      "from": "Model Selection",
      "to": "Validation Set",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Stochastic Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Belief States Update",
      "to": "Predict Step",
      "relationship": "follows"
    },
    {
      "from": "Binary Features",
      "to": "Naive Bayes Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimization Constraints",
      "relationship": "has_subtopic"
    },
    {
      "from": "Objective Function Dual",
      "to": "Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "ReLU Function",
      "to": "GELU Function",
      "relationship": "related_to"
    },
    {
      "from": "Naive Bayes Classifier",
      "to": "Laplace Smoothing",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization",
      "to": "The double descent phenomenon",
      "relationship": "subtopic"
    },
    {
      "from": "Binary Classification Problem",
      "to": "MLP Model",
      "relationship": "subtopic"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "LN-S Module",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Chain Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "LQR Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic Regression",
      "to": "Classification Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Estimator Simplification",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Empirical Risk Minimization (ERM)",
      "relationship": "depends_on"
    },
    {
      "from": "Simultaneous Generalization Guarantee",
      "to": "Generalization Error Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Model-wise Double Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Geometric Margin",
      "relationship": "related_to"
    },
    {
      "from": "Regularization",
      "to": "Model Complexity",
      "relationship": "controls"
    },
    {
      "from": "Locally Weighted Linear Regression",
      "to": "Weight Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Step 3",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning",
      "to": "Supervised Learning with Non-Linear Models",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Architectures",
      "to": "ResNet Architecture",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Expectation-Maximization (EM) Algorithm",
      "relationship": "depends_on"
    },
    {
      "from": "Model Complexity Measures",
      "to": "Norm of Learned Model",
      "relationship": "subtopic"
    },
    {
      "from": "Optimizers and Generalization",
      "to": "Implicit Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis Classes",
      "to": "Finite Hypothesis Classes",
      "relationship": "subtopic"
    },
    {
      "from": "The normal equations",
      "to": "Least squares revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Housing Prices Model",
      "relationship": "depends_on"
    },
    {
      "from": "Naive Bayes Classifier",
      "to": "Parameter Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Probabilistic Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Density Transformation",
      "to": "ICA Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Jensen's Inequality",
      "to": "Random Variables and Expectations",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Separability",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Descent Methods",
      "to": "Batch Gradient Descent",
      "relationship": "contains"
    },
    {
      "from": "Support vector machines",
      "to": "Optimal margin classifier (option reading)",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Engineering",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Test Error",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models (GLMs)",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Stochastic Gradient Descent (SGD)",
      "to": "Gradient Calculation",
      "relationship": "contains"
    },
    {
      "from": "Functional Margin",
      "to": "Scaling w and b",
      "relationship": "subtopic"
    },
    {
      "from": "Sample Complexity Bounds",
      "to": "Bias-Variance Tradeoff",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function",
      "to": "Policy Execution",
      "relationship": "depends_on"
    },
    {
      "from": "Lambda Parameter",
      "to": "Regularized Loss",
      "relationship": "subtopic"
    },
    {
      "from": "ICA Ambiguities",
      "to": "Scaling Ambiguity",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Empirical Risk Minimization",
      "relationship": "depends_on"
    },
    {
      "from": "LQR Algorithm Steps",
      "to": "Step 1: Estimate Matrices",
      "relationship": "subtopic"
    },
    {
      "from": "Transformers",
      "to": "Training Process",
      "relationship": "has_subtopic"
    },
    {
      "from": "Optimal Policy",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Logistic Regression",
      "to": "Hessian Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Backward Function for Loss Functions",
      "to": "Logistic Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "Rewriting System Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Sequential Minimal Optimization (SMO)",
      "to": "Alpha Values Update",
      "relationship": "subtopic"
    },
    {
      "from": "Multi-class Classification",
      "to": "Softmax Function",
      "relationship": "subtopic"
    },
    {
      "from": "Functional Margin",
      "to": "Confidence and Correct Prediction",
      "relationship": "depends_on"
    },
    {
      "from": "Learned Features",
      "to": "Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Principal Components",
      "relationship": "subtopic"
    },
    {
      "from": "Posterior Distribution Q(z)",
      "to": "Tight Bound Conditions",
      "relationship": "subtopic"
    },
    {
      "from": "1-D Convolution",
      "to": "Filter Vector",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Sequential Minimal Optimization (SMO) Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "LQR Extension",
      "to": "Kalman Filter",
      "relationship": "depends_on"
    },
    {
      "from": "Value Iteration",
      "to": "Convergence of Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Regularization",
      "relationship": "depends_on"
    },
    {
      "from": "Supervised learning",
      "to": "Classification and logistic regression",
      "relationship": "subtopic"
    },
    {
      "from": "Principal Component Analysis (PCA)",
      "to": "Variance Maximization",
      "relationship": "subtopic"
    },
    {
      "from": "Training Set",
      "to": "Likelihood Estimation",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Vector Selection",
      "to": "Stop Words",
      "relationship": "depends_on"
    },
    {
      "from": "Constrained Optimization",
      "to": "Lagrange Multipliers",
      "relationship": "depends_on"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Independent components analysis",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "SMO Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Other Normalization Layers",
      "relationship": "related_to"
    },
    {
      "from": "Transformation to Convex Form",
      "to": "Final Optimization Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Softmax Function",
      "to": "Logits",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Statistics",
      "to": "Prior Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Sigmoid Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "Log-Likelihood Maximization",
      "to": "Convergence Proof",
      "relationship": "depends_on"
    },
    {
      "from": "Backpropagation",
      "to": "Gradient Calculation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Optimization Problem",
      "to": "Lagrangian Formulation",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vectors",
      "to": "Dual Form of Problem",
      "relationship": "related_to"
    },
    {
      "from": "Training Loss/Cost Function",
      "to": "Regularized Loss J\u03bb(\u03b8)",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Value Iteration",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backward Function for Parameters",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines (SVMs)",
      "to": "Notation for SVMs",
      "relationship": "subtopic"
    },
    {
      "from": "Sequential Decision Making",
      "to": "Reinforcement Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Models",
      "to": "Generative Learning Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Linear Model Limitations",
      "to": "Underfitting",
      "relationship": "results_in"
    },
    {
      "from": "VC Dimension",
      "to": "Shattering Sets",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Mixing Matrix Ambiguity",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning",
      "to": "Naive Bayes Classifier",
      "relationship": "depends_on"
    },
    {
      "from": "SIMCLR",
      "to": "Augmentation Techniques",
      "relationship": "depends_on"
    },
    {
      "from": "Sufficient Statistic for Bernoulli (T(y))",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Methods",
      "to": "Adam: A method for stochastic optimization",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Likelihood Function",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problems",
      "to": "Feasibility Constraints",
      "relationship": "subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Update Step",
      "relationship": "has_subtopic"
    },
    {
      "from": "Empirical Error",
      "to": "Uniform Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Learning Model Parameters",
      "to": "Inverted Pendulum Problem",
      "relationship": "example_of"
    },
    {
      "from": "1-D Convolution",
      "to": "Bias Scalar",
      "relationship": "depends_on"
    },
    {
      "from": "Generative learning algorithms",
      "to": "Gaussian discriminant analysis",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Independent Component Analysis (ICA)",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Value Functions in RL",
      "relationship": "has_subtopic"
    },
    {
      "from": "Finite Horizon MDPs",
      "to": "Discount Factor Role",
      "relationship": "related_to"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Gradient Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Partial Derivatives",
      "relationship": "related_to"
    },
    {
      "from": "Probability Bound",
      "to": "Uniform Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Double Descent Phenomenon",
      "to": "Optimal regularization can mitigate double descent",
      "relationship": "related_to"
    },
    {
      "from": "Data Augmentation",
      "to": "Negative Pair",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "LMS Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Finite Horizon MDP",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Transformations and Densities",
      "relationship": "subtopic"
    },
    {
      "from": "SMO Algorithm",
      "to": "Constraints in SMO",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "EM Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Support Vector Machines (SVM)",
      "relationship": "related_to"
    },
    {
      "from": "Constructing GLMs",
      "to": "Ordinary least squares",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Loss Functions",
      "relationship": "contains"
    },
    {
      "from": "Neural Networks",
      "to": "Two-Layer Neural Network",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Adaptation Methods",
      "to": "Pretraining Methods in CV",
      "relationship": "related_to"
    },
    {
      "from": "Support Vector Machines (SVM)",
      "to": "Sequential Minimal Optimization (SMO)",
      "relationship": "depends_on"
    },
    {
      "from": "Bias-variance tradeoff",
      "to": "A mathematical decomposition (for regression)",
      "relationship": "subtopic"
    },
    {
      "from": "Polynomial Fitting",
      "to": "Overfitting",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Principal Components Analysis (PCA)",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Dual Formulation of SVM",
      "to": "KKT Conditions",
      "relationship": "related_to"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Representation Function",
      "relationship": "depends_on"
    },
    {
      "from": "Naive Bayes Algorithm",
      "to": "Discretization",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Locally Weighted Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrange Duality",
      "to": "Lagrangian",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Stacking Neurons",
      "relationship": "contains"
    },
    {
      "from": "Support vector machines",
      "to": "Functional and geometric margins",
      "relationship": "subtopic"
    },
    {
      "from": "L2 Regularization",
      "to": "Regularizer",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "EM Algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Log Partition Function (a(\u03b7))",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "LQR Updates",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Functions",
      "to": "Training Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "LMS Update Rule",
      "relationship": "contains"
    },
    {
      "from": "Optimizers",
      "to": "Gradient Descent (GD)",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Regression Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Backward Pass",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kalman Filter",
      "to": "Belief States Update",
      "relationship": "has_subtopic"
    },
    {
      "from": "Validation Set",
      "to": "Hold Out Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Softplus Function",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Partial Derivatives",
      "relationship": "depends_on"
    },
    {
      "from": "Learning Guarantees",
      "to": "Hoeffding Inequality (Chernoff Bound)",
      "relationship": "depends_on"
    },
    {
      "from": "Variational Auto-Encoder (VAE)",
      "to": "Neural Network Parameterization",
      "relationship": "depends_on"
    },
    {
      "from": "Cubic Function Example",
      "to": "Linear Function Over Features",
      "relationship": "related_to"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Continuous State Space",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization Error",
      "to": "The generalization error of random features regression: Precise asymptotics and the double descent curve",
      "relationship": "related_to"
    },
    {
      "from": "Newton's Method",
      "to": "Maximizing Functions",
      "relationship": "related_to"
    },
    {
      "from": "House Price Prediction Example",
      "to": "Deep Learning Representations",
      "relationship": "subtopic"
    },
    {
      "from": "Finite Horizon MDPs",
      "to": "Payoff Calculation",
      "relationship": "subtopic"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "Expected Immediate Reward",
      "relationship": "related_to"
    },
    {
      "from": "Clustering",
      "to": "K-means Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Error Term",
      "to": "Probabilistic Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Feature Construction for Strings",
      "to": "Machine Learning Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Differential Dynamic Programming (DDP)",
      "to": "Linearization of Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Support vector machines",
      "to": "SMO algorithm (optional reading)",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "subtopic"
    },
    {
      "from": "SVM Optimization",
      "to": "Support Vectors",
      "relationship": "related_to"
    },
    {
      "from": "Solving Dual Problem",
      "to": "Dual Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation Algorithm",
      "to": "Efficiency Considerations",
      "relationship": "related_to"
    },
    {
      "from": "Multi-layer Neural Networks",
      "to": "Total Neurons and Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Total Parameters in Conv1D",
      "relationship": "subtopic"
    },
    {
      "from": "Self-Supervised Learning",
      "to": "Supervised Contrastive Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Regression Problems",
      "to": "Least Square Cost Function",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Ascent",
      "to": "Optimization Techniques",
      "relationship": "subtopic"
    },
    {
      "from": "LMS Update Rule",
      "to": "Widrow-Hoff Learning Rule",
      "relationship": "related_to"
    },
    {
      "from": "Distortion Function",
      "to": "k-means Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Bayesian Machine Learning",
      "to": "Training Set",
      "relationship": "depends_on"
    },
    {
      "from": "Dimensionality Reduction",
      "to": "Principal Component Analysis (PCA)",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised learning",
      "to": "Generalized linear models",
      "relationship": "subtopic"
    },
    {
      "from": "Machine_Learning",
      "to": "Text_Classification",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Loss Function J(\u03b8)",
      "relationship": "subtopic"
    },
    {
      "from": "Probability Distribution",
      "to": "Likelihood Function",
      "relationship": "depends_on"
    },
    {
      "from": "Backward Pass",
      "to": "Update Step",
      "relationship": "follows"
    },
    {
      "from": "Gaussian Distribution",
      "to": "Density Visualization",
      "relationship": "subtopic"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Naive Bayes Classifier",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Latent Variable Models",
      "relationship": "contains"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "Biases",
      "relationship": "depends_on"
    },
    {
      "from": "Gradient Descent Optimizer",
      "to": "Minimum Norm Solution",
      "relationship": "subtopic"
    },
    {
      "from": "Optimization Techniques",
      "to": "Lagrange Multipliers",
      "relationship": "subtopic"
    },
    {
      "from": "Dynamic Programming",
      "to": "Value Iteration Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Probe Approach",
      "to": "Adaptation Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Fitted Value Iteration",
      "to": "Value Iteration",
      "relationship": "depends_on"
    },
    {
      "from": "Feature Maps and Kernels",
      "to": "Kernel Function Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Linearization of Dynamics",
      "to": "Taylor Expansion",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Adaptation Techniques",
      "to": "In-context Learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "True Error",
      "to": "Uniform Convergence",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Beta Update Equation",
      "relationship": "depends_on"
    },
    {
      "from": "Kalman Filter",
      "to": "Step 2",
      "relationship": "subtopic"
    },
    {
      "from": "The normal equations",
      "to": "Matrix derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Inner Product Computation",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning Overview",
      "to": "Policy Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Convolutional Neural Networks (CNNs)",
      "relationship": "related_to"
    },
    {
      "from": "Sparsity Regularization",
      "to": "L1 Norm (LASSO)",
      "relationship": "relates_to"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Mixture of Gaussians Model",
      "relationship": "related_to"
    },
    {
      "from": "Policy Gradient Methods",
      "to": "Expected Reward Function",
      "relationship": "related_to"
    },
    {
      "from": "Finetuning Pretrained Models",
      "to": "Optimization Process",
      "relationship": "subtopic"
    },
    {
      "from": "Explicit Regularization Techniques",
      "to": "Regularization in Deep Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "ICA Ambiguities",
      "relationship": "subtopic"
    },
    {
      "from": "ELBO Interpretation",
      "to": "KL Divergence",
      "relationship": "depends_on"
    },
    {
      "from": "Generalization and regularization",
      "to": "Generalization",
      "relationship": "subtopic"
    },
    {
      "from": "Negative Log-Likelihood",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Neural Networks",
      "to": "Basic Modules",
      "relationship": "subtopic"
    },
    {
      "from": "Value Function Approximation",
      "to": "Fitted Value Iteration",
      "relationship": "subtopic"
    },
    {
      "from": "Mixture of Gaussians Model",
      "to": "Model Parameters",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Function Representation",
      "relationship": "contains"
    },
    {
      "from": "Data Scarcity",
      "to": "Leave-One-Out Cross Validation",
      "relationship": "related_to"
    },
    {
      "from": "Convex Functions",
      "to": "Strict Convexity",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Adaptation Techniques",
      "to": "Zero-shot Adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Vocabulary",
      "to": "Stop_Words",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Fundamentals",
      "to": "Backward Function in Machine Learning",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-Variance Tradeoff",
      "to": "Training Model (h_S)",
      "relationship": "related_to"
    },
    {
      "from": "Derived Features",
      "to": "School Quality",
      "relationship": "subtopic"
    },
    {
      "from": "Finite Horizon MDP",
      "to": "Time Dependent Dynamics",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional Probability Modeling",
      "to": "Parameterized Model",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "Sample complexity bounds (optional readings)",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization in Machine Learning",
      "to": "Sparsity Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Negative Log-Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Process (MDP)",
      "to": "State Transition Probabilities",
      "relationship": "depends_on"
    },
    {
      "from": "Classification Problem",
      "to": "Binary Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis (GDA)",
      "to": "Model Parameters",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Model",
      "to": "Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Discriminative Learning",
      "to": "Conditional Distribution",
      "relationship": "related_to"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Feature Map Phi",
      "relationship": "related_to"
    },
    {
      "from": "Generalization and Regularization",
      "to": "Training Loss Function",
      "relationship": "contains"
    },
    {
      "from": "Unsupervised Learning",
      "to": "Clustering and k-means algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Bias and Variance Tradeoff",
      "to": "Model Selection via Cross Validation",
      "relationship": "subtopic"
    },
    {
      "from": "Gaussian Data Example",
      "to": "Rotation Matrix R",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDP)",
      "to": "State Transition Probabilities",
      "relationship": "component_of"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Value Function Definition",
      "relationship": "subtopic"
    },
    {
      "from": "Vector w",
      "to": "Decision Boundary",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Kernel Methods",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Functions",
      "to": "Sufficient Conditions for Valid Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Conditional Distribution Modeling",
      "to": "Exponential Family Distributions",
      "relationship": "subtopic"
    },
    {
      "from": "Two-Layer Neural Network",
      "to": "ReLU Function",
      "relationship": "depends_on"
    },
    {
      "from": "ELBO (Evidence Lower Bound)",
      "to": "Optimization Objective",
      "relationship": "subtopic"
    },
    {
      "from": "Lagrangian Function",
      "to": "Dual Form of Problem",
      "relationship": "subtopic"
    },
    {
      "from": "PCA",
      "to": "Eigenvectors of Sigma",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Optimal Margin Classifier",
      "relationship": "has_subtopic"
    },
    {
      "from": "Exponential Family Distributions",
      "to": "Poisson Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Deep Learning",
      "relationship": "related_to"
    },
    {
      "from": "Implicit Bias Studies",
      "to": "Surprises in high-dimensional ridgeless least squares interpolation",
      "relationship": "related_to"
    },
    {
      "from": "Markov Decision Processes (MDPs)",
      "to": "Policy Iteration",
      "relationship": "contains"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Geometric Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Derivatives",
      "to": "Gradient Definition",
      "relationship": "has_subtopic"
    },
    {
      "from": "Other Normalization Layers",
      "to": "Group Normalization",
      "relationship": "subtopic"
    },
    {
      "from": "Model Complexity Measures",
      "to": "Number of Parameters",
      "relationship": "related_to"
    },
    {
      "from": "Optimization Problems",
      "to": "Duality Gap",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "VC Dimension",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Models",
      "to": "Regularization",
      "relationship": "subtopic"
    },
    {
      "from": "Hypothesis Class Size",
      "to": "Generalization Error Bound",
      "relationship": "related_to"
    },
    {
      "from": "Matrix Notation in Machine Learning",
      "to": "Training Examples Representation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Loss Function",
      "to": "Average Loss",
      "relationship": "subtopic"
    },
    {
      "from": "Dual Formulation",
      "to": "Inner Products and Support Vectors",
      "relationship": "depends_on"
    },
    {
      "from": "Efficient Update",
      "to": "Constraints on \u03b1_i and \u03b1_j",
      "relationship": "subtopic"
    },
    {
      "from": "ResNet Architecture",
      "to": "Convolutional Layers",
      "relationship": "subtopic"
    },
    {
      "from": "Bernoulli Random Variable Z",
      "to": "Generalization Error Guarantees",
      "relationship": "subtopic"
    },
    {
      "from": "Expectation in MDPs",
      "to": "Rewards Dependence",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised Learning",
      "to": "Machine Learning Concepts",
      "relationship": "related_to"
    },
    {
      "from": "Latent Variable Models",
      "to": "Variational Auto-Encoder (VAE)",
      "relationship": "subtopic"
    },
    {
      "from": "Policy Iteration",
      "to": "Policy Improvement",
      "relationship": "subtopic"
    },
    {
      "from": "Value Iteration",
      "to": "Computational Complexity",
      "relationship": "related_to"
    },
    {
      "from": "Layer Normalization (LN)",
      "to": "Affine Transformation in LN",
      "relationship": "subtopic"
    },
    {
      "from": "Independent Component Analysis (ICA)",
      "to": "Cocktail Party Problem",
      "relationship": "related_to"
    },
    {
      "from": "Parameter Estimation",
      "to": "Laplace Smoothing",
      "relationship": "depends_on"
    },
    {
      "from": "Reinforcement learning",
      "to": "Learning a model for an MDP",
      "relationship": "subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "General EM algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Risk Minimization (ERM)",
      "to": "Finite Hypothesis Classes",
      "relationship": "subtopic"
    },
    {
      "from": "Empirical Risk Minimization",
      "to": "Hypotheses Training Error",
      "relationship": "related_to"
    },
    {
      "from": "Reinforcement Learning Overview",
      "to": "Total Payoff Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "Dynamics Model",
      "to": "Optimal Value Function",
      "relationship": "related_to"
    },
    {
      "from": "Update Step",
      "to": "Kalman Gain",
      "relationship": "defines"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Training vs Test Datasets",
      "relationship": "related_to"
    },
    {
      "from": "Other Activation Functions",
      "to": "Multi-layer Neural Networks",
      "relationship": "related_to"
    },
    {
      "from": "Unlabeled Data",
      "to": "Pretraining Phase",
      "relationship": "related_to"
    },
    {
      "from": "Natural Parameter for Bernoulli (\u03b7)",
      "to": "Bernoulli Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Derived Features",
      "to": "Walkability",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Perceptron Learning Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Leaky ReLU",
      "to": "Activation Functions",
      "relationship": "subtopic"
    },
    {
      "from": "Properties of Kernels",
      "to": "Explicit Definition of Feature Map",
      "relationship": "depends_on"
    },
    {
      "from": "Backward Function for Loss Functions",
      "to": "Cross-Entropy Loss",
      "relationship": "subtopic"
    }
  ]
}