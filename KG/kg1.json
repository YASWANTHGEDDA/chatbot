{
  "nodes": [
    {
      "id": "Machine Learning Basics",
      "type": "major",
      "parent": null,
      "description": "Introduction to fundamental concepts in machine learning."
    },
    {
      "id": "Function Representation",
      "type": "subnode",
      "parent": "Machine Learning Basics",
      "description": "How functions are represented and parameterized in ML models."
    },
    {
      "id": "Linear Function Approximation",
      "type": "subnode",
      "parent": "Function Representation",
      "description": "Approximating y as a linear function of x with parameters \u03b8."
    },
    {
      "id": "Parameters (Weights)",
      "type": "subnode",
      "parent": "Linear Function Approximation",
      "description": "\u03b8i's are the parameters or weights in the model."
    },
    {
      "id": "Cost Function",
      "type": "subnode",
      "parent": "Function Representation",
      "description": "Measures the accuracy of the hypothesis function using mean squared error."
    },
    {
      "id": "Ordinary Least Squares Regression",
      "type": "subnode",
      "parent": "Cost Function",
      "description": "Least-squares cost function leading to OLS regression model."
    },
    {
      "id": "LMS Algorithm",
      "type": "major",
      "parent": null,
      "description": "Iterative method for minimizing the mean squared error."
    },
    {
      "id": "Gradient Descent",
      "type": "major",
      "parent": null,
      "description": "Optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient."
    },
    {
      "id": "Learning Rate (\u03b1)",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Hyperparameter controlling the size of steps in gradient descent."
    },
    {
      "id": "Cost Function J(\u03b8)",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Function to be minimized, representing error between predictions and actual values."
    },
    {
      "id": "Update Rule",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Rule defining how parameters are updated in each iteration of gradient descent."
    },
    {
      "id": "LMS Update Rule",
      "type": "subnode",
      "parent": "Update Rule",
      "description": "Least Mean Squares update rule for adjusting parameters based on error between prediction and actual output."
    },
    {
      "id": "Learning a model for an MDP",
      "type": "major",
      "parent": null,
      "description": "Techniques to learn the dynamics of an environment from data."
    },
    {
      "id": "Continuous state MDPs",
      "type": "subnode",
      "parent": "Learning a model for an MDP",
      "description": "Discussion on handling continuous states in reinforcement learning problems."
    },
    {
      "id": "Discretization",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Methods for converting continuous state spaces into discrete ones."
    },
    {
      "id": "Value function approximation",
      "type": "subnode",
      "parent": "Continuous state MDPs",
      "description": "Techniques to approximate value functions in large or continuous state spaces."
    },
    {
      "id": "Connections between Policy and Value Iteration (Optional)",
      "type": "subnode",
      "parent": "Learning a model for an MDP",
      "description": "Analysis of the relationship between policy and value iteration methods."
    },
    {
      "id": "LQR, DDP and LQG",
      "type": "major",
      "parent": null,
      "description": "Techniques for optimal control in linear systems with quadratic costs."
    },
    {
      "id": "Finite-horizon MDPs",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Discussion on decision-making processes over a fixed time horizon."
    },
    {
      "id": "Linear Quadratic Regulation (LQR)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Optimal control for linear systems with quadratic cost functions"
    },
    {
      "id": "From non-linear dynamics to LQR",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Linearization techniques and differential dynamic programming"
    },
    {
      "id": "Linearization of dynamics",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Approximating nonlinear systems with linear models"
    },
    {
      "id": "Differential Dynamic Programming (DDP)",
      "type": "subnode",
      "parent": "From non-linear dynamics to LQR",
      "description": "Optimization technique for nonlinear control problems"
    },
    {
      "id": "Linear Quadratic Gaussian (LQG)",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Combination of LQR with stochastic disturbances"
    },
    {
      "id": "Policy Gradient (REINFORCE)",
      "type": "major",
      "parent": null,
      "description": "Gradient-based methods for optimizing policies in reinforcement learning"
    },
    {
      "id": "Supervised Learning Examples",
      "type": "major",
      "parent": null,
      "description": "Examples of supervised learning problems with datasets and feature-target relationships"
    },
    {
      "id": "Machine Learning Algorithms",
      "type": "major",
      "parent": null,
      "description": "Various algorithms used in machine learning for prediction and analysis."
    },
    {
      "id": "Stochastic Gradient Descent (SGD)",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Variant of gradient descent where parameters are updated after observing each training example, making it faster but potentially less accurate."
    },
    {
      "id": "Batch Gradient Descent",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Variant of gradient descent that computes the loss for each training example and updates parameters once per epoch."
    },
    {
      "id": "Normal Equations",
      "type": "major",
      "parent": null,
      "description": "Direct method for solving linear regression problems without iterative optimization."
    },
    {
      "id": "Matrix Derivatives",
      "type": "subnode",
      "parent": "Normal Equations",
      "description": "Derivation of matrix derivatives for functions mapping matrices to real numbers."
    },
    {
      "id": "Linear Regression",
      "type": "major",
      "parent": null,
      "description": "Technique for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables)."
    },
    {
      "id": "Convex Function",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Function whose domain is a convex set and which satisfies the property that for any two points in its domain, the function value at the midpoint of these points is less than or equal to the average of the function values at the two points."
    },
    {
      "id": "Optimization Problem",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Mathematical problem that seeks to minimize a cost function over a set of possible solutions."
    },
    {
      "id": "Gradient Definition",
      "type": "subnode",
      "parent": "Matrix Derivatives",
      "description": "Definition and example of the gradient of a function with respect to an n-by-d matrix."
    },
    {
      "id": "Least Squares Revisited",
      "type": "major",
      "parent": null,
      "description": "Revisiting least squares using tools from matrix derivatives."
    },
    {
      "id": "Design Matrix",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Definition and role of the design matrix in linear regression problems."
    },
    {
      "id": "Vector y",
      "type": "subnode",
      "parent": "Least Squares Revisited",
      "description": "Description of vector y containing target values from training set."
    },
    {
      "id": "Supervised Learning",
      "type": "major",
      "parent": null,
      "description": "Learning to predict outputs from inputs using labeled training data."
    },
    {
      "id": "Regression Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Statistical approach to predicting continuous outcomes based on input variables."
    },
    {
      "id": "Classification Problem",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Task of predicting discrete class labels based on input features."
    },
    {
      "id": "Hypothesis Function",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Function learned by a machine learning model to make predictions."
    },
    {
      "id": "Feature Selection",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Process of selecting a subset of relevant features for use in model construction."
    },
    {
      "id": "Widrow-Hoff Learning Rule",
      "type": "subnode",
      "parent": "LMS Update Rule",
      "description": "Alternative name for the LMS update rule in machine learning."
    },
    {
      "id": "Error Term",
      "type": "subnode",
      "parent": "LMS Update Rule",
      "description": "Difference between actual output and predicted value used to adjust parameters."
    },
    {
      "id": "Single Training Example",
      "type": "subnode",
      "parent": "LMS Update Rule",
      "description": "Derivation and application of LMS rule for a single data point."
    },
    {
      "id": "Probabilistic Models in Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Models that describe the probability distribution of data given parameters."
    },
    {
      "id": "Conditional Probability Distribution",
      "type": "subnode",
      "parent": "Probabilistic Models in Machine Learning",
      "description": "Distribution of y given x and parameter \u03b8."
    },
    {
      "id": "Design Matrix X",
      "type": "subnode",
      "parent": "Probabilistic Models in Machine Learning",
      "description": "Matrix containing all input data points x(i)."
    },
    {
      "id": "Likelihood Function",
      "type": "subnode",
      "parent": "Probabilistic Models in Machine Learning",
      "description": "Function that calculates the likelihood of parameters based on observed data."
    },
    {
      "id": "Independence Assumption",
      "type": "subnode",
      "parent": "Likelihood Function",
      "description": "Assumption that errors \u03b5(i) are independent."
    },
    {
      "id": "Maximum Likelihood Estimation (MLE)",
      "type": "subnode",
      "parent": "Probabilistic Models in Machine Learning",
      "description": "Method to estimate parameters by maximizing likelihood function L(\u03b8)."
    },
    {
      "id": "Log Likelihood Function",
      "type": "subnode",
      "parent": "Maximum Likelihood Estimation (MLE)",
      "description": "Natural logarithm of the likelihood function for easier computation."
    },
    {
      "id": "Machine Learning Concepts",
      "type": "major",
      "parent": null,
      "description": "Overview of fundamental concepts in machine learning."
    },
    {
      "id": "Least-Squares Cost Function",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Method to minimize the sum of squared differences between observed and predicted values."
    },
    {
      "id": "Probabilistic Interpretation",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Interpreting linear regression from a probabilistic perspective."
    },
    {
      "id": "Invertibility of X^TX Matrix",
      "type": "subnode",
      "parent": "Least-Squares Cost Function",
      "description": "Conditions for the matrix to be invertible in least-squares calculations."
    },
    {
      "id": "Error Term Distribution",
      "type": "subnode",
      "parent": "Probabilistic Interpretation",
      "description": "Assumption that error terms follow a Gaussian distribution with mean zero and variance sigma squared."
    },
    {
      "id": "Underfitting",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Model is too simple to capture the underlying pattern of data."
    },
    {
      "id": "Overfitting",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Model performs well on training data but poorly on unseen data."
    },
    {
      "id": "Locally Weighted Linear Regression (LWR)",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Regression algorithm that assigns more weight to nearby points when predicting a target value."
    },
    {
      "id": "Locally Weighted Linear Regression (LWLR)",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "A variant of linear regression where weights are assigned to training examples based on their proximity to the query point."
    },
    {
      "id": "Weight Calculation",
      "type": "subnode",
      "parent": "Locally Weighted Linear Regression (LWLR)",
      "description": "In LWLR, weights are calculated using a formula that depends on the distance between data points and the query point."
    },
    {
      "id": "Bandwidth Parameter (\u03c4)",
      "type": "subnode",
      "parent": "Weight Calculation",
      "description": "Controls how quickly the weight of a training example decreases with its distance from the query point."
    },
    {
      "id": "Least-Squares Regression",
      "type": "subnode",
      "parent": "Machine Learning Concepts",
      "description": "Regression technique that minimizes the sum of squared errors between predictions and actual values."
    },
    {
      "id": "Probabilistic Assumptions",
      "type": "subnode",
      "parent": "Likelihood Function",
      "description": "Underlying probabilistic assumptions for justifying least-squares regression as MLE."
    },
    {
      "id": "Locally Weighted Linear Regression",
      "type": "subnode",
      "parent": "Linear Regression",
      "description": "Non-parametric algorithm that uses a weighted approach for regression tasks."
    },
    {
      "id": "Classification and Logistic Regression",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Predicting categorical outcomes using logistic functions."
    },
    {
      "id": "Logistic Regression",
      "type": "subnode",
      "parent": "Classification and Logistic Regression",
      "description": "Method for binary classification using a logistic function to model the probability of class membership."
    },
    {
      "id": "Perceptron Learning Algorithm",
      "type": "subnode",
      "parent": "Classification and Logistic Regression",
      "description": "An algorithm for learning linear classifiers."
    },
    {
      "id": "Multi-class Classification",
      "type": "subnode",
      "parent": "Classification and Logistic Regression",
      "description": "Extending binary classification to multiple classes."
    },
    {
      "id": "Maximizing l(theta)",
      "type": "subnode",
      "parent": "Classification and Logistic Regression",
      "description": "Techniques for maximizing the likelihood function in logistic regression."
    },
    {
      "id": "Generalized Linear Models",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Models that extend linear models to non-linear relationships."
    },
    {
      "id": "Exponential Family",
      "type": "subnode",
      "parent": "Generalized Linear Models",
      "description": "A class of probability distributions including normal, binomial, and Poisson distributions."
    },
    {
      "id": "Constructing GLMs",
      "type": "subnode",
      "parent": "Generalized Linear Models",
      "description": "Methods for constructing generalized linear models from exponential family distributions."
    },
    {
      "id": "Ordinary Least Squares",
      "type": "subnode",
      "parent": "Constructing GLMs",
      "description": "A method to fit a model by minimizing the sum of squared residuals."
    },
    {
      "id": "Logistic Regression (GLM)",
      "type": "subnode",
      "parent": "Constructing GLMs",
      "description": "Application of generalized linear models in logistic regression."
    },
    {
      "id": "Generative Learning Algorithms",
      "type": "subnode",
      "parent": "Supervised Learning",
      "description": "Algorithms that model the distribution of data to make predictions."
    },
    {
      "id": "Gaussian Discriminant Analysis",
      "type": "subnode",
      "parent": "Generative Learning Algorithms",
      "description": "A generative classifier based on Gaussian distributions."
    },
    {
      "id": "Multivariate Normal Distribution",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis",
      "description": "Distribution of multiple correlated variables following a normal distribution."
    },
    {
      "id": "GDA Model",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis",
      "description": "Model for classification using Gaussian discriminant analysis."
    },
    {
      "id": "Discussion: GDA and Logistic Regression",
      "type": "subnode",
      "parent": "Gaussian Discriminant Analysis",
      "description": "Comparison between GDA and logistic regression models."
    },
    {
      "id": "Naive Bayes",
      "type": "subnode",
      "parent": "Generative Learning Algorithms",
      "description": "A simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions."
    },
    {
      "id": "Laplace Smoothing",
      "type": "subnode",
      "parent": "Naive Bayes",
      "description": "Technique to handle zero probabilities in categorical data."
    },
    {
      "id": "Event Models for Text Classification",
      "type": "subnode",
      "parent": "Naive Bayes",
      "description": "Models used for classifying text documents based on word occurrences."
    },
    {
      "id": "Kernel Methods",
      "type": "major",
      "parent": null,
      "description": "Techniques using kernel functions to extend linear methods to non-linear spaces."
    },
    {
      "id": "Feature Maps",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Mapping data into a higher-dimensional space for better separation."
    },
    {
      "id": "LMS with Features",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Least mean squares algorithm applied to feature-mapped data."
    },
    {
      "id": "LMS with Kernel Trick",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Efficient computation of LMS using kernel functions without explicit mapping."
    },
    {
      "id": "Properties of Kernels",
      "type": "subnode",
      "parent": "Kernel Methods",
      "description": "Characteristics and requirements for valid kernel functions."
    },
    {
      "id": "Support Vector Machines",
      "type": "major",
      "parent": null,
      "description": "Algorithms that find the optimal hyperplane to separate data points with maximal margin."
    },
    {
      "id": "Margins: Intuition",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Understanding the concept of margins in SVMs."
    },
    {
      "id": "Notation (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Mathematical notation used in SVM formulations."
    },
    {
      "id": "Functional and Geometric Margins",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Definitions of functional and geometric margins in SVMs."
    },
    {
      "id": "Optimal Margin Classifier (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Finding the optimal hyperplane that maximizes the margin between classes."
    },
    {
      "id": "Lagrange Duality (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Using duality to solve optimization problems in SVMs."
    },
    {
      "id": "Dual Form of Optimal Margin Classifier (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Formulation of the optimal margin classifier using Lagrange multipliers."
    },
    {
      "id": "Regularization and Non-separable Case (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Handling non-linearly separable data in SVMs with regularization techniques."
    },
    {
      "id": "SMO Algorithm (Optional)",
      "type": "subnode",
      "parent": "Support Vector Machines",
      "description": "Sequential minimal optimization algorithm for solving the dual form of SVM problems."
    },
    {
      "id": "Coordinate Ascent",
      "type": "subnode",
      "parent": "SMO Algorithm (Optional)",
      "description": "Optimization technique used in SMO to update Lagrange multipliers."
    },
    {
      "id": "Sequential Minimal Optimization (SMO)",
      "type": "subnode",
      "parent": "SMO Algorithm (Optional)",
      "description": "Algorithm for efficiently solving the dual form of SVM problems."
    },
    {
      "id": "Deep Learning",
      "type": "major",
      "parent": null,
      "description": "Learning representations from data using neural networks with multiple layers."
    },
    {
      "id": "Supervised Learning with Non-linear Models",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Using non-linear models for supervised learning tasks."
    },
    {
      "id": "Neural Networks",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Networks of interconnected nodes that learn to represent data hierarchically."
    },
    {
      "id": "Modules in Modern Neural Networks",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Discussion on the components that make up contemporary neural networks."
    },
    {
      "id": "Backpropagation",
      "type": "subnode",
      "parent": "Deep Learning",
      "description": "Algorithm for computing gradients of the loss function with respect to weights in a neural network."
    },
    {
      "id": "Preliminaries on Partial Derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Basic concepts and rules related to partial derivatives used in backpropagation."
    },
    {
      "id": "General Strategy of Backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the steps involved in performing backpropagation through a neural network."
    },
    {
      "id": "Backward Functions for Basic Modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Derivation and implementation of backward functions for basic neural network modules."
    },
    {
      "id": "Back-propagation for MLPs",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Specific backpropagation techniques tailored for multi-layer perceptrons."
    },
    {
      "id": "Modern Neural Networks",
      "type": "major",
      "parent": null,
      "description": "Overview of modern neural network architectures and training techniques."
    },
    {
      "id": "Preliminaries on partial derivatives",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Introduction to the mathematical concepts needed for backpropagation."
    },
    {
      "id": "General strategy of backpropagation",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Overview of the algorithmic approach used in backpropagation."
    },
    {
      "id": "Backward functions for basic modules",
      "type": "subnode",
      "parent": "Backpropagation",
      "description": "Details on how to compute gradients for simple network components."
    },
    {
      "id": "Vectorization over training examples",
      "type": "subnode",
      "parent": "Modern Neural Networks",
      "description": "Techniques to optimize the computation of gradients across multiple samples."
    },
    {
      "id": "Generalization and regularization",
      "type": "major",
      "parent": null,
      "description": "Focuses on methods to improve model performance on unseen data."
    },
    {
      "id": "Generalization",
      "type": "subnode",
      "parent": "Generalization and regularization",
      "description": "Discusses strategies for improving a model's ability to generalize from training data."
    },
    {
      "id": "Bias-variance tradeoff",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Explains the balance between model complexity and generalization error."
    },
    {
      "id": "A mathematical decomposition (for regression)",
      "type": "subnode",
      "parent": "Bias-variance tradeoff",
      "description": "Mathematical analysis of bias and variance in regression models."
    },
    {
      "id": "The double descent phenomenon",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Describes a pattern where model performance improves after reaching high complexity."
    },
    {
      "id": "Sample complexity bounds (optional readings)",
      "type": "subnode",
      "parent": "Generalization",
      "description": "Theoretical limits on the number of samples needed for learning."
    },
    {
      "id": "Preliminaries",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Introduction to necessary theoretical concepts."
    },
    {
      "id": "The case of finite \u0125",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Analysis for the scenario where hypothesis space is limited."
    },
    {
      "id": "The case of infinite \u0125",
      "type": "subnode",
      "parent": "Sample complexity bounds (optional readings)",
      "description": "Discussion on sample complexity when the hypothesis space is unbounded."
    },
    {
      "id": "Regularization and model selection",
      "type": "major",
      "parent": null,
      "description": "Techniques to prevent overfitting and choose optimal models."
    },
    {
      "id": "Regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Methods for penalizing overly complex models to improve generalization."
    },
    {
      "id": "Implicit regularization effect (optional reading)",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Exploration of how certain training methods implicitly regularize the model."
    },
    {
      "id": "Model selection via cross validation",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Use of cross-validation to select the best performing model."
    },
    {
      "id": "Bayesian statistics and regularization",
      "type": "subnode",
      "parent": "Regularization and model selection",
      "description": "Application of Bayesian methods in regularizing models."
    },
    {
      "id": "Unsupervised learning",
      "type": "major",
      "parent": null,
      "description": "Techniques for learning from data without labeled responses."
    },
    {
      "id": "Clustering and the k-means algorithm",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Introduction to clustering methods with a focus on k-means."
    },
    {
      "id": "EM algorithms",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Exploration of Expectation-Maximization techniques for unsupervised learning."
    },
    {
      "id": "EM for mixture of Gaussians",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Application of EM to model data as a mixture of Gaussian distributions."
    },
    {
      "id": "Jensen's inequality",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Mathematical principle used in deriving the EM algorithm."
    },
    {
      "id": "General EM algorithms",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Overview of various extensions and applications of the EM framework."
    },
    {
      "id": "Other interpretation of ELBO",
      "type": "subnode",
      "parent": "General EM algorithms",
      "description": "Alternative perspectives on the Evidence Lower Bound in variational inference."
    },
    {
      "id": "Mixture of Gaussians revisited",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Re-examination of Gaussian mixture models using advanced techniques."
    },
    {
      "id": "Variational inference and variational auto-encoder (optional reading)",
      "type": "subnode",
      "parent": "EM algorithms",
      "description": "Introduction to variational methods in unsupervised learning, including VAEs."
    },
    {
      "id": "Principal components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Dimensionality reduction technique for identifying principal components in data."
    },
    {
      "id": "Independent components analysis",
      "type": "subnode",
      "parent": "Unsupervised learning",
      "description": "Technique to separate mixed signals into independent sources."
    },
    {
      "id": "ICA ambiguities",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Discussion on the challenges and limitations of ICA."
    },
    {
      "id": "Densities and linear transformations",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Analysis of how densities change under linear transformations in ICA."
    },
    {
      "id": "ICA algorithm",
      "type": "subnode",
      "parent": "Independent components analysis",
      "description": "Detailed explanation of the Independent Components Analysis process."
    },
    {
      "id": "Self-supervised learning and foundation models",
      "type": "major",
      "parent": null,
      "description": "Techniques for training models on large datasets without explicit supervision."
    },
    {
      "id": "Pretraining and adaptation",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Overview of pre-training methods followed by fine-tuning."
    },
    {
      "id": "Pretraining methods in computer vision",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Discussion on self-supervised techniques for visual data."
    },
    {
      "id": "Pretrained large language models",
      "type": "subnode",
      "parent": "Self-supervised learning and foundation models",
      "description": "Exploration of pre-trained models in natural language processing."
    },
    {
      "id": "Open up the blackbox of Transformers",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Detailed look at the architecture and workings of Transformer models."
    },
    {
      "id": "Zero-shot learning and in-context learning",
      "type": "subnode",
      "parent": "Pretrained large language models",
      "description": "Discussion on model capabilities without explicit training data."
    },
    {
      "id": "Reinforcement Learning and Control",
      "type": "major",
      "parent": null,
      "description": "Techniques for training agents to make decisions in complex environments."
    },
    {
      "id": "Reinforcement learning",
      "type": "subnode",
      "parent": "Reinforcement Learning and Control",
      "description": "Introduction to reinforcement learning concepts and algorithms."
    },
    {
      "id": "Markov decision processes",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Mathematical framework for modeling decision-making in uncertain environments."
    },
    {
      "id": "Value iteration and policy iteration",
      "type": "subnode",
      "parent": "Reinforcement learning",
      "description": "Algorithms for finding optimal policies in MDPs."
    },
    {
      "id": "Linear Q",
      "type": "subnode",
      "parent": "LQR, DDP and LQG",
      "description": "Introduction to linear quadratic control problems."
    },
    {
      "id": "Machine Learning Models",
      "type": "major",
      "parent": null,
      "description": "Overview of different models used in machine learning."
    },
    {
      "id": "Classification Model",
      "type": "subnode",
      "parent": "Machine Learning Models",
      "description": "Models that predict categorical outcomes."
    },
    {
      "id": "Log Likelihood",
      "type": "subnode",
      "parent": "Likelihood Function",
      "description": "Natural logarithm of the likelihood function for easier computation."
    },
    {
      "id": "Gradient Ascent",
      "type": "subnode",
      "parent": "Log Likelihood",
      "description": "Optimization technique used to maximize a function by iteratively moving in the direction of steepest ascent."
    },
    {
      "id": "Machine Learning",
      "type": "major",
      "parent": null,
      "description": "Field of study focusing on algorithms that learn from data."
    },
    {
      "id": "Logistic Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Sigmoid function mapping real values to [0,1] range for binary classification."
    },
    {
      "id": "Hypothesis Representation",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Formulation of hypotheses using the logistic function in logistic regression."
    },
    {
      "id": "Derivative of Sigmoid Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Calculation showing derivative simplifies to g(z)(1-g(z))."
    },
    {
      "id": "Parameter Fitting",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Process of fitting parameters via maximum likelihood estimation."
    },
    {
      "id": "Machine Learning Overview",
      "type": "major",
      "parent": null,
      "description": "Introduction to machine learning concepts and algorithms."
    },
    {
      "id": "Binary Classification",
      "type": "subnode",
      "parent": "Classification Problem",
      "description": "Type of classification problem with two possible outcomes (0 or 1)."
    },
    {
      "id": "Non-parametric Algorithms",
      "type": "subnode",
      "parent": "Machine Learning Overview",
      "description": "Algorithms where the amount of information needed grows with the size of the training set."
    },
    {
      "id": "Stochastic Gradient Ascent Rule",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Update rule for parameters in logistic regression using stochastic gradient ascent."
    },
    {
      "id": "Logistic Loss Function",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Loss function for logistic regression, defined as a function of t and y."
    },
    {
      "id": "Negative Log-Likelihood",
      "type": "subnode",
      "parent": "Logistic Regression",
      "description": "Equivalent to the negative of the log-likelihood in logistic regression."
    },
    {
      "id": "Chain Rule Application",
      "type": "subnode",
      "parent": "Gradient Descent",
      "description": "Application of chain rule for computing gradients in logistic regression."
    }
  ],
  "edges": [
    {
      "from": "General EM algorithms",
      "to": "Other interpretation of ELBO",
      "relationship": "has_subtopic"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Continuous state MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gaussian Discriminant Analysis",
      "to": "GDA Model",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Lagrange Duality (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Backward functions for basic modules",
      "relationship": "has_subtopic"
    },
    {
      "from": "SMO Algorithm (Optional)",
      "to": "Sequential Minimal Optimization (SMO)",
      "relationship": "subtopic"
    },
    {
      "from": "Generalized Linear Models",
      "to": "Exponential Family",
      "relationship": "subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Vectorization over training examples",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Jensen's inequality",
      "relationship": "has_subtopic"
    },
    {
      "from": "Normal Equations",
      "to": "Least Squares Revisited",
      "relationship": "subtopic"
    },
    {
      "from": "Cost Function",
      "to": "Gradient Descent",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Normal Equations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Dual Form of Optimal Margin Classifier (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "The case of infinite \u0125",
      "relationship": "has_subtopic"
    },
    {
      "from": "Probabilistic Interpretation",
      "to": "Regression Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Modules in Modern Neural Networks",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Learning a model for an MDP",
      "relationship": "has_subtopic"
    },
    {
      "from": "Deep Learning",
      "to": "Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Logistic Loss Function",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Optimization Problem",
      "relationship": "depends_on"
    },
    {
      "from": "Logistic Regression",
      "to": "Logistic Function",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Batch Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Regularization and Non-separable Case (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Chain Rule Application",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Markov decision processes",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Probabilistic Interpretation",
      "relationship": "related_to"
    },
    {
      "from": "Least-Squares Cost Function",
      "to": "Invertibility of X^TX Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Naive Bayes",
      "to": "Laplace Smoothing",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Linear Regression",
      "relationship": "has_subtopic"
    },
    {
      "from": "Likelihood Function",
      "to": "Least-Squares Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Logistic Regression",
      "to": "Hypothesis Representation",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Negative Log-Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "Gaussian Discriminant Analysis",
      "to": "Discussion: GDA and Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Bias-variance tradeoff",
      "to": "A mathematical decomposition (for regression)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "LMS Algorithm",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining and adaptation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Kernel Methods",
      "to": "Feature Maps",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Cost Function",
      "to": "Ordinary Least Squares Regression",
      "relationship": "special_case_of"
    },
    {
      "from": "Function Representation",
      "to": "Linear Function Approximation",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning",
      "to": "Logistic Regression",
      "relationship": "related_to"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretrained large language models",
      "relationship": "has_subtopic"
    },
    {
      "from": "Pretrained large language models",
      "to": "Zero-shot learning and in-context learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Normal Equations",
      "relationship": "related_to"
    },
    {
      "from": "Maximum Likelihood Estimation (MLE)",
      "to": "Log Likelihood Function",
      "relationship": "related_to"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Perceptron Learning Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "LMS Update Rule",
      "to": "Error Term",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression (LWR)",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Probabilistic Interpretation",
      "relationship": "subtopic"
    },
    {
      "from": "Kernel Methods",
      "to": "LMS with Kernel Trick",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Stochastic Gradient Ascent Rule",
      "relationship": "depends_on"
    },
    {
      "from": "Gaussian Discriminant Analysis",
      "to": "Multivariate Normal Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "LMS Update Rule",
      "to": "Single Training Example",
      "relationship": "related_to"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "Preliminaries",
      "relationship": "has_subtopic"
    },
    {
      "from": "Log Likelihood",
      "to": "Gradient Ascent",
      "relationship": "depends_on"
    },
    {
      "from": "Hypothesis Function",
      "to": "Supervised Learning",
      "relationship": "subtopic_of"
    },
    {
      "from": "Probabilistic Models in Machine Learning",
      "to": "Conditional Probability Distribution",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Functional and Geometric Margins",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Implicit regularization effect (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Locally Weighted Linear Regression",
      "to": "Machine Learning Overview",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "General Strategy of Backpropagation",
      "relationship": "subtopic"
    },
    {
      "from": "Constructing GLMs",
      "to": "Logistic Regression (GLM)",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "EM algorithms",
      "to": "Mixture of Gaussians revisited",
      "relationship": "has_subtopic"
    },
    {
      "from": "SMO Algorithm (Optional)",
      "to": "Coordinate Ascent",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement Learning and Control",
      "to": "Reinforcement learning",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Parameter Fitting",
      "relationship": "subtopic"
    },
    {
      "from": "Locally Weighted Linear Regression (LWLR)",
      "to": "Weight Calculation",
      "relationship": "depends_on"
    },
    {
      "from": "LMS Update Rule",
      "to": "Widrow-Hoff Learning Rule",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "Backward Functions for Basic Modules",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Independent components analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Algorithms",
      "to": "Gradient Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Normal Equations",
      "relationship": "subtopic"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Linearization of dynamics",
      "relationship": "subtopic_of"
    },
    {
      "from": "Supervised Learning",
      "to": "Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "LMS Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Weight Calculation",
      "to": "Bandwidth Parameter (\u03c4)",
      "relationship": "contains"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Kernel Methods",
      "relationship": "related_to"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Feature Selection",
      "relationship": "subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Back-propagation for MLPs",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "Least Squares Revisited",
      "to": "Design Matrix",
      "relationship": "depends_on"
    },
    {
      "from": "Support Vector Machines",
      "to": "Margins: Intuition",
      "relationship": "subtopic"
    },
    {
      "from": "Regularization and model selection",
      "to": "Bayesian statistics and regularization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Overfitting",
      "relationship": "related_to"
    },
    {
      "from": "Naive Bayes",
      "to": "Event Models for Text Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "subtopic_of"
    },
    {
      "from": "Supervised Learning",
      "to": "Generalized Linear Models",
      "relationship": "subtopic"
    },
    {
      "from": "Likelihood Function",
      "to": "Probabilistic Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Generalized Linear Models",
      "to": "Constructing GLMs",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Function Representation",
      "relationship": "contains"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Regulation (LQR)",
      "relationship": "subtopic_of"
    },
    {
      "from": "Regularization and model selection",
      "to": "Model selection via cross validation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generalization",
      "to": "Bias-variance tradeoff",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Regression Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Function Approximation",
      "to": "Parameters (Weights)",
      "relationship": "defines"
    },
    {
      "from": "From non-linear dynamics to LQR",
      "to": "Differential Dynamic Programming (DDP)",
      "relationship": "subtopic_of"
    },
    {
      "from": "Probabilistic Assumptions",
      "to": "Likelihood Function",
      "relationship": "leads_to"
    },
    {
      "from": "Locally Weighted Linear Regression",
      "to": "Non-parametric Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Convex Function",
      "relationship": "related_to"
    },
    {
      "from": "Generalization",
      "to": "Sample complexity bounds (optional readings)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Basics",
      "to": "Gradient Descent",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Least-Squares Regression",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Methods",
      "to": "Properties of Kernels",
      "relationship": "subtopic"
    },
    {
      "from": "Likelihood Function",
      "to": "Log Likelihood",
      "relationship": "related_to"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Multi-class Classification",
      "relationship": "subtopic"
    },
    {
      "from": "Generalization",
      "to": "The double descent phenomenon",
      "relationship": "has_subtopic"
    },
    {
      "from": "Likelihood Function",
      "to": "Independence Assumption",
      "relationship": "depends_on"
    },
    {
      "from": "Normal Equations",
      "to": "Matrix Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Derivative of Sigmoid Function",
      "relationship": "subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Locally Weighted Linear Regression (LWLR)",
      "relationship": "related_to"
    },
    {
      "from": "Learning a model for an MDP",
      "to": "Connections between Policy and Value Iteration (Optional)",
      "relationship": "has_subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "subtopic_of"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "From non-linear dynamics to LQR",
      "relationship": "subtopic_of"
    },
    {
      "from": "EM algorithms",
      "to": "EM for mixture of Gaussians",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Optimal Margin Classifier (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Least Squares Revisited",
      "to": "Vector y",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Feature Selection",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Classification and Logistic Regression",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Q",
      "relationship": "has_subtopic"
    },
    {
      "from": "Probabilistic Models in Machine Learning",
      "to": "Design Matrix X",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Linear Regression",
      "relationship": "contains"
    },
    {
      "from": "Batch Gradient Descent",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "related_to"
    },
    {
      "from": "Backpropagation",
      "to": "General strategy of backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Linear Regression",
      "relationship": "depends_on"
    },
    {
      "from": "Deep Learning",
      "to": "Modules in Modern Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Function Representation",
      "to": "Cost Function",
      "relationship": "leads_to"
    },
    {
      "from": "EM algorithms",
      "to": "Variational inference and variational auto-encoder (optional reading)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Cost Function J(\u03b8)",
      "relationship": "related_to"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Linear Quadratic Gaussian (LQG)",
      "relationship": "subtopic_of"
    },
    {
      "from": "Gradient Descent",
      "to": "Batch Gradient Descent",
      "relationship": "related_to"
    },
    {
      "from": "Kernel Methods",
      "to": "LMS with Features",
      "relationship": "subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "Notation (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Normal Equations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries on partial derivatives",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Back-propagation for MLPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Gaussian Discriminant Analysis",
      "relationship": "subtopic"
    },
    {
      "from": "Likelihood Function",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Discretization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "EM algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "Independent components analysis",
      "to": "Densities and linear transformations",
      "relationship": "has_subtopic"
    },
    {
      "from": "Classification Problem",
      "to": "Logistic Regression",
      "relationship": "depends_on"
    },
    {
      "from": "Modern Neural Networks",
      "to": "Backpropagation",
      "relationship": "has_subtopic"
    },
    {
      "from": "Machine Learning Concepts",
      "to": "Likelihood Function",
      "relationship": "related_to"
    },
    {
      "from": "Gradient Descent",
      "to": "Stochastic Gradient Descent (SGD)",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning",
      "to": "Supervised Learning with Non-linear Models",
      "relationship": "subtopic"
    },
    {
      "from": "Deep Learning",
      "to": "Neural Networks",
      "relationship": "subtopic"
    },
    {
      "from": "Probabilistic Models in Machine Learning",
      "to": "Likelihood Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Gradient Descent",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Non-parametric Algorithms",
      "relationship": "related_to"
    },
    {
      "from": "Update Rule",
      "to": "LMS Update Rule",
      "relationship": "subtopic"
    },
    {
      "from": "Matrix Derivatives",
      "to": "Gradient Definition",
      "relationship": "has_subtopic"
    },
    {
      "from": "Probabilistic Interpretation",
      "to": "Error Term Distribution",
      "relationship": "subtopic"
    },
    {
      "from": "Self-supervised learning and foundation models",
      "to": "Pretraining methods in computer vision",
      "relationship": "has_subtopic"
    },
    {
      "from": "Support Vector Machines",
      "to": "SMO Algorithm (Optional)",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Value iteration and policy iteration",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Classification Problem",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generalization and regularization",
      "to": "Generalization",
      "relationship": "has_subtopic"
    },
    {
      "from": "Constructing GLMs",
      "to": "Ordinary Least Squares",
      "relationship": "subtopic"
    },
    {
      "from": "Gradient Descent",
      "to": "Learning Rate (\u03b1)",
      "relationship": "depends_on"
    },
    {
      "from": "Linear Regression",
      "to": "Underfitting",
      "relationship": "related_to"
    },
    {
      "from": "Pretrained large language models",
      "to": "Open up the blackbox of Transformers",
      "relationship": "has_subtopic"
    },
    {
      "from": "Binary Classification",
      "to": "Classification Problem",
      "relationship": "subtopic"
    },
    {
      "from": "LQR, DDP and LQG",
      "to": "Finite-horizon MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Probabilistic Models in Machine Learning",
      "to": "Maximum Likelihood Estimation (MLE)",
      "relationship": "has_subtopic"
    },
    {
      "from": "Classification Model",
      "to": "Probabilistic Assumptions",
      "relationship": "depends_on"
    },
    {
      "from": "Machine Learning Models",
      "to": "Classification Model",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Cost Function",
      "relationship": "has_subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Support Vector Machines",
      "relationship": "related_to"
    },
    {
      "from": "EM algorithms",
      "to": "General EM algorithms",
      "relationship": "has_subtopic"
    },
    {
      "from": "LMS Update Rule",
      "to": "Gradient Descent",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Principal components analysis",
      "relationship": "has_subtopic"
    },
    {
      "from": "Generative Learning Algorithms",
      "to": "Naive Bayes",
      "relationship": "subtopic"
    },
    {
      "from": "Unsupervised learning",
      "to": "Clustering and the k-means algorithm",
      "relationship": "has_subtopic"
    },
    {
      "from": "Backpropagation",
      "to": "Preliminaries on Partial Derivatives",
      "relationship": "subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Locally Weighted Linear Regression (LWLR)",
      "relationship": "variant_of"
    },
    {
      "from": "Linear Regression",
      "to": "LMS Algorithm",
      "relationship": "subtopic"
    },
    {
      "from": "Reinforcement learning",
      "to": "Continuous state MDPs",
      "relationship": "has_subtopic"
    },
    {
      "from": "Classification and Logistic Regression",
      "to": "Maximizing l(theta)",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Generative Learning Algorithms",
      "relationship": "subtopic"
    },
    {
      "from": "Continuous state MDPs",
      "to": "Value function approximation",
      "relationship": "subtopic_of"
    },
    {
      "from": "Independent components analysis",
      "to": "ICA ambiguities",
      "relationship": "has_subtopic"
    },
    {
      "from": "Logistic Regression",
      "to": "Classification Problem",
      "relationship": "subtopic"
    },
    {
      "from": "Sample complexity bounds (optional readings)",
      "to": "The case of finite \u0125",
      "relationship": "has_subtopic"
    },
    {
      "from": "Linear Regression",
      "to": "Locally Weighted Linear Regression",
      "relationship": "subtopic"
    },
    {
      "from": "Supervised Learning",
      "to": "Deep Learning",
      "relationship": "related_to"
    },
    {
      "from": "Linear Regression",
      "to": "Least-Squares Cost Function",
      "relationship": "subtopic"
    }
  ]
}